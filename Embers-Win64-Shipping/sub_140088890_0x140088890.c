// 函数: sub_140088890
// 地址: 0x140088890
// 来自: E:\Embers\Embers\Binaries\Win64\Embers-Win64-Shipping.exe

int128_t var_58 = arg16[0].o
int128_t var_68 = arg15[0].o
int128_t var_78 = arg14[0].o
int128_t var_88 = arg13[0].o
int128_t var_98 = arg12[0].o
int128_t var_a8 = arg11[0].o
int128_t var_b8 = arg10[0].o
int128_t var_c8 = arg9[0].o
int128_t var_d8 = arg8[0].o
int128_t var_e8 = arg7[0].o
float zmm0[0x8] = *arg21
double* r15 = arg19
double zmm3[0x4]
zmm3[0].o = arg17
float var_460[0x8]
float zmm4[0x8]
int32_t zmm5[0x8]

if (_mm256_movemask_ps(zmm0) != 0xff)
    int32_t rdi_9 = ((arg20 s>> 0x1f u>> 0x1d) + arg20) & 0xfffffff8
    arg5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
    float var_340_1[0x8] = arg5
    float var_360_1[0x8] = arg5
    float var_380_1[0x8] = arg5
    float var_440_2[0x8] = zmm0
    float var_480_1[0x8]
    float var_420[0x8]
    int32_t rsi_1
    
    if (rdi_9 s<= 0)
        arg5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
        var_480_1 = arg5
        rsi_1 = 0
        
        if (0 s>= arg20)
            zmm3 = var_440_2
            zmm5 = var_480_1
        else
        label_1400894f0:
            zmm0[0].o = zx.o(rsi_1)
            zmm0[0].o = __vpshufd_xmmdq_xmmdq_immb(zmm0[0].o, 0)
            arg5[0].o = __vpaddd_xmmdq_xmmdq_memdq(zmm0[0].o, data_142e11d00)
            zmm0[0].o = __vpaddd_xmmdq_xmmdq_memdq(zmm0[0].o, data_142fc9500)
            arg6[0].o = zx.o(arg20)
            arg6[0].o = __vpshufd_xmmdq_xmmdq_immb(arg6[0].o, 0)
            arg11[0].o = __vpcmpgtd_xmmdq_xmmdq_xmmdq(arg6[0].o, zmm0[0].o)
            arg9[0].o = __vpcmpgtd_xmmdq_xmmdq_xmmdq(arg6[0].o, arg5[0].o)
            arg12 = _mm256_insertf128_ps(arg9, arg11[0].o, 1)
            arg6[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
            zmm0[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(arg6[0].o, zmm0[0].o)
            arg6[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
            arg5[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(arg6[0].o, arg5[0].o)
            zmm0 = _mm256_and_ps(arg12, _mm256_insertf128_ps(arg5, zmm0[0].o, 1))
            int64_t rax_20 = sx.q(zmm0[0])
            var_420[0].q = rax_20
            void* rbx_6 = arg4 + (rax_20 << 2)
            int32_t temp0_393 = __vpextrd_gpr32d_xmmdq_immb(zmm0[0].o, 1)
            var_460[0].q = arg1
            int64_t r15_2 = sx.q(temp0_393)
            void* r12_2 = arg4 + (r15_2 << 2)
            arg7[0].o = _mm256_extractf128_ps(zmm0[0].o, 1)
            int64_t r11_1 = sx.q(arg7[0])
            void* r10_2 = arg4 + (r11_1 << 2)
            int64_t r13_2 = sx.q(__vpextrd_gpr32d_xmmdq_immb(arg7[0].o, 1))
            void* rsi_6 = arg4 + (r13_2 << 2)
            arg8 = __vandps_ymmqq_ymmqq_memqq(arg12, data_142fc9520)
            zmm4[0].o = _mm256_extractf128_ps(arg8[0].o, 1)
            arg5[0].o = *(zx.q(zmm4[0]) + r10_2)
            arg10 = __vandps_ymmqq_ymmqq_memqq(arg12, data_142fc9540)
            arg6[0].o = _mm256_extractf128_ps(arg10[0].o, 1)
            zmm5[0].o = *(zx.q(arg6[0]) + r10_2)
            int32_t temp0_400 = __vpextrd_gpr32d_xmmdq_immb(arg7[0].o, 2)
            uint64_t rdi_21 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm4[0].o, 1))
            arg5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, *(rdi_21 + rsi_6), 0x10)
            float* rdi_22 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg6[0].o, 1))
            zmm5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm5[0].o, *(rdi_22 + rsi_6), 0x10)
            int64_t rcx_11 = sx.q(temp0_400)
            void* rdi_23 = arg4 + (rcx_11 << 2)
            uint64_t rsi_7 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm4[0].o, 2))
            arg5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, *(rsi_7 + rdi_23), 0x20)
            float* rsi_8 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg6[0].o, 2))
            zmm5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm5[0].o, *(rsi_8 + rdi_23), 0x20)
            int64_t rdi_25 = sx.q(__vpextrd_gpr32d_xmmdq_immb(arg7[0].o, 3))
            arg7[0].o = *(zx.q(arg8[0].d) + rbx_6)
            zmm3[0].o = *(zx.q(arg10[0].d) + rbx_6)
            void* rsi_11 = arg4 + (rdi_25 << 2)
            uint64_t rbx_7 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm4[0].o, 3))
            arg5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, *(rbx_7 + rsi_11), 0x30)
            float* rbx_8 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg6[0].o, 3))
            arg6[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm5[0].o, *(rbx_8 + rsi_11), 0x30)
            int32_t temp0_414 = __vpextrd_gpr32d_xmmdq_immb(zmm0[0].o, 2)
            uint64_t rsi_12 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg8[0].o, 1))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg7[0].o, *(rsi_12 + r12_2), 0x10)
            uint64_t rsi_13 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg10[0].o, 1))
            zmm3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm3[0].o, *(rsi_13 + r12_2), 0x10)
            int64_t rbx_10 = sx.q(temp0_414)
            void* rsi_14 = arg4 + (rbx_10 << 2)
            uint64_t rax_22 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg8[0].o, 2))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rax_22 + rsi_14), 0x20)
            uint64_t rax_23 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg10[0].o, 2))
            zmm3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm3[0].o, *(rax_23 + rsi_14), 0x20)
            zmm5[0].o = *(arg4 + (var_420[0].q << 2))
            arg7[0].o = *(arg4 + (r11_1 << 2))
            arg7[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg7[0].o, *(arg4 + (r13_2 << 2)), 0x10)
            arg7[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg7[0].o, *(arg4 + (rcx_11 << 2)), 0x20)
            arg7[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg7[0].o, *(arg4 + (rdi_25 << 2)), 0x30)
            zmm5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm5[0].o, *(arg4 + (r15_2 << 2)), 0x10)
            zmm5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm5[0].o, *(arg4 + (rbx_10 << 2)), 0x20)
            int64_t rax_26 = sx.q(__vpextrd_gpr32d_xmmdq_immb(zmm0[0].o, 3))
            zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm5[0].o, *(arg4 + (rax_26 << 2)), 0x30)
            void* rax_27 = arg4 + (rax_26 << 2)
            uint64_t rcx_12 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg8[0].o, 3))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rcx_12 + rax_27), 0x30)
            float* rcx_13 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg10[0].o, 3))
            zmm3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm3[0].o, *(rcx_13 + rax_27), 0x30)
            zmm5 = _mm256_insertf128_ps(zmm0, arg7[0].o, 1)
            arg5 = _mm256_insertf128_ps(zmm4, arg5[0].o, 1)
            arg6 = _mm256_insertf128_ps(zmm3, arg6[0].o, 1)
            int32_t var_280_2[0x8] = zmm5
            zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
            float temp0_438[0x8] = _mm256_broadcast_ss(arg3[3])
            float var_200_4[0x8] = temp0_438
            float var_1e0_4[0x8] = temp0_438
            float var_1c0_4[0x8] = temp0_438
            float var_1a0_4[0x8] = temp0_438
            zmm3 = _mm256_broadcast_ss(arg3[1])
            float temp0_440[0x8] = _mm256_mul_ps(arg6, zmm3)
            arg8 = _mm256_broadcast_ss(arg3[2])
            arg10 = _mm256_mul_ps(zmm5, arg8)
            arg13 = _mm256_broadcast_ss(*arg3)
            arg8 = _mm256_mul_ps(arg5, arg8)
            float temp0_445[0x8] = _mm256_mul_ps(arg5, arg13)
            float temp0_446[0x8] = _mm256_sub_ps(temp0_440, arg8)
            float temp0_448[0x8] = _mm256_sub_ps(arg10, _mm256_mul_ps(arg6, arg13))
            float temp0_450[0x8] = _mm256_sub_ps(temp0_445, _mm256_mul_ps(zmm5, zmm3))
            zmm3 = _mm256_add_ps(temp0_446, temp0_446)
            float temp0_452[0x8] = _mm256_add_ps(temp0_448, temp0_448)
            float temp0_453[0x8] = _mm256_add_ps(temp0_450, temp0_450)
            zmm5 = _mm256_broadcast_ss(arg3[1])
            float temp0_455[0x8] = _mm256_mul_ps(temp0_453, zmm5)
            arg8 = _mm256_broadcast_ss(arg3[2])
            arg10 = _mm256_mul_ps(zmm3, arg8)
            arg13 = _mm256_broadcast_ss(*arg3)
            float temp0_460[0x8] = _mm256_sub_ps(temp0_455, _mm256_mul_ps(temp0_452, arg8))
            arg8 = _mm256_sub_ps(arg10, _mm256_mul_ps(temp0_453, arg13))
            arg10 = _mm256_mul_ps(temp0_438, zmm0)
            arg10 = _mm256_sub_ps(arg10, arg10)
            arg10 = _mm256_add_ps(arg10, arg10)
            double var_400_4[0x4] = zmm3
            float var_3e0_4[0x8] = temp0_452
            float var_3c0_4[0x8] = temp0_453
            double var_3a0_4[0x4] = arg10
            arg13 = _mm256_mul_ps(temp0_452, arg13)
            float temp0_467[0x8] = _mm256_mul_ps(temp0_438, temp0_452)
            float temp0_468[0x8] = _mm256_mul_ps(temp0_438, temp0_453)
            arg10 = _mm256_mul_ps(temp0_438, arg10)
            float temp0_470[0x8] = _mm256_mul_ps(temp0_438, zmm3)
            zmm3 = _mm256_sub_ps(arg13, _mm256_mul_ps(zmm3, zmm5))
            float temp0_473[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_470, var_280_2)
            float var_180_2[0x8] = temp0_473
            float temp0_474[0x8] = _mm256_add_ps(temp0_473, temp0_460)
            float temp0_475[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_467, arg5)
            float var_160_2[0x8] = temp0_475
            float temp0_476[0x8] = _mm256_add_ps(temp0_475, arg8)
            zmm4 = _mm256_add_ps(temp0_474, _mm256_broadcast_ss(arg3[4]))
            arg7 = _mm256_add_ps(temp0_476, _mm256_broadcast_ss(arg3[5]))
            float temp0_481[0x8] = _mm256_broadcast_ss(arg3[6])
            float temp0_482[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_468, arg6)
            arg8 = _mm256_add_ps(_mm256_add_ps(temp0_482, zmm3), temp0_481)
            float temp0_486[0x8] = _mm256_sub_ps(zmm4, _mm256_broadcast_ss(*arg2))
            float temp0_488[0x8] = _mm256_add_ps(_mm256_mul_ps(temp0_486, temp0_486), zmm0)
            float temp0_490[0x8] = _mm256_sub_ps(arg7, _mm256_broadcast_ss(arg2[1]))
            float temp0_492[0x8] = _mm256_add_ps(temp0_488, _mm256_mul_ps(temp0_490, temp0_490))
            float temp0_494[0x8] = _mm256_sub_ps(arg8, _mm256_broadcast_ss(arg2[2]))
            float temp0_496[0x8] = _mm256_add_ps(temp0_492, _mm256_mul_ps(temp0_494, temp0_494))
            float temp0_497[0x8] = _mm256_rsqrt_ps(temp0_496)
            float var_140_2[0x8] = temp0_482
            float temp0_499[0x8] = _mm256_mul_ps(temp0_497, _mm256_mul_ps(temp0_496, temp0_497))
            float temp0_502[0x8] = __vmulps_ymmqq_ymmqq_memqq(
                _mm256_mul_ps(temp0_497, _mm256_sub_ps(data_142fc9480, temp0_499)), data_142fc94a0)
            float temp0_503[0x8] = _mm256_rcp_ps(temp0_502)
            float temp0_504[0x8] = _mm256_mul_ps(temp0_503, temp0_502)
            arg6 = data_142fc94c0
            float temp0_506[0x8] = _mm256_mul_ps(temp0_503, _mm256_sub_ps(arg6, temp0_504))
            float temp0_508[0x8] = _mm256_sub_ps(_mm256_broadcast_ss(*var_460[0].q), temp0_506)
            arg5 = __vaddps_ymmqq_ymmqq_memqq(arg10, zmm0)
            arg6[0].o = arg17
            arg6[0].o = _mm_permute_ps(arg6[0].o, 0)
            arg6 = _mm256_insertf128_ps(arg6, arg6[0].o, 1)
            zmm0 = _mm256_sub_ps(temp0_508, arg6)
            arg10 = __vcmpps_ymmqq_ymmqq_memqq_immb(zmm0, data_142fc94e0, 1)
            float var_120_2[0x8] = arg5
            arg5[0].o = _mm256_extractf128_ps(arg10[0].o, 1)
            arg5[0].o &= arg11[0].o
            arg6[0].o = __vandps_xmmdq_xmmdq_xmmdq(arg10[0].o, arg9[0].o)
            arg5[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(arg6[0].o, arg5[0].o)
            arg6[0].o = __vpacksswb_xmmdq_xmmdq_xmmdq(arg5[0].o, zmm0[0].o)
            
            if (__vpmovmskb_gpr32d_xmmdq(arg6[0].o) == 0)
                r15 = arg19
                zmm3 = var_440_2
                zmm5 = var_480_1
            else
                float temp0_519[0x8] = _mm256_mul_ps(zmm4, zmm0)
                zmm3 = _mm256_mul_ps(arg7, zmm0)
                float temp0_521[0x8] = _mm256_mul_ps(arg8, zmm0)
                arg6 = __vaddps_ymmqq_ymmqq_memqq(temp0_519, var_380_1)
                zmm3 = __vaddps_ymmqq_ymmqq_memqq(zmm3, var_360_1)
                zmm4 = __vaddps_ymmqq_ymmqq_memqq(temp0_521, var_340_1)
                int32_t temp0_525[0x8] = _mm256_and_ps(arg10, arg12)
                var_380_1 = _mm256_maskstore_ps(temp0_525, arg6)
                var_360_1 = _mm256_maskstore_ps(temp0_525, zmm3)
                var_340_1 = _mm256_maskstore_ps(temp0_525, zmm4)
                zmm5 = var_480_1
                zmm0 = _mm256_add_ps(zmm5, zmm0)
                arg6[0].o = __vpmovsxwd_xmmdq_xmmq(arg5[0].q)
                arg5[0].o = __vpshufd_xmmdq_xmmdq_immb(arg5[0].o, 0x4e)
                arg5[0].o = __vpmovsxwd_xmmdq_xmmq(arg5[0].q)
                arg5 = _mm256_insertf128_ps(arg6, arg5[0].o, 1)
                zmm5 = _mm256_blendv_ps(zmm5, zmm0, arg5)
                r15 = arg19
                zmm3 = var_440_2
    else
        arg10 = _mm256_broadcast_ss(arg3[3])
        zmm0[0].o = *arg3
        arg5[0].o = arg3[1]
        arg6[0].o = arg3[2]
        var_460 = _mm256_broadcast_ss(*arg1)
        var_420 = _mm256_broadcast_ss(*arg2)
        float temp0_116[0x8] = _mm256_broadcast_ss(arg2[1])
        zmm4 = _mm256_broadcast_ss(arg2[2])
        float var_2c0_2[0x8] = zmm4
        arg9[0].o = _mm_permute_ps(zmm3[0].o, 0)
        arg9 = _mm256_insertf128_ps(arg9, arg9[0].o, 1)
        int32_t rbx_2 = 0
        arg5[0].o = _mm_permute_ps(arg5[0].o, 0)
        arg11 = _mm256_insertf128_ps(arg5, arg5[0].o, 1)
        arg5[0].o = _mm_permute_ps(arg6[0].o, 0)
        arg12 = _mm256_insertf128_ps(arg5, arg5[0].o, 1)
        zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0)
        arg13 = _mm256_insertf128_ps(zmm0, zmm0[0].o, 1)
        rsi_1 = 0
        zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        var_480_1 = zmm0
        
        do
            int64_t rax_4 = sx.q(rbx_2)
            zmm0[0].o = *(arg4 + rax_4)
            arg5[0].o = *(arg4 + rax_4 + 0x10)
            arg6[0].o = *(arg4 + rax_4 + 0x20)
            zmm4[0].o = *(arg4 + rax_4 + 0x30)
            zmm5[0].o = *(arg4 + rax_4 + 0x40)
            arg7[0].o = *(arg4 + rax_4 + 0x50)
            arg8[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg5[0].o, 0xec)
            zmm3[0].o = _mm_permute_ps(arg6[0].o, 0x44)
            arg14[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg8[0].o, zmm3[0].o, 8)
            arg8[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm5[0].o, 0xec)
            zmm3[0].o = _mm_permute_ps(arg7[0].o, 0x44)
            arg15[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg8[0].o, zmm3[0].o, 8)
            arg8[0].o = _mm_permute_ps(zmm0[0].o, 0xe5)
            zmm3[0].o = _mm_permute_ps(arg5[0].o, 0xf0)
            zmm3[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, arg8[0].o, 1)
            arg8[0].o = _mm_permute_ps(arg6[0].o, 0xa4)
            arg16[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, arg8[0].o, 8)
            arg8[0].o = _mm_permute_ps(zmm4[0].o, 0xe5)
            zmm3[0].o = _mm_permute_ps(zmm5[0].o, 0xf0)
            zmm3[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, arg8[0].o, 1)
            arg8[0].o = _mm_permute_ps(arg7[0].o, 0xa4)
            zmm3[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, arg8[0].o, 8)
            zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0x4e)
            zmm0[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg5[0].o, 2)
            zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg6[0].o, 0xc4)
            arg5[0].o = _mm_permute_ps(zmm4[0].o, 0x4e)
            arg5[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm5[0].o, 2)
            arg5[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, arg7[0].o, 0xc4)
            arg6 = _mm256_insertf128_ps(arg14, arg15[0].o, 1)
            double temp0_150[0x4] = _mm256_insertf128_ps(arg16, zmm3[0].o, 1)
            zmm0 = _mm256_insertf128_ps(zmm0, arg5[0].o, 1)
            arg7[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg7[0].o, arg7[0].o)
            float var_3a0_2[0x8] = arg7
            float temp0_153[0x8] = _mm256_mul_ps(arg11, zmm0)
            float temp0_154[0x8] = _mm256_mul_ps(arg12, arg6)
            zmm5 = _mm256_mul_ps(arg13, temp0_150)
            float temp0_157[0x8] = _mm256_sub_ps(temp0_153, _mm256_mul_ps(arg12, temp0_150))
            float temp0_159[0x8] = _mm256_sub_ps(temp0_154, _mm256_mul_ps(arg13, zmm0))
            float temp0_161[0x8] = _mm256_sub_ps(zmm5, _mm256_mul_ps(arg11, arg6))
            float temp0_162[0x8] = _mm256_add_ps(temp0_157, temp0_157)
            float temp0_163[0x8] = _mm256_add_ps(temp0_159, temp0_159)
            float temp0_164[0x8] = _mm256_add_ps(temp0_161, temp0_161)
            zmm3 = __vaddps_ymmqq_ymmqq_memqq(_mm256_mul_ps(arg10, temp0_162), arg6)
            float temp0_169[0x8] =
                _mm256_sub_ps(_mm256_mul_ps(temp0_164, arg11), _mm256_mul_ps(temp0_163, arg12))
            zmm5 = _mm256_mul_ps(temp0_162, arg12)
            zmm3 = _mm256_add_ps(zmm3, temp0_169)
            float temp0_173[0x8] = _mm256_sub_ps(zmm5, _mm256_mul_ps(temp0_164, arg13))
            zmm5 = __vaddps_ymmqq_ymmqq_memqq(_mm256_mul_ps(arg10, temp0_163), temp0_150)
            zmm4 = _mm256_add_ps(zmm5, temp0_173)
            double var_200_2[0x4] = arg10
            double var_1e0_2[0x4] = arg10
            double var_1c0_2[0x4] = arg10
            double var_1a0_2[0x4] = arg10
            float temp0_182[0x8] = _mm256_add_ps(
                __vaddps_ymmqq_ymmqq_memqq(_mm256_mul_ps(arg10, temp0_164), zmm0), 
                _mm256_sub_ps(_mm256_mul_ps(temp0_163, arg13), _mm256_mul_ps(temp0_162, arg11)))
            arg5 = _mm256_add_ps(zmm3, _mm256_broadcast_ss(arg3[4]))
            arg14 = _mm256_add_ps(zmm4, _mm256_broadcast_ss(arg3[5]))
            arg15 = _mm256_add_ps(temp0_182, _mm256_broadcast_ss(arg3[6]))
            float temp0_189[0x8] = __vsubps_ymmqq_ymmqq_memqq(arg5, var_420)
            float temp0_190[0x8] = __vsubps_ymmqq_ymmqq_memqq(arg14, temp0_116)
            float temp0_194[0x8] = _mm256_add_ps(
                _mm256_add_ps(_mm256_mul_ps(temp0_189, temp0_189), arg7), 
                _mm256_mul_ps(temp0_190, temp0_190))
            float temp0_195[0x8] = __vsubps_ymmqq_ymmqq_memqq(arg15, var_2c0_2)
            float temp0_197[0x8] = _mm256_add_ps(temp0_194, _mm256_mul_ps(temp0_195, temp0_195))
            float temp0_198[0x8] = _mm256_rsqrt_ps(temp0_197)
            float temp0_200[0x8] = _mm256_mul_ps(temp0_198, _mm256_mul_ps(temp0_197, temp0_198))
            float temp0_203[0x8] = __vmulps_ymmqq_ymmqq_memqq(
                _mm256_mul_ps(temp0_198, _mm256_sub_ps(data_142fc9480, temp0_200)), data_142fc94a0)
            float temp0_204[0x8] = _mm256_rcp_ps(temp0_203)
            float temp0_205[0x8] = _mm256_mul_ps(temp0_204, temp0_203)
            zmm3 = data_142fc94c0
            arg6 = var_460
            arg16 = _mm256_sub_ps(
                _mm256_sub_ps(arg6, _mm256_mul_ps(temp0_204, _mm256_sub_ps(zmm3, temp0_205))), arg9)
            zmm0 = __vcmpps_ymmqq_ymmqq_memqq_immb(arg16, data_142fc94e0, 1)
            
            if (_mm256_movemask_ps(zmm0) != 0)
                float temp0_212[0x8] = _mm256_mul_ps(arg5, arg16)
                float temp0_213[0x8] = _mm256_mul_ps(arg14, arg16)
                zmm3 = _mm256_mul_ps(arg15, arg16)
                float temp0_215[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_212, var_380_1)
                float temp0_216[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_213, var_360_1)
                zmm3 = __vaddps_ymmqq_ymmqq_memqq(zmm3, var_340_1)
                var_380_1 = _mm256_maskstore_ps(zmm0, temp0_215)
                var_360_1 = _mm256_maskstore_ps(zmm0, temp0_216)
                var_340_1 = _mm256_maskstore_ps(zmm0, zmm3)
                arg5 = _mm256_add_ps(var_480_1, arg16)
                arg6 = _mm256_blendv_ps(var_480_1, arg5, zmm0)
                var_480_1 = arg6
            
            rsi_1 += 8
            rbx_2 += 0x60
        while (rsi_1 s< rdi_9)
        
        if (rsi_1 s< arg20)
            goto label_1400894f0
        
        zmm3 = var_440_2
        zmm5 = var_480_1
    zmm0[0].o = _mm256_extractf128_ps(zmm3[0].o, 1)
    arg5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
    zmm0[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg5[0].o)
    arg6[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(arg6[0].o, arg6[0].o)
    zmm0[0].o ^= arg6[0].o
    arg5[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(zmm3[0].o, arg5[0].o)
    arg5[0].o ^= arg6[0].o
    zmm0 = _mm256_insertf128_ps(arg5, zmm0[0].o, 1)
    arg5 = __vandps_ymmqq_ymmqq_memqq(zmm0, var_380_1)
    float temp0_559[0x8] = _mm256_hadd_ps(arg5, arg5)
    arg5 = _mm256_hadd_ps(temp0_559, temp0_559)
    arg6 = __vandps_ymmqq_ymmqq_memqq(zmm0, var_360_1)
    float temp0_562[0x8] = _mm256_hadd_ps(arg6, arg6)
    float temp0_563[0x8] = _mm256_hadd_ps(temp0_562, temp0_562)
    zmm3[0].o = _mm256_extractf128_ps(arg5[0].o, 1)
    arg5[0].o = arg5[0].o f+ zmm3[0].d
    zmm3 = __vandps_ymmqq_ymmqq_memqq(zmm0, var_340_1)
    zmm4[0].o = _mm256_extractf128_ps(temp0_563[0].o, 1)
    zmm3 = _mm256_hadd_ps(zmm3, zmm3)
    temp0_563[0].o = temp0_563[0].o f+ zmm4[0]
    zmm3 = _mm256_hadd_ps(zmm3, zmm3)
    arg5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, temp0_563[0].o, 0x10)
    temp0_563[0].o = _mm256_extractf128_ps(zmm3[0].o, 1)
    temp0_563[0].o = zmm3[0].o f+ temp0_563[0]
    arg5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, temp0_563[0].o, 0x20)
    temp0_563[0].o = __vmovsd_xmmdq_memq(*r15)
    temp0_563[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(temp0_563[0].o, r15[1].d, 0x20)
    arg5[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg5[0].o, temp0_563[0].o)
    *r15 = arg5[0]
    *(r15 + 4) = __vextractps_memd_xmmdq_immb(arg5[0].o, 1)
    r15[1].d = __vextractps_memd_xmmdq_immb(arg5[0].o, 2)
    zmm0 = _mm256_and_ps(zmm0, zmm5)
    zmm0 = _mm256_hadd_ps(zmm0, zmm0)
else
    int32_t rdi_4 = ((arg20 s>> 0x1f u>> 0x1d) + arg20) & 0xfffffff8
    zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
    float var_2e0_1[0x8] = zmm0
    float var_300_1[0x8] = zmm0
    float var_320_1[0x8] = zmm0
    float var_480[0x8]
    int32_t rsi
    
    if (rdi_4 s<= 0)
        zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        var_480 = zmm0
        rsi = 0
        
        if (0 s>= arg20)
            zmm4 = var_480
        else
        label_140089045:
            zmm0[0].o = zx.o(rsi)
            zmm0[0].o = __vpshufd_xmmdq_xmmdq_immb(zmm0[0].o, 0)
            arg5[0].o = __vpaddd_xmmdq_xmmdq_memdq(zmm0[0].o, data_142e11d00)
            zmm0[0].o = __vpaddd_xmmdq_xmmdq_memdq(zmm0[0].o, data_142fc9500)
            arg6[0].o = zx.o(arg20)
            arg6[0].o = __vpshufd_xmmdq_xmmdq_immb(arg6[0].o, 0)
            arg11[0].o = __vpcmpgtd_xmmdq_xmmdq_xmmdq(arg6[0].o, zmm0[0].o)
            arg10[0].o = __vpcmpgtd_xmmdq_xmmdq_xmmdq(arg6[0].o, arg5[0].o)
            arg12 = _mm256_insertf128_ps(arg10, arg11[0].o, 1)
            zmm5[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
            zmm0[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
            zmm5[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
            arg5[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(zmm5[0].o, arg5[0].o)
            zmm0 = _mm256_and_ps(arg12, _mm256_insertf128_ps(arg5, zmm0[0].o, 1))
            int64_t r14_1 = sx.q(zmm0[0])
            void* r13_1 = arg4 + (r14_1 << 2)
            int32_t temp0_237 = __vpextrd_gpr32d_xmmdq_immb(zmm0[0].o, 1)
            var_460[0].q = arg1
            int64_t r15_1 = sx.q(temp0_237)
            void* rbx_3 = arg4 + (r15_1 << 2)
            zmm5[0].o = _mm256_extractf128_ps(zmm0[0].o, 1)
            int64_t r10_1 = sx.q(zmm5[0])
            void* rdi_10 = arg4 + (r10_1 << 2)
            int64_t r12_1 = sx.q(__vpextrd_gpr32d_xmmdq_immb(zmm5[0].o, 1))
            void* rsi_2 = arg4 + (r12_1 << 2)
            arg7 = __vandps_ymmqq_ymmqq_memqq(arg12, data_142fc9520)
            arg8[0].o = _mm256_extractf128_ps(arg7[0].o, 1)
            arg5[0].o = *(zx.q(arg8[0].d) + rdi_10)
            arg9 = __vandps_ymmqq_ymmqq_memqq(arg12, data_142fc9540)
            zmm3[0].o = _mm256_extractf128_ps(arg9[0].o, 1)
            zmm4[0].o = *(zx.q(zmm3[0].d) + rdi_10)
            int32_t temp0_244 = __vpextrd_gpr32d_xmmdq_immb(zmm5[0].o, 2)
            uint64_t rdi_11 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg8[0].o, 1))
            arg5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, *(rdi_11 + rsi_2), 0x10)
            float* rdi_12 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm3[0].o, 1))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rdi_12 + rsi_2), 0x10)
            int64_t rax_13 = sx.q(temp0_244)
            void* rsi_3 = arg4 + (rax_13 << 2)
            uint64_t rdi_13 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg8[0].o, 2))
            arg5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, *(rdi_13 + rsi_3), 0x20)
            float* rdi_14 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm3[0].o, 2))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rdi_14 + rsi_3), 0x20)
            int64_t rsi_5 = sx.q(__vpextrd_gpr32d_xmmdq_immb(zmm5[0].o, 3))
            zmm5[0].o = *(zx.q(arg7[0]) + r13_1)
            arg6[0].o = *(zx.q(arg9[0]) + r13_1)
            void* rdi_17 = arg4 + (rsi_5 << 2)
            uint64_t rcx = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg8[0].o, 3))
            arg5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, *(rcx + rdi_17), 0x30)
            float* rcx_1 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm3[0].o, 3))
            zmm3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rcx_1 + rdi_17), 0x30)
            int32_t temp0_258 = __vpextrd_gpr32d_xmmdq_immb(zmm0[0].o, 2)
            uint64_t rdi_18 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg7[0].o, 1))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm5[0].o, *(rdi_18 + rbx_3), 0x10)
            float* rdi_19 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg9[0].o, 1))
            arg6[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg6[0].o, *(rdi_19 + rbx_3), 0x10)
            int64_t rcx_3 = sx.q(temp0_258)
            void* rdi_20 = arg4 + (rcx_3 << 2)
            uint64_t rbx_4 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg7[0].o, 2))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rbx_4 + rdi_20), 0x20)
            float* rbx_5 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg9[0].o, 2))
            arg6[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg6[0].o, *(rbx_5 + rdi_20), 0x20)
            zmm5[0].o = *(arg4 + (r14_1 << 2))
            arg8[0].o = *(arg4 + (r10_1 << 2))
            arg8[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg8[0].o, *(arg4 + (r12_1 << 2)), 0x10)
            arg8[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg8[0].o, *(arg4 + (rax_13 << 2)), 0x20)
            arg8[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg8[0].o, *(arg4 + (rsi_5 << 2)), 0x30)
            zmm5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm5[0].o, *(arg4 + (r15_1 << 2)), 0x10)
            zmm5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm5[0].o, *(arg4 + (rcx_3 << 2)), 0x20)
            int64_t rax_15 = sx.q(__vpextrd_gpr32d_xmmdq_immb(zmm0[0].o, 3))
            zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm5[0].o, *(arg4 + (rax_15 << 2)), 0x30)
            void* rax_16 = arg4 + (rax_15 << 2)
            uint64_t rcx_4 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg7[0].o, 3))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rcx_4 + rax_16), 0x30)
            float* rcx_5 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg9[0].o, 3))
            arg6[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg6[0].o, *(rcx_5 + rax_16), 0x30)
            arg7 = _mm256_insertf128_ps(zmm0, arg8[0].o, 1)
            arg5 = _mm256_insertf128_ps(zmm4, arg5[0].o, 1)
            arg6 = _mm256_insertf128_ps(arg6, zmm3[0].o, 1)
            zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
            zmm5 = _mm256_broadcast_ss(arg3[3])
            int32_t var_200_3[0x8] = zmm5
            int32_t var_1e0_3[0x8] = zmm5
            int32_t var_1c0_3[0x8] = zmm5
            int32_t var_1a0_3[0x8] = zmm5
            zmm3 = _mm256_broadcast_ss(arg3[1])
            float temp0_284[0x8] = _mm256_mul_ps(arg6, zmm3)
            arg8 = _mm256_broadcast_ss(arg3[2])
            float temp0_286[0x8] = _mm256_mul_ps(arg7, arg8)
            arg13 = _mm256_broadcast_ss(*arg3)
            arg8 = _mm256_mul_ps(arg5, arg8)
            float temp0_289[0x8] = _mm256_mul_ps(arg5, arg13)
            float temp0_290[0x8] = _mm256_sub_ps(temp0_284, arg8)
            float temp0_292[0x8] = _mm256_sub_ps(temp0_286, _mm256_mul_ps(arg6, arg13))
            float temp0_294[0x8] = _mm256_sub_ps(temp0_289, _mm256_mul_ps(arg7, zmm3))
            zmm3 = _mm256_add_ps(temp0_290, temp0_290)
            float temp0_296[0x8] = _mm256_add_ps(temp0_292, temp0_292)
            float temp0_297[0x8] = _mm256_add_ps(temp0_294, temp0_294)
            float temp0_298[0x8] = _mm256_broadcast_ss(arg3[1])
            float temp0_299[0x8] = _mm256_mul_ps(temp0_297, temp0_298)
            arg8 = _mm256_broadcast_ss(arg3[2])
            float temp0_301[0x8] = _mm256_mul_ps(zmm3, arg8)
            arg13 = _mm256_broadcast_ss(*arg3)
            float temp0_304[0x8] = _mm256_sub_ps(temp0_299, _mm256_mul_ps(temp0_296, arg8))
            arg8 = _mm256_sub_ps(temp0_301, _mm256_mul_ps(temp0_297, arg13))
            float temp0_307[0x8] = _mm256_mul_ps(zmm5, zmm0)
            float temp0_308[0x8] = _mm256_sub_ps(temp0_307, temp0_307)
            float temp0_309[0x8] = _mm256_add_ps(temp0_308, temp0_308)
            double var_400_3[0x4] = zmm3
            float var_3e0_3[0x8] = temp0_296
            float var_3c0_3[0x8] = temp0_297
            float var_3a0_3[0x8] = temp0_309
            arg13 = _mm256_mul_ps(temp0_296, arg13)
            float temp0_311[0x8] = _mm256_mul_ps(zmm5, temp0_296)
            float temp0_312[0x8] = _mm256_mul_ps(zmm5, temp0_297)
            float temp0_313[0x8] = _mm256_mul_ps(zmm5, temp0_309)
            zmm5 = _mm256_mul_ps(zmm5, zmm3)
            zmm3 = _mm256_sub_ps(arg13, _mm256_mul_ps(zmm3, temp0_298))
            float temp0_317[0x8] = __vaddps_ymmqq_ymmqq_memqq(zmm5, arg7)
            float var_180_1[0x8] = temp0_317
            float temp0_318[0x8] = _mm256_add_ps(temp0_317, temp0_304)
            float temp0_319[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_311, arg5)
            float var_160_1[0x8] = temp0_319
            float temp0_320[0x8] = _mm256_add_ps(temp0_319, arg8)
            zmm5 = _mm256_add_ps(temp0_318, _mm256_broadcast_ss(arg3[4]))
            arg7 = _mm256_add_ps(temp0_320, _mm256_broadcast_ss(arg3[5]))
            float temp0_325[0x8] = _mm256_broadcast_ss(arg3[6])
            float temp0_326[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_312, arg6)
            zmm3 = _mm256_add_ps(temp0_326, zmm3)
            arg8 = _mm256_add_ps(zmm3, temp0_325)
            float temp0_330[0x8] = _mm256_sub_ps(zmm5, _mm256_broadcast_ss(*arg2))
            float temp0_332[0x8] = _mm256_add_ps(_mm256_mul_ps(temp0_330, temp0_330), zmm0)
            float temp0_334[0x8] = _mm256_sub_ps(arg7, _mm256_broadcast_ss(arg2[1]))
            float temp0_336[0x8] = _mm256_add_ps(temp0_332, _mm256_mul_ps(temp0_334, temp0_334))
            float temp0_338[0x8] = _mm256_sub_ps(arg8, _mm256_broadcast_ss(arg2[2]))
            float temp0_340[0x8] = _mm256_add_ps(temp0_336, _mm256_mul_ps(temp0_338, temp0_338))
            float temp0_341[0x8] = _mm256_rsqrt_ps(temp0_340)
            float var_140_1[0x8] = temp0_326
            float temp0_343[0x8] = _mm256_mul_ps(temp0_341, _mm256_mul_ps(temp0_340, temp0_341))
            float temp0_346[0x8] = __vmulps_ymmqq_ymmqq_memqq(
                _mm256_mul_ps(temp0_341, _mm256_sub_ps(data_142fc9480, temp0_343)), data_142fc94a0)
            float temp0_347[0x8] = _mm256_rcp_ps(temp0_346)
            float temp0_348[0x8] = _mm256_mul_ps(temp0_347, temp0_346)
            arg6 = data_142fc94c0
            float temp0_350[0x8] = _mm256_mul_ps(temp0_347, _mm256_sub_ps(arg6, temp0_348))
            float temp0_352[0x8] = _mm256_sub_ps(_mm256_broadcast_ss(*var_460[0].q), temp0_350)
            float temp0_353[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_313, zmm0)
            arg6[0].o = arg17
            arg6[0].o = _mm_permute_ps(arg6[0].o, 0)
            arg6 = _mm256_insertf128_ps(arg6, arg6[0].o, 1)
            float temp0_356[0x8] = _mm256_sub_ps(temp0_352, arg6)
            arg9 = __vcmpps_ymmqq_ymmqq_memqq_immb(temp0_356, data_142fc94e0, 1)
            float var_120_1[0x8] = temp0_353
            temp0_353[0].o = _mm256_extractf128_ps(arg9[0].o, 1)
            temp0_353[0].o &= arg11[0].o
            arg6[0].o = __vandps_xmmdq_xmmdq_xmmdq(arg9[0].o, arg10[0].o)
            zmm3[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(arg6[0].o, temp0_353[0].o)
            temp0_353[0].o = __vpacksswb_xmmdq_xmmdq_xmmdq(zmm3[0].o, temp0_356[0].o)
            
            if (__vpmovmskb_gpr32d_xmmdq(temp0_353[0].o) == 0)
                r15 = arg19
                zmm4 = var_480
            else
                float temp0_363[0x8] = _mm256_mul_ps(zmm5, temp0_356)
                float temp0_364[0x8] = _mm256_mul_ps(arg7, temp0_356)
                float temp0_365[0x8] = _mm256_mul_ps(arg8, temp0_356)
                float temp0_366[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_363, var_320_1)
                float temp0_367[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_364, var_300_1)
                float temp0_368[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_365, var_2e0_1)
                int32_t temp0_369[0x8] = _mm256_and_ps(arg9, arg12)
                var_320_1 = _mm256_maskstore_ps(temp0_369, temp0_366)
                var_300_1 = _mm256_maskstore_ps(temp0_369, temp0_367)
                var_2e0_1 = _mm256_maskstore_ps(temp0_369, temp0_368)
                float temp0_373[0x8] = _mm256_add_ps(var_480, temp0_356)
                temp0_366[0].o = __vpmovsxwd_xmmdq_xmmq(zmm3[0])
                temp0_367[0].o = __vpshufd_xmmdq_xmmdq_immb(zmm3[0].o, 0x4e)
                temp0_367[0].o = __vpmovsxwd_xmmdq_xmmq(temp0_367[0].q)
                zmm4 = _mm256_blendv_ps(var_480, temp0_373, 
                    _mm256_insertf128_ps(temp0_366, temp0_367[0].o, 1))
                r15 = arg19
    else
        arg9 = _mm256_broadcast_ss(arg3[3])
        arg12[0].o = *arg3
        arg5[0].o = arg3[1]
        zmm0[0].o = arg3[2]
        var_460 = _mm256_broadcast_ss(*arg1)
        float temp0_4[0x8] = _mm256_broadcast_ss(*arg2)
        float temp0_5[0x8] = _mm256_broadcast_ss(arg2[1])
        float temp0_6[0x8] = _mm256_broadcast_ss(arg2[2])
        arg8[0].o = _mm_permute_ps(zmm3[0].o, 0)
        arg6 = _mm256_insertf128_ps(arg8, arg8[0].o, 1)
        float var_440_1[0x8] = arg6
        int32_t rbx_1 = 0
        arg5[0].o = _mm_permute_ps(arg5[0].o, 0)
        arg10 = _mm256_insertf128_ps(arg5, arg5[0].o, 1)
        zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0)
        arg11 = _mm256_insertf128_ps(zmm0, zmm0[0].o, 1)
        zmm0[0].o = _mm_permute_ps(arg12[0].o, 0)
        arg12 = _mm256_insertf128_ps(zmm0, zmm0[0].o, 1)
        rsi = 0
        zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        var_480 = zmm0
        
        do
            int64_t rax_2 = sx.q(rbx_1)
            zmm0[0].o = *(arg4 + rax_2)
            arg5[0].o = *(arg4 + rax_2 + 0x10)
            zmm3[0].o = *(arg4 + rax_2 + 0x20)
            zmm4[0].o = *(arg4 + rax_2 + 0x30)
            zmm5[0].o = *(arg4 + rax_2 + 0x40)
            arg7[0].o = *(arg4 + rax_2 + 0x50)
            arg8[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg5[0].o, 0xec)
            arg6[0].o = _mm_permute_ps(zmm3[0].o, 0x44)
            arg13[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg8[0].o, arg6[0].o, 8)
            arg8[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm5[0].o, 0xec)
            arg6[0].o = _mm_permute_ps(arg7[0].o, 0x44)
            arg14[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg8[0].o, arg6[0].o, 8)
            arg8[0].o = _mm_permute_ps(zmm0[0].o, 0xe5)
            arg6[0].o = _mm_permute_ps(arg5[0].o, 0xf0)
            arg6[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg6[0].o, arg8[0].o, 1)
            arg8[0].o = _mm_permute_ps(zmm3[0].o, 0xa4)
            arg15[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg6[0].o, arg8[0].o, 8)
            arg8[0].o = _mm_permute_ps(zmm4[0].o, 0xe5)
            arg6[0].o = _mm_permute_ps(zmm5[0].o, 0xf0)
            arg6[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg6[0].o, arg8[0].o, 1)
            arg8[0].o = _mm_permute_ps(arg7[0].o, 0xa4)
            arg6[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg6[0].o, arg8[0].o, 8)
            zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0x4e)
            zmm0[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg5[0].o, 2)
            zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm3[0].o, 0xc4)
            arg5[0].o = _mm_permute_ps(zmm4[0].o, 0x4e)
            arg5[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm5[0].o, 2)
            arg5[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, arg7[0].o, 0xc4)
            double temp0_38[0x4] = _mm256_insertf128_ps(arg13, arg14[0].o, 1)
            arg6 = _mm256_insertf128_ps(arg15, arg6[0].o, 1)
            zmm0 = _mm256_insertf128_ps(zmm0, arg5[0].o, 1)
            arg7[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg7[0].o, arg7[0].o)
            float var_3a0_1[0x8] = arg7
            float temp0_42[0x8] = _mm256_mul_ps(arg10, zmm0)
            float temp0_43[0x8] = _mm256_mul_ps(arg11, temp0_38)
            zmm5 = _mm256_mul_ps(arg12, arg6)
            float temp0_46[0x8] = _mm256_sub_ps(temp0_42, _mm256_mul_ps(arg11, arg6))
            float temp0_48[0x8] = _mm256_sub_ps(temp0_43, _mm256_mul_ps(arg12, zmm0))
            float temp0_50[0x8] = _mm256_sub_ps(zmm5, _mm256_mul_ps(arg10, temp0_38))
            float temp0_51[0x8] = _mm256_add_ps(temp0_46, temp0_46)
            float temp0_52[0x8] = _mm256_add_ps(temp0_48, temp0_48)
            float temp0_53[0x8] = _mm256_add_ps(temp0_50, temp0_50)
            zmm3 = __vaddps_ymmqq_ymmqq_memqq(_mm256_mul_ps(arg9, temp0_51), temp0_38)
            float temp0_58[0x8] =
                _mm256_sub_ps(_mm256_mul_ps(temp0_53, arg10), _mm256_mul_ps(temp0_52, arg11))
            zmm5 = _mm256_mul_ps(temp0_51, arg11)
            zmm3 = _mm256_add_ps(zmm3, temp0_58)
            float temp0_62[0x8] = _mm256_sub_ps(zmm5, _mm256_mul_ps(temp0_53, arg12))
            zmm5 = __vaddps_ymmqq_ymmqq_memqq(_mm256_mul_ps(arg9, temp0_52), arg6)
            zmm4 = _mm256_add_ps(zmm5, temp0_62)
            float var_200_1[0x8] = arg9
            float var_1e0_1[0x8] = arg9
            float var_1c0_1[0x8] = arg9
            float var_1a0_1[0x8] = arg9
            float temp0_71[0x8] = _mm256_add_ps(
                __vaddps_ymmqq_ymmqq_memqq(_mm256_mul_ps(arg9, temp0_53), zmm0), 
                _mm256_sub_ps(_mm256_mul_ps(temp0_52, arg12), _mm256_mul_ps(temp0_51, arg10)))
            arg16 = _mm256_add_ps(zmm3, _mm256_broadcast_ss(arg3[4]))
            arg13 = _mm256_add_ps(zmm4, _mm256_broadcast_ss(arg3[5]))
            arg14 = _mm256_add_ps(temp0_71, _mm256_broadcast_ss(arg3[6]))
            float temp0_78[0x8] = __vsubps_ymmqq_ymmqq_memqq(arg16, temp0_4)
            float temp0_79[0x8] = __vsubps_ymmqq_ymmqq_memqq(arg13, temp0_5)
            float temp0_83[0x8] = _mm256_add_ps(
                _mm256_add_ps(_mm256_mul_ps(temp0_78, temp0_78), arg7), 
                _mm256_mul_ps(temp0_79, temp0_79))
            float temp0_84[0x8] = __vsubps_ymmqq_ymmqq_memqq(arg14, temp0_6)
            float temp0_86[0x8] = _mm256_add_ps(temp0_83, _mm256_mul_ps(temp0_84, temp0_84))
            float temp0_87[0x8] = _mm256_rsqrt_ps(temp0_86)
            float temp0_89[0x8] = _mm256_mul_ps(temp0_87, _mm256_mul_ps(temp0_86, temp0_87))
            float temp0_92[0x8] = __vmulps_ymmqq_ymmqq_memqq(
                _mm256_mul_ps(temp0_87, _mm256_sub_ps(data_142fc9480, temp0_89)), data_142fc94a0)
            float temp0_93[0x8] = _mm256_rcp_ps(temp0_92)
            float temp0_94[0x8] = _mm256_mul_ps(temp0_93, temp0_92)
            arg6 = data_142fc94c0
            arg5 = var_460
            arg15 = __vsubps_ymmqq_ymmqq_memqq(
                _mm256_sub_ps(arg5, _mm256_mul_ps(temp0_93, _mm256_sub_ps(arg6, temp0_94))), 
                var_440_1)
            zmm0 = __vcmpps_ymmqq_ymmqq_memqq_immb(arg15, data_142fc94e0, 1)
            
            if (_mm256_movemask_ps(zmm0) != 0)
                float temp0_101[0x8] = _mm256_mul_ps(arg16, arg15)
                float temp0_102[0x8] = _mm256_mul_ps(arg13, arg15)
                zmm3 = _mm256_mul_ps(arg14, arg15)
                float temp0_104[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_101, var_320_1)
                float temp0_105[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_102, var_300_1)
                zmm3 = __vaddps_ymmqq_ymmqq_memqq(zmm3, var_2e0_1)
                var_320_1 = _mm256_maskstore_ps(zmm0, temp0_104)
                var_300_1 = _mm256_maskstore_ps(zmm0, temp0_105)
                var_2e0_1 = _mm256_maskstore_ps(zmm0, zmm3)
                arg5 = _mm256_add_ps(var_480, arg15)
                arg6 = _mm256_blendv_ps(var_480, arg5, zmm0)
                var_480 = arg6
            
            rsi += 8
            rbx_1 += 0x60
        while (rsi s< rdi_4)
        
        if (rsi s< arg20)
            goto label_140089045
        
        zmm4 = var_480
    float temp0_535[0x8] = _mm256_hadd_ps(var_320_1, var_320_1)
    float temp0_536[0x8] = _mm256_hadd_ps(temp0_535, temp0_535)
    zmm3[0].o = _mm256_extractf128_ps(temp0_536[0].o, 1)
    float temp0_538[0x8] = _mm256_hadd_ps(var_300_1, var_300_1)
    temp0_536[0].o = temp0_536[0].o f+ zmm3[0].d
    arg5 = _mm256_hadd_ps(temp0_538, temp0_538)
    zmm3[0].o = _mm256_extractf128_ps(arg5[0].o, 1)
    float temp0_541[0x8] = _mm256_hadd_ps(var_2e0_1, var_2e0_1)
    arg5[0].o = arg5[0].o f+ zmm3[0].d
    float temp0_542[0x8] = _mm256_hadd_ps(temp0_541, temp0_541)
    temp0_536[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(temp0_536[0].o, arg5[0].o, 0x10)
    arg5[0].o = _mm256_extractf128_ps(temp0_542[0].o, 1)
    arg5[0].o = temp0_542[0].o f+ arg5[0]
    temp0_536[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(temp0_536[0].o, arg5[0].o, 0x20)
    arg5[0].o = __vmovsd_xmmdq_memq(*r15)
    arg5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, r15[1].d, 0x20)
    temp0_536[0].o = __vaddps_xmmdq_xmmdq_xmmdq(temp0_536[0].o, arg5[0].o)
    *r15 = temp0_536[0]
    *(r15 + 4) = __vextractps_memd_xmmdq_immb(temp0_536[0].o, 1)
    r15[1].d = __vextractps_memd_xmmdq_immb(temp0_536[0].o, 2)
    zmm0 = _mm256_hadd_ps(zmm4, zmm4)
float temp0_579[0x8] = _mm256_hadd_ps(zmm0, zmm0)
arg5[0].o = _mm256_extractf128_ps(temp0_579[0].o, 1)
temp0_579[0].o = temp0_579[0].o f+ arg5[0]
temp0_579[0].o = temp0_579[0].o f+ *arg18
*arg18 = temp0_579[0]
arg7[0].o = var_e8
arg8[0].o = var_d8
arg9[0].o = var_c8
arg10[0].o = var_b8
arg11[0].o = var_a8
arg12[0].o = var_98
arg13[0].o = var_88
arg14[0].o = var_78
arg15[0].o = var_68
arg16[0].o = var_58
_mm256_zeroupper()
return arg18
