// 函数: sub_140086e10
// 地址: 0x140086e10
// 来自: E:\Embers\Embers\Binaries\Win64\Embers-Win64-Shipping.exe

int128_t var_58 = arg16[0].o
int128_t var_68 = arg15[0].o
int128_t var_78 = arg14[0].o
int128_t var_88 = arg13[0].o
int128_t var_98 = arg12[0].o
int128_t var_a8 = arg11[0].o
int128_t var_b8 = arg10[0].o
int128_t var_c8 = arg9[0].o
int128_t var_d8 = arg8[0].o
int128_t var_e8 = arg7[0].o
float zmm0[0x8] = *arg21
double* r11 = arg19
double zmm3[0x4]
zmm3[0].o = arg17
float var_4c0[0x8]
float zmm4[0x8]
float zmm5[0x8]

if (_mm256_movemask_ps(zmm0) != 0xff)
    int32_t rdi_9 = ((arg20 s>> 0x1f u>> 0x1d) + arg20) & 0xfffffff8
    arg5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
    double var_3c0_1[0x4] = arg5
    double var_3e0_1[0x4] = arg5
    double var_400_1[0x4] = arg5
    float var_340_1[0x8] = zmm0
    float var_4e0_1[0x8]
    float var_4a0[0x8]
    int32_t rsi_1
    
    if (rdi_9 s<= 0)
        arg5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
        var_4e0_1 = arg5
        rsi_1 = 0
        
        if (0 s>= arg20)
            zmm3 = var_340_1
            zmm5 = var_4e0_1
        else
        label_1400879f8:
            zmm0[0].o = zx.o(rsi_1)
            zmm0[0].o = __vpshufd_xmmdq_xmmdq_immb(zmm0[0].o, 0)
            arg5[0].o = __vpaddd_xmmdq_xmmdq_memdq(zmm0[0].o, data_142e11d00)
            zmm0[0].o = __vpaddd_xmmdq_xmmdq_memdq(zmm0[0].o, data_142fc9500)
            arg6[0].o = zx.o(arg20)
            arg6[0].o = __vpshufd_xmmdq_xmmdq_immb(arg6[0].o, 0)
            arg13[0].o = __vpcmpgtd_xmmdq_xmmdq_xmmdq(arg6[0].o, zmm0[0].o)
            arg14[0].o = __vpcmpgtd_xmmdq_xmmdq_xmmdq(arg6[0].o, arg5[0].o)
            double temp0_356[0x4] = _mm256_insertf128_ps(arg14, arg13[0].o, 1)
            arg6[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
            zmm0[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(arg6[0].o, zmm0[0].o)
            arg6[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
            arg5[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(arg6[0].o, arg5[0].o)
            zmm0 = _mm256_insertf128_ps(arg5, zmm0[0].o, 1)
            arg7 = _mm256_and_ps(temp0_356, zmm0)
            int64_t rax_22 = sx.q(arg7[0])
            var_4a0[0].q = rax_22
            void* rbx_9 = arg4 + (rax_22 << 2)
            int64_t r15_2 = sx.q(__vpextrd_gpr32d_xmmdq_immb(arg7[0].o, 1))
            void* r12_2 = arg4 + (r15_2 << 2)
            arg8[0].o = _mm256_extractf128_ps(arg7[0].o, 1)
            var_4c0[0].q = arg1
            int64_t r11_2 = sx.q(arg8[0])
            void* r10_1 = arg4 + (r11_2 << 2)
            int64_t r13_2 = sx.q(__vpextrd_gpr32d_xmmdq_immb(arg8[0].o, 1))
            void* rsi_6 = arg4 + (r13_2 << 2)
            arg9 = __vandps_ymmqq_ymmqq_memqq(temp0_356, data_142fc9520)
            zmm0[0].o = _mm256_extractf128_ps(arg9[0].o, 1)
            arg5[0].o = *(zx.q(zmm0[0]) + r10_1)
            arg10 = __vandps_ymmqq_ymmqq_memqq(temp0_356, data_142fc9540)
            arg6[0].o = _mm256_extractf128_ps(arg10[0].o, 1)
            zmm4[0].o = *(zx.q(arg6[0]) + r10_1)
            int32_t temp0_370 = __vpextrd_gpr32d_xmmdq_immb(arg8[0].o, 2)
            uint64_t rdi_20 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm0[0].o, 1))
            arg5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, *(rdi_20 + rsi_6), 0x10)
            float* rdi_21 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg6[0].o, 1))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rdi_21 + rsi_6), 0x10)
            int64_t rcx_8 = sx.q(temp0_370)
            void* rdi_22 = arg4 + (rcx_8 << 2)
            uint64_t rsi_7 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm0[0].o, 2))
            arg5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, *(rsi_7 + rdi_22), 0x20)
            float* rsi_8 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg6[0].o, 2))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rsi_8 + rdi_22), 0x20)
            int64_t rdi_24 = sx.q(__vpextrd_gpr32d_xmmdq_immb(arg8[0].o, 3))
            arg8[0].o = *(zx.q(arg9[0]) + rbx_9)
            zmm5[0].o = *(zx.q(arg10[0].d) + rbx_9)
            void* rsi_11 = arg4 + (rdi_24 << 2)
            uint64_t rbx_10 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm0[0].o, 3))
            zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, *(rbx_10 + rsi_11), 0x30)
            float* rbx_11 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg6[0].o, 3))
            arg5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rbx_11 + rsi_11), 0x30)
            int32_t temp0_384 = __vpextrd_gpr32d_xmmdq_immb(arg7[0].o, 2)
            uint64_t rsi_12 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg9[0].o, 1))
            arg6[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg8[0].o, *(rsi_12 + r12_2), 0x10)
            uint64_t rsi_13 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg10[0].o, 1))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm5[0].o, *(rsi_13 + r12_2), 0x10)
            int64_t rbx_13 = sx.q(temp0_384)
            void* rsi_14 = arg4 + (rbx_13 << 2)
            uint64_t rax_25 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg9[0].o, 2))
            arg6[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg6[0].o, *(rax_25 + rsi_14), 0x20)
            uint64_t rax_26 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg10[0].o, 2))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rax_26 + rsi_14), 0x20)
            zmm5[0].o = *(arg4 + (var_4a0[0].q << 2))
            arg8[0].o = *(arg4 + (r11_2 << 2))
            arg8[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg8[0].o, *(arg4 + (r13_2 << 2)), 0x10)
            arg8[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg8[0].o, *(arg4 + (rcx_8 << 2)), 0x20)
            arg8[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg8[0].o, *(arg4 + (rdi_24 << 2)), 0x30)
            zmm5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm5[0].o, *(arg4 + (r15_2 << 2)), 0x10)
            zmm5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm5[0].o, *(arg4 + (rbx_13 << 2)), 0x20)
            int64_t rax_29 = sx.q(__vpextrd_gpr32d_xmmdq_immb(arg7[0].o, 3))
            zmm5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm5[0].o, *(arg4 + (rax_29 << 2)), 0x30)
            void* rax_30 = arg4 + (rax_29 << 2)
            uint64_t rcx_9 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg9[0].o, 3))
            arg6[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg6[0].o, *(rcx_9 + rax_30), 0x30)
            float* rcx_10 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg10[0].o, 3))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rcx_10 + rax_30), 0x30)
            arg8 = _mm256_insertf128_ps(zmm5, arg8[0].o, 1)
            arg6 = _mm256_insertf128_ps(arg6, zmm0[0].o, 1)
            double temp0_406[0x4] = _mm256_insertf128_ps(zmm4, arg5[0].o, 1)
            int32_t var_2a0_2[0x8] = arg8
            float temp0_407[0x8] = _mm256_broadcast_ss(arg3[3])
            float temp0_408[0x8] = _mm256_broadcast_ss(arg3[1])
            float temp0_409[0x8] = _mm256_mul_ps(temp0_406, temp0_408)
            float temp0_410[0x8] = _mm256_broadcast_ss(arg3[2])
            arg11 = _mm256_mul_ps(arg8, temp0_410)
            arg12 = _mm256_broadcast_ss(*arg3)
            float temp0_413[0x8] = _mm256_mul_ps(arg6, arg12)
            arg10 = _mm256_sub_ps(temp0_409, _mm256_mul_ps(arg6, temp0_410))
            arg11 = _mm256_sub_ps(arg11, _mm256_mul_ps(temp0_406, arg12))
            arg12 = _mm256_broadcast_ss(arg3[1])
            arg5 = _mm256_broadcast_ss(arg3[2])
            float temp0_420[0x8] = _mm256_broadcast_ss(*arg3)
            float temp0_421[0x8] = _mm256_broadcast_ss(arg3[4])
            float temp0_422[0x8] = _mm256_broadcast_ss(arg3[5])
            arg15 = _mm256_broadcast_ss(arg3[6])
            float temp0_425[0x8] = _mm256_sub_ps(temp0_413, _mm256_mul_ps(arg8, temp0_408))
            arg8 = _mm256_add_ps(arg10, arg10)
            float temp0_427[0x8] = _mm256_add_ps(arg11, arg11)
            float temp0_428[0x8] = _mm256_add_ps(temp0_425, temp0_425)
            arg10 = _mm256_mul_ps(temp0_428, arg12)
            arg11 = _mm256_mul_ps(arg8, arg5)
            arg5 = _mm256_sub_ps(arg10, _mm256_mul_ps(temp0_427, arg5))
            arg10 = _mm256_sub_ps(arg11, _mm256_mul_ps(temp0_428, temp0_420))
            arg11[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg11[0].o, arg11[0].o)
            double var_240_2[0x4] = arg11
            float var_220_4[0x8] = temp0_407
            float var_200_4[0x8] = temp0_407
            float var_1e0_4[0x8] = temp0_407
            float var_1c0_4[0x8] = temp0_407
            arg11 = _mm256_mul_ps(temp0_407, arg11)
            arg11 = _mm256_sub_ps(arg11, arg11)
            arg11 = _mm256_add_ps(arg11, arg11)
            int32_t var_480_4[0x8] = arg8
            float var_460_4[0x8] = temp0_427
            float var_440_4[0x8] = temp0_428
            double var_420_4[0x4] = arg11
            float temp0_439[0x8] = _mm256_mul_ps(temp0_427, temp0_420)
            float temp0_440[0x8] = _mm256_mul_ps(temp0_407, temp0_427)
            float temp0_441[0x8] = _mm256_mul_ps(temp0_407, temp0_428)
            arg11 = _mm256_mul_ps(temp0_407, arg11)
            float temp0_444[0x8] =
                __vaddps_ymmqq_ymmqq_memqq(_mm256_mul_ps(temp0_407, arg8), var_2a0_2)
            float temp0_445[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_440, arg6)
            float temp0_446[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_441, temp0_406)
            arg11 = __vaddps_ymmqq_ymmqq_memqq(arg11, var_240_2)
            float var_180_2[0x8] = temp0_444
            float temp0_449[0x8] = _mm256_sub_ps(temp0_439, _mm256_mul_ps(arg8, arg12))
            arg5 = _mm256_add_ps(temp0_444, arg5)
            float temp0_451[0x8] = _mm256_broadcast_ss(*arg2)
            arg8 = _mm256_broadcast_ss(arg2[1])
            arg12 = _mm256_broadcast_ss(arg2[2])
            float var_160_2[0x8] = temp0_445
            float var_140_2[0x8] = temp0_446
            arg9 = _mm256_add_ps(temp0_445, arg10)
            float temp0_455[0x8] = _mm256_add_ps(temp0_446, temp0_449)
            zmm0 = _mm256_add_ps(arg5, temp0_421)
            arg5 = _mm256_add_ps(arg9, temp0_422)
            arg6 = _mm256_add_ps(temp0_455, arg15)
            float temp0_459[0x8] = _mm256_sub_ps(zmm0, temp0_451)
            float temp0_460[0x8] = _mm256_sub_ps(arg5, arg8)
            float temp0_461[0x8] = _mm256_sub_ps(arg6, arg12)
            float* rax_31 = var_4c0[0].q
            float temp0_466[0x8] = _mm256_add_ps(
                _mm256_mul_ps(temp0_459, _mm256_broadcast_ss(*rax_31)), 
                _mm256_mul_ps(temp0_460, _mm256_broadcast_ss(rax_31[1])))
            float temp0_468[0x8] = _mm256_mul_ps(temp0_461, _mm256_broadcast_ss(rax_31[2]))
            zmm4 = _mm256_add_ps(temp0_466, temp0_468)
            temp0_468[0].o = arg17
            temp0_468[0].o = _mm_permute_ps(temp0_468[0].o, 0)
            zmm5 = _mm256_insertf128_ps(temp0_468, temp0_468[0].o, 1)
            arg7 = _mm256_sub_ps(zmm4, zmm5)
            arg8 = __vcmpps_ymmqq_ymmqq_memqq_immb(arg7, data_142fc94e0, 1)
            double var_120_2[0x4] = arg11
            zmm4[0].o = _mm256_extractf128_ps(arg8[0].o, 1)
            zmm4[0].o &= arg13[0].o
            zmm5[0].o = __vandps_xmmdq_xmmdq_xmmdq(arg8[0].o, arg14[0].o)
            zmm4[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm4[0].o)
            zmm5[0].o = __vpacksswb_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
            
            if (__vpmovmskb_gpr32d_xmmdq(zmm5[0].o) == 0)
                r11 = arg19
                zmm3 = var_340_1
                zmm5 = var_4e0_1
            else
                float temp0_479[0x8] = _mm256_mul_ps(zmm0, arg7)
                arg5 = _mm256_mul_ps(arg5, arg7)
                float temp0_481[0x8] = _mm256_mul_ps(arg6, arg7)
                float temp0_482[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_479, var_400_1)
                arg5 = __vaddps_ymmqq_ymmqq_memqq(arg5, var_3e0_1)
                arg6 = __vaddps_ymmqq_ymmqq_memqq(temp0_481, var_3c0_1)
                zmm3 = _mm256_and_ps(arg8, temp0_356)
                var_400_1 = _mm256_maskstore_ps(zmm3, temp0_482)
                var_3e0_1 = _mm256_maskstore_ps(zmm3, arg5)
                var_3c0_1 = _mm256_maskstore_ps(zmm3, arg6)
                zmm0 = _mm256_add_ps(var_4e0_1, arg7)
                arg5[0].o = __vpmovsxwd_xmmdq_xmmq(zmm4[0].q)
                arg6[0].o = __vpshufd_xmmdq_xmmdq_immb(zmm4[0].o, 0x4e)
                arg6[0].o = __vpmovsxwd_xmmdq_xmmq(arg6[0].q)
                arg5 = _mm256_insertf128_ps(arg5, arg6[0].o, 1)
                zmm5 = _mm256_blendv_ps(var_4e0_1, zmm0, arg5)
                r11 = arg19
                zmm3 = var_340_1
    else
        arg9 = _mm256_broadcast_ss(arg3[3])
        zmm0[0].o = *arg3
        arg5[0].o = arg3[1]
        arg6[0].o = arg3[2]
        var_4c0 = _mm256_broadcast_ss(*arg2)
        var_4a0 = _mm256_broadcast_ss(arg2[1])
        float temp0_106[0x8] = _mm256_broadcast_ss(arg2[2])
        float temp0_107[0x8] = _mm256_broadcast_ss(*arg1)
        float temp0_108[0x8] = _mm256_broadcast_ss(arg1[1])
        zmm4 = _mm256_broadcast_ss(arg1[2])
        float var_320_2[0x8] = zmm4
        arg11[0].o = _mm_permute_ps(zmm3[0].o, 0)
        zmm3 = _mm256_insertf128_ps(arg11, arg11[0].o, 1)
        double var_1a0_1[0x4] = zmm3
        int32_t rbx_2 = 0
        arg5[0].o = _mm_permute_ps(arg5[0].o, 0)
        arg13 = _mm256_insertf128_ps(arg5, arg5[0].o, 1)
        arg5[0].o = _mm_permute_ps(arg6[0].o, 0)
        arg14 = _mm256_insertf128_ps(arg5, arg5[0].o, 1)
        zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0)
        arg15 = _mm256_insertf128_ps(zmm0, zmm0[0].o, 1)
        rsi_1 = 0
        zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        var_4e0_1 = zmm0
        
        do
            int64_t rax_4 = sx.q(rbx_2)
            zmm0[0].o = *(arg4 + rax_4)
            arg5[0].o = *(arg4 + rax_4 + 0x10)
            arg6[0].o = *(arg4 + rax_4 + 0x20)
            zmm4[0].o = *(arg4 + rax_4 + 0x30)
            zmm5[0].o = *(arg4 + rax_4 + 0x40)
            arg7[0].o = *(arg4 + rax_4 + 0x50)
            arg8[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg5[0].o, 0xec)
            zmm3[0].o = _mm_permute_ps(arg6[0].o, 0x44)
            arg12[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg8[0].o, zmm3[0].o, 8)
            arg8[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm5[0].o, 0xec)
            zmm3[0].o = _mm_permute_ps(arg7[0].o, 0x44)
            arg16[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg8[0].o, zmm3[0].o, 8)
            arg8[0].o = _mm_permute_ps(zmm0[0].o, 0xe5)
            zmm3[0].o = _mm_permute_ps(arg5[0].o, 0xf0)
            zmm3[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, arg8[0].o, 1)
            arg8[0].o = _mm_permute_ps(arg6[0].o, 0xa4)
            arg11[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, arg8[0].o, 8)
            arg8[0].o = _mm_permute_ps(zmm4[0].o, 0xe5)
            arg10[0].o = _mm_permute_ps(zmm5[0].o, 0xf0)
            arg8[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg10[0].o, arg8[0].o, 1)
            zmm3[0].o = _mm_permute_ps(arg7[0].o, 0xa4)
            zmm3[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg8[0].o, zmm3[0].o, 8)
            zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0x4e)
            zmm0[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg5[0].o, 2)
            zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg6[0].o, 0xc4)
            arg5[0].o = _mm_permute_ps(zmm4[0].o, 0x4e)
            arg5[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm5[0].o, 2)
            arg5[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, arg7[0].o, 0xc4)
            arg6 = _mm256_insertf128_ps(arg12, arg16[0].o, 1)
            double temp0_142[0x4] = _mm256_insertf128_ps(arg11, zmm3[0].o, 1)
            zmm0 = _mm256_insertf128_ps(zmm0, arg5[0].o, 1)
            arg5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
            double var_420_2[0x4] = arg5
            arg5 = _mm256_mul_ps(arg13, zmm0)
            float temp0_146[0x8] = _mm256_mul_ps(arg14, arg6)
            float temp0_147[0x8] = _mm256_mul_ps(arg15, temp0_142)
            arg5 = _mm256_sub_ps(arg5, _mm256_mul_ps(arg14, temp0_142))
            float temp0_151[0x8] = _mm256_sub_ps(temp0_146, _mm256_mul_ps(arg15, zmm0))
            float temp0_153[0x8] = _mm256_sub_ps(temp0_147, _mm256_mul_ps(arg13, arg6))
            arg5 = _mm256_add_ps(arg5, arg5)
            float temp0_155[0x8] = _mm256_add_ps(temp0_151, temp0_151)
            float temp0_156[0x8] = _mm256_add_ps(temp0_153, temp0_153)
            zmm3 = __vaddps_ymmqq_ymmqq_memqq(_mm256_mul_ps(arg9, arg5), arg6)
            float temp0_161[0x8] =
                _mm256_sub_ps(_mm256_mul_ps(temp0_156, arg13), _mm256_mul_ps(temp0_155, arg14))
            float temp0_162[0x8] = _mm256_mul_ps(arg5, arg14)
            zmm3 = _mm256_add_ps(zmm3, temp0_161)
            float temp0_165[0x8] = _mm256_sub_ps(temp0_162, _mm256_mul_ps(temp0_156, arg15))
            zmm5 = __vaddps_ymmqq_ymmqq_memqq(_mm256_mul_ps(arg9, temp0_155), temp0_142)
            zmm4 = _mm256_add_ps(zmm5, temp0_165)
            float var_220_2[0x8] = arg9
            float var_200_2[0x8] = arg9
            float var_1e0_2[0x8] = arg9
            float var_1c0_2[0x8] = arg9
            float temp0_174[0x8] = _mm256_add_ps(
                __vaddps_ymmqq_ymmqq_memqq(_mm256_mul_ps(arg9, temp0_156), zmm0), 
                _mm256_sub_ps(_mm256_mul_ps(temp0_155, arg15), _mm256_mul_ps(arg5, arg13)))
            zmm0 = _mm256_add_ps(zmm3, _mm256_broadcast_ss(arg3[4]))
            arg5 = _mm256_add_ps(zmm4, _mm256_broadcast_ss(arg3[5]))
            arg12 = _mm256_add_ps(temp0_174, _mm256_broadcast_ss(arg3[6]))
            float temp0_185[0x8] = _mm256_add_ps(
                __vmulps_ymmqq_ymmqq_memqq(__vsubps_ymmqq_ymmqq_memqq(zmm0, var_4c0), temp0_107), 
                __vmulps_ymmqq_ymmqq_memqq(__vsubps_ymmqq_ymmqq_memqq(arg5, var_4a0), temp0_108))
            zmm3 =
                __vmulps_ymmqq_ymmqq_memqq(__vsubps_ymmqq_ymmqq_memqq(arg12, temp0_106), var_320_2)
            arg16 = __vsubps_ymmqq_ymmqq_memqq(_mm256_add_ps(temp0_185, zmm3), var_1a0_1)
            arg6 = __vcmpps_ymmqq_ymmqq_memqq_immb(arg16, data_142fc94e0, 1)
            
            if (_mm256_movemask_ps(arg6) != 0)
                float temp0_192[0x8] = _mm256_mul_ps(zmm0, arg16)
                arg5 = _mm256_mul_ps(arg5, arg16)
                zmm3 = _mm256_mul_ps(arg12, arg16)
                float temp0_195[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_192, var_400_1)
                arg5 = __vaddps_ymmqq_ymmqq_memqq(arg5, var_3e0_1)
                zmm3 = __vaddps_ymmqq_ymmqq_memqq(zmm3, var_3c0_1)
                var_400_1 = _mm256_maskstore_ps(arg6, temp0_195)
                var_3e0_1 = _mm256_maskstore_ps(arg6, arg5)
                var_3c0_1 = _mm256_maskstore_ps(arg6, zmm3)
                arg5 = var_4e0_1
                zmm0 = _mm256_add_ps(arg5, arg16)
                arg5 = _mm256_blendv_ps(arg5, zmm0, arg6)
                var_4e0_1 = arg5
            
            rsi_1 += 8
            rbx_2 += 0x60
        while (rsi_1 s< rdi_9)
        
        if (rsi_1 s< arg20)
            goto label_1400879f8
        
        zmm3 = var_340_1
        zmm5 = var_4e0_1
    zmm0[0].o = _mm256_extractf128_ps(zmm3[0].o, 1)
    arg5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
    zmm0[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg5[0].o)
    arg6[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(arg6[0].o, arg6[0].o)
    zmm0[0].o ^= arg6[0].o
    arg5[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(zmm3[0].o, arg5[0].o)
    arg5[0].o ^= arg6[0].o
    zmm0 = _mm256_insertf128_ps(arg5, zmm0[0].o, 1)
    arg5 = __vandps_ymmqq_ymmqq_memqq(zmm0, var_400_1)
    arg5 = _mm256_hadd_ps(arg5, arg5)
    arg5 = _mm256_hadd_ps(arg5, arg5)
    arg6 = __vandps_ymmqq_ymmqq_memqq(zmm0, var_3e0_1)
    float temp0_522[0x8] = _mm256_hadd_ps(arg6, arg6)
    float temp0_523[0x8] = _mm256_hadd_ps(temp0_522, temp0_522)
    zmm3[0].o = _mm256_extractf128_ps(arg5[0].o, 1)
    arg5[0].o = arg5[0].o f+ zmm3[0].d
    zmm3 = __vandps_ymmqq_ymmqq_memqq(zmm0, var_3c0_1)
    zmm4[0].o = _mm256_extractf128_ps(temp0_523[0].o, 1)
    zmm3 = _mm256_hadd_ps(zmm3, zmm3)
    temp0_523[0].o = temp0_523[0].o f+ zmm4[0]
    zmm3 = _mm256_hadd_ps(zmm3, zmm3)
    arg5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, temp0_523[0].o, 0x10)
    temp0_523[0].o = _mm256_extractf128_ps(zmm3[0].o, 1)
    temp0_523[0].o = zmm3[0].o f+ temp0_523[0]
    arg5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, temp0_523[0].o, 0x20)
    temp0_523[0].o = __vmovsd_xmmdq_memq(*r11)
    temp0_523[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(temp0_523[0].o, r11[1].d, 0x20)
    arg5[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg5[0].o, temp0_523[0].o)
    *r11 = arg5[0].d
    *(r11 + 4) = __vextractps_memd_xmmdq_immb(arg5[0].o, 1)
    r11[1].d = __vextractps_memd_xmmdq_immb(arg5[0].o, 2)
    zmm0 = _mm256_and_ps(zmm0, zmm5)
    zmm0 = _mm256_hadd_ps(zmm0, zmm0)
else
    int32_t rdi_4 = ((arg20 s>> 0x1f u>> 0x1d) + arg20) & 0xfffffff8
    zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
    float var_360_1[0x8] = zmm0
    float var_380_1[0x8] = zmm0
    float var_3a0_1[0x8] = zmm0
    float var_4e0[0x8]
    int32_t rsi
    
    if (rdi_4 s<= 0)
        zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        var_4e0 = zmm0
        rsi = 0
        
        if (0 s>= arg20)
            zmm4 = var_4e0
        else
        label_140087583:
            zmm0[0].o = zx.o(rsi)
            zmm0[0].o = __vpshufd_xmmdq_xmmdq_immb(zmm0[0].o, 0)
            arg5[0].o = __vpaddd_xmmdq_xmmdq_memdq(zmm0[0].o, data_142e11d00)
            zmm0[0].o = __vpaddd_xmmdq_xmmdq_memdq(zmm0[0].o, data_142fc9500)
            arg6[0].o = zx.o(arg20)
            arg6[0].o = __vpshufd_xmmdq_xmmdq_immb(arg6[0].o, 0)
            arg12[0].o = __vpcmpgtd_xmmdq_xmmdq_xmmdq(arg6[0].o, zmm0[0].o)
            arg13[0].o = __vpcmpgtd_xmmdq_xmmdq_xmmdq(arg6[0].o, arg5[0].o)
            arg14 = _mm256_insertf128_ps(arg13, arg12[0].o, 1)
            zmm5[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
            zmm0[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
            zmm5[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
            arg5[0].o = __vpaddd_xmmdq_xmmdq_xmmdq(zmm5[0].o, arg5[0].o)
            zmm0 = _mm256_insertf128_ps(arg5, zmm0[0].o, 1)
            zmm5 = _mm256_and_ps(arg14, zmm0)
            int64_t rax_7 = sx.q(zmm5[0])
            var_4c0[0].q = rax_7
            void* rbx_3 = arg4 + (rax_7 << 2)
            int64_t r15_1 = sx.q(__vpextrd_gpr32d_xmmdq_immb(zmm5[0].o, 1))
            void* r12_1 = arg4 + (r15_1 << 2)
            arg7[0].o = _mm256_extractf128_ps(zmm5[0].o, 1)
            int64_t rcx_1 = sx.q(arg7[0])
            void* rdi_10 = arg4 + (rcx_1 << 2)
            int64_t r13_1 = sx.q(__vpextrd_gpr32d_xmmdq_immb(arg7[0].o, 1))
            void* rsi_2 = arg4 + (r13_1 << 2)
            int32_t temp0_220[0x8] = __vandps_ymmqq_ymmqq_memqq(arg14, data_142fc9520)
            zmm0[0].o = _mm256_extractf128_ps(temp0_220[0].o, 1)
            arg5[0].o = *(zx.q(zmm0[0]) + rdi_10)
            arg9 = __vandps_ymmqq_ymmqq_memqq(arg14, data_142fc9540)
            zmm3[0].o = _mm256_extractf128_ps(arg9[0].o, 1)
            zmm4[0].o = *(zx.q(zmm3[0].d) + rdi_10)
            int32_t temp0_224 = __vpextrd_gpr32d_xmmdq_immb(arg7[0].o, 2)
            uint64_t rdi_11 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm0[0].o, 1))
            arg5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, *(rdi_11 + rsi_2), 0x10)
            float* rdi_12 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm3[0].o, 1))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rdi_12 + rsi_2), 0x10)
            int64_t r11_1 = sx.q(temp0_224)
            void* rsi_3 = arg4 + (r11_1 << 2)
            uint64_t rdi_13 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm0[0].o, 2))
            arg5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, *(rdi_13 + rsi_3), 0x20)
            float* rdi_14 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm3[0].o, 2))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rdi_14 + rsi_3), 0x20)
            int64_t rsi_5 = sx.q(__vpextrd_gpr32d_xmmdq_immb(arg7[0].o, 3))
            arg7[0].o = *(zx.q(temp0_220[0]) + rbx_3)
            arg6[0].o = *(zx.q(arg9[0]) + rbx_3)
            void* rdi_17 = arg4 + (rsi_5 << 2)
            uint64_t rbx_4 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm0[0].o, 3))
            zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, *(rbx_4 + rdi_17), 0x30)
            float* rbx_5 = zx.q(__vpextrd_gpr32d_xmmdq_immb(zmm3[0].o, 3))
            arg5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(rbx_5 + rdi_17), 0x30)
            int32_t temp0_238 = __vpextrd_gpr32d_xmmdq_immb(zmm5[0].o, 2)
            uint64_t rbx_6 = zx.q(__vpextrd_gpr32d_xmmdq_immb(temp0_220[0].o, 1))
            zmm3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg7[0].o, *(rbx_6 + r12_1), 0x10)
            uint64_t rbx_7 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg9[0].o, 1))
            arg6[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg6[0].o, *(rbx_7 + r12_1), 0x10)
            int64_t rdi_19 = sx.q(temp0_238)
            void* rbx_8 = arg4 + (rdi_19 << 2)
            uint64_t rax_14 = zx.q(__vpextrd_gpr32d_xmmdq_immb(temp0_220[0].o, 2))
            zmm3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm3[0].o, *(rax_14 + rbx_8), 0x20)
            uint64_t rax_15 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg9[0].o, 2))
            arg6[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg6[0].o, *(rax_15 + rbx_8), 0x20)
            zmm4[0].o = *(arg4 + (var_4c0[0].q << 2))
            arg7[0].o = *(arg4 + (rcx_1 << 2))
            arg7[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg7[0].o, *(arg4 + (r13_1 << 2)), 0x10)
            arg7[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg7[0].o, *(arg4 + (r11_1 << 2)), 0x20)
            arg7[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg7[0].o, *(arg4 + (rsi_5 << 2)), 0x30)
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(arg4 + (r15_1 << 2)), 0x10)
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(arg4 + (rdi_19 << 2)), 0x20)
            int64_t rax_18 = sx.q(__vpextrd_gpr32d_xmmdq_immb(zmm5[0].o, 3))
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(arg4 + (rax_18 << 2)), 0x30)
            void* rax_19 = arg4 + (rax_18 << 2)
            uint64_t rcx_2 = zx.q(__vpextrd_gpr32d_xmmdq_immb(temp0_220[0].o, 3))
            zmm3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm3[0].o, *(rcx_2 + rax_19), 0x30)
            float* rcx_3 = zx.q(__vpextrd_gpr32d_xmmdq_immb(arg9[0].o, 3))
            arg6[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg6[0].o, *(rcx_3 + rax_19), 0x30)
            arg7 = _mm256_insertf128_ps(zmm4, arg7[0].o, 1)
            double temp0_259[0x4] = _mm256_insertf128_ps(zmm3, zmm0[0].o, 1)
            double temp0_260[0x4] = _mm256_insertf128_ps(arg6, arg5[0].o, 1)
            float temp0_261[0x8] = _mm256_broadcast_ss(arg3[3])
            float temp0_262[0x8] = _mm256_broadcast_ss(arg3[1])
            float temp0_263[0x8] = _mm256_mul_ps(temp0_260, temp0_262)
            float temp0_264[0x8] = _mm256_broadcast_ss(arg3[2])
            arg10 = _mm256_mul_ps(arg7, temp0_264)
            arg11 = _mm256_broadcast_ss(*arg3)
            arg8 = _mm256_mul_ps(temp0_259, arg11)
            float temp0_269[0x8] = _mm256_sub_ps(temp0_263, _mm256_mul_ps(temp0_259, temp0_264))
            arg10 = _mm256_sub_ps(arg10, _mm256_mul_ps(temp0_260, arg11))
            arg11 = _mm256_broadcast_ss(arg3[1])
            arg5 = _mm256_broadcast_ss(arg3[2])
            float temp0_274[0x8] = _mm256_broadcast_ss(*arg3)
            zmm3 = _mm256_broadcast_ss(arg3[4])
            float temp0_276[0x8] = _mm256_broadcast_ss(arg3[5])
            arg15 = _mm256_broadcast_ss(arg3[6])
            float temp0_279[0x8] = _mm256_sub_ps(arg8, _mm256_mul_ps(arg7, temp0_262))
            float temp0_280[0x8] = _mm256_add_ps(temp0_269, temp0_269)
            arg8 = _mm256_add_ps(arg10, arg10)
            float temp0_282[0x8] = _mm256_add_ps(temp0_279, temp0_279)
            float temp0_283[0x8] = _mm256_mul_ps(temp0_282, arg11)
            arg10 = _mm256_mul_ps(temp0_280, arg5)
            arg5 = _mm256_sub_ps(temp0_283, _mm256_mul_ps(arg8, arg5))
            arg9 = _mm256_sub_ps(arg10, _mm256_mul_ps(temp0_282, temp0_274))
            arg10[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg10[0].o, arg10[0].o)
            double var_240_1[0x4] = arg10
            float var_220_3[0x8] = temp0_261
            float var_200_3[0x8] = temp0_261
            float var_1e0_3[0x8] = temp0_261
            float var_1c0_3[0x8] = temp0_261
            arg10 = _mm256_mul_ps(temp0_261, arg10)
            arg10 = _mm256_sub_ps(arg10, arg10)
            arg10 = _mm256_add_ps(arg10, arg10)
            float var_480_3[0x8] = temp0_280
            int32_t var_460_3[0x8] = arg8
            float var_440_3[0x8] = temp0_282
            double var_420_3[0x4] = arg10
            float temp0_293[0x8] = _mm256_mul_ps(arg8, temp0_274)
            arg8 = _mm256_mul_ps(temp0_261, arg8)
            float temp0_295[0x8] = _mm256_mul_ps(temp0_261, temp0_282)
            arg10 = _mm256_mul_ps(temp0_261, arg10)
            float temp0_298[0x8] =
                __vaddps_ymmqq_ymmqq_memqq(_mm256_mul_ps(temp0_261, temp0_280), arg7)
            arg8 = __vaddps_ymmqq_ymmqq_memqq(arg8, temp0_259)
            float temp0_300[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_295, temp0_260)
            arg10 = __vaddps_ymmqq_ymmqq_memqq(arg10, var_240_1)
            float var_180_1[0x8] = temp0_298
            float temp0_303[0x8] = _mm256_sub_ps(temp0_293, _mm256_mul_ps(temp0_280, arg11))
            arg5 = _mm256_add_ps(temp0_298, arg5)
            float temp0_305[0x8] = _mm256_broadcast_ss(*arg2)
            arg11 = _mm256_broadcast_ss(arg2[1])
            arg16 = _mm256_broadcast_ss(arg2[2])
            int32_t var_160_1[0x8] = arg8
            float var_140_1[0x8] = temp0_300
            float temp0_308[0x8] = _mm256_add_ps(arg8, arg9)
            float temp0_309[0x8] = _mm256_add_ps(temp0_300, temp0_303)
            float temp0_310[0x8] = _mm256_add_ps(arg5, zmm3)
            arg5 = _mm256_add_ps(temp0_308, temp0_276)
            float temp0_312[0x8] = _mm256_add_ps(temp0_309, arg15)
            float temp0_313[0x8] = _mm256_sub_ps(temp0_310, temp0_305)
            zmm3 = _mm256_sub_ps(arg5, arg11)
            float temp0_315[0x8] = _mm256_sub_ps(temp0_312, arg16)
            float temp0_320[0x8] = _mm256_add_ps(
                _mm256_mul_ps(temp0_313, _mm256_broadcast_ss(*arg1)), 
                _mm256_mul_ps(zmm3, _mm256_broadcast_ss(arg1[1])))
            zmm3 = _mm256_mul_ps(temp0_315, _mm256_broadcast_ss(arg1[2]))
            float temp0_323[0x8] = _mm256_add_ps(temp0_320, zmm3)
            zmm3[0].o = arg17
            zmm3[0].o = _mm_permute_ps(zmm3[0].o, 0)
            zmm3 = _mm256_insertf128_ps(zmm3, zmm3[0].o, 1)
            arg7 = _mm256_sub_ps(temp0_323, zmm3)
            arg8 = __vcmpps_ymmqq_ymmqq_memqq_immb(arg7, data_142fc94e0, 1)
            double var_120_1[0x4] = arg10
            temp0_323[0].o = _mm256_extractf128_ps(arg8[0].o, 1)
            temp0_323[0].o &= arg12[0].o
            zmm3[0].o = __vandps_xmmdq_xmmdq_xmmdq(arg8[0].o, arg13[0].o)
            zmm3[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(zmm3[0].o, temp0_323[0].o)
            temp0_323[0].o = __vpacksswb_xmmdq_xmmdq_xmmdq(zmm3[0].o, temp0_310[0].o)
            
            if (__vpmovmskb_gpr32d_xmmdq(temp0_323[0].o) == 0)
                r11 = arg19
                zmm4 = var_4e0
            else
                float temp0_333[0x8] = _mm256_mul_ps(temp0_310, arg7)
                arg5 = _mm256_mul_ps(arg5, arg7)
                float temp0_335[0x8] = _mm256_mul_ps(temp0_312, arg7)
                float temp0_336[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_333, var_3a0_1)
                arg5 = __vaddps_ymmqq_ymmqq_memqq(arg5, var_380_1)
                float temp0_338[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_335, var_360_1)
                zmm4 = _mm256_and_ps(arg8, arg14)
                var_3a0_1 = _mm256_maskstore_ps(zmm4, temp0_336)
                var_380_1 = _mm256_maskstore_ps(zmm4, arg5)
                var_360_1 = _mm256_maskstore_ps(zmm4, temp0_338)
                float temp0_343[0x8] = _mm256_add_ps(var_4e0, arg7)
                arg5[0].o = __vpmovsxwd_xmmdq_xmmq(zmm3[0])
                temp0_338[0].o = __vpshufd_xmmdq_xmmdq_immb(zmm3[0].o, 0x4e)
                temp0_338[0].o = __vpmovsxwd_xmmdq_xmmq(temp0_338[0].q)
                zmm4 = _mm256_blendv_ps(var_4e0, temp0_343, 
                    _mm256_insertf128_ps(arg5, temp0_338[0].o, 1))
                r11 = arg19
    else
        arg9 = _mm256_broadcast_ss(arg3[3])
        arg11[0].o = *arg3
        arg5[0].o = arg3[1]
        zmm0[0].o = arg3[2]
        var_4c0 = _mm256_broadcast_ss(*arg2)
        float temp0_4[0x8] = _mm256_broadcast_ss(arg2[1])
        float temp0_5[0x8] = _mm256_broadcast_ss(arg2[2])
        float temp0_6[0x8] = _mm256_broadcast_ss(*arg1)
        float temp0_7[0x8] = _mm256_broadcast_ss(arg1[1])
        arg6 = _mm256_broadcast_ss(arg1[2])
        float var_320_1[0x8] = arg6
        arg10[0].o = _mm_permute_ps(zmm3[0].o, 0)
        arg10 = _mm256_insertf128_ps(arg10, arg10[0].o, 1)
        int32_t rbx_1 = 0
        arg5[0].o = _mm_permute_ps(arg5[0].o, 0)
        arg12 = _mm256_insertf128_ps(arg5, arg5[0].o, 1)
        zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0)
        arg13 = _mm256_insertf128_ps(zmm0, zmm0[0].o, 1)
        zmm0[0].o = _mm_permute_ps(arg11[0].o, 0)
        arg14 = _mm256_insertf128_ps(zmm0, zmm0[0].o, 1)
        rsi = 0
        zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        var_4e0 = zmm0
        
        do
            int64_t rax_2 = sx.q(rbx_1)
            zmm0[0].o = *(arg4 + rax_2)
            arg5[0].o = *(arg4 + rax_2 + 0x10)
            zmm3[0].o = *(arg4 + rax_2 + 0x20)
            zmm4[0].o = *(arg4 + rax_2 + 0x30)
            zmm5[0].o = *(arg4 + rax_2 + 0x40)
            arg7[0].o = *(arg4 + rax_2 + 0x50)
            arg8[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg5[0].o, 0xec)
            arg6[0].o = _mm_permute_ps(zmm3[0].o, 0x44)
            arg11[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg8[0].o, arg6[0].o, 8)
            arg8[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm5[0].o, 0xec)
            arg6[0].o = _mm_permute_ps(arg7[0].o, 0x44)
            arg15[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg8[0].o, arg6[0].o, 8)
            arg8[0].o = _mm_permute_ps(zmm0[0].o, 0xe5)
            arg6[0].o = _mm_permute_ps(arg5[0].o, 0xf0)
            arg6[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg6[0].o, arg8[0].o, 1)
            arg8[0].o = _mm_permute_ps(zmm3[0].o, 0xa4)
            arg16[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg6[0].o, arg8[0].o, 8)
            arg8[0].o = _mm_permute_ps(zmm4[0].o, 0xe5)
            arg6[0].o = _mm_permute_ps(zmm5[0].o, 0xf0)
            arg6[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg6[0].o, arg8[0].o, 1)
            arg8[0].o = _mm_permute_ps(arg7[0].o, 0xa4)
            arg6[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg6[0].o, arg8[0].o, 8)
            zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0x4e)
            zmm0[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg5[0].o, 2)
            zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm3[0].o, 0xc4)
            arg5[0].o = _mm_permute_ps(zmm4[0].o, 0x4e)
            arg5[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm5[0].o, 2)
            arg5[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, arg7[0].o, 0xc4)
            double temp0_40[0x4] = _mm256_insertf128_ps(arg11, arg15[0].o, 1)
            arg6 = _mm256_insertf128_ps(arg16, arg6[0].o, 1)
            zmm0 = _mm256_insertf128_ps(zmm0, arg5[0].o, 1)
            arg5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
            double var_420_1[0x4] = arg5
            arg5 = _mm256_mul_ps(arg12, zmm0)
            float temp0_45[0x8] = _mm256_mul_ps(arg13, temp0_40)
            float temp0_46[0x8] = _mm256_mul_ps(arg14, arg6)
            arg5 = _mm256_sub_ps(arg5, _mm256_mul_ps(arg13, arg6))
            float temp0_50[0x8] = _mm256_sub_ps(temp0_45, _mm256_mul_ps(arg14, zmm0))
            float temp0_52[0x8] = _mm256_sub_ps(temp0_46, _mm256_mul_ps(arg12, temp0_40))
            arg5 = _mm256_add_ps(arg5, arg5)
            float temp0_54[0x8] = _mm256_add_ps(temp0_50, temp0_50)
            float temp0_55[0x8] = _mm256_add_ps(temp0_52, temp0_52)
            zmm3 = __vaddps_ymmqq_ymmqq_memqq(_mm256_mul_ps(arg9, arg5), temp0_40)
            float temp0_60[0x8] =
                _mm256_sub_ps(_mm256_mul_ps(temp0_55, arg12), _mm256_mul_ps(temp0_54, arg13))
            float temp0_61[0x8] = _mm256_mul_ps(arg5, arg13)
            zmm3 = _mm256_add_ps(zmm3, temp0_60)
            float temp0_64[0x8] = _mm256_sub_ps(temp0_61, _mm256_mul_ps(temp0_55, arg14))
            zmm5 = __vaddps_ymmqq_ymmqq_memqq(_mm256_mul_ps(arg9, temp0_54), arg6)
            zmm4 = _mm256_add_ps(zmm5, temp0_64)
            float var_220_1[0x8] = arg9
            float var_200_1[0x8] = arg9
            float var_1e0_1[0x8] = arg9
            float var_1c0_1[0x8] = arg9
            arg5 = _mm256_add_ps(__vaddps_ymmqq_ymmqq_memqq(_mm256_mul_ps(arg9, temp0_55), zmm0), 
                _mm256_sub_ps(_mm256_mul_ps(temp0_54, arg14), _mm256_mul_ps(arg5, arg12)))
            zmm0 = _mm256_add_ps(zmm3, _mm256_broadcast_ss(arg3[4]))
            arg16 = _mm256_add_ps(zmm4, _mm256_broadcast_ss(arg3[5]))
            arg11 = _mm256_add_ps(arg5, _mm256_broadcast_ss(arg3[6]))
            arg5 = _mm256_add_ps(
                __vmulps_ymmqq_ymmqq_memqq(__vsubps_ymmqq_ymmqq_memqq(zmm0, var_4c0), temp0_6), 
                __vmulps_ymmqq_ymmqq_memqq(__vsubps_ymmqq_ymmqq_memqq(arg16, temp0_4), temp0_7))
            arg6 = __vmulps_ymmqq_ymmqq_memqq(__vsubps_ymmqq_ymmqq_memqq(arg11, temp0_5), var_320_1)
            arg15 = _mm256_sub_ps(_mm256_add_ps(arg5, arg6), arg10)
            arg5 = __vcmpps_ymmqq_ymmqq_memqq_immb(arg15, data_142fc94e0, 1)
            
            if (_mm256_movemask_ps(arg5) != 0)
                float temp0_91[0x8] = _mm256_mul_ps(zmm0, arg15)
                float temp0_92[0x8] = _mm256_mul_ps(arg16, arg15)
                zmm3 = _mm256_mul_ps(arg11, arg15)
                float temp0_94[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_91, var_3a0_1)
                float temp0_95[0x8] = __vaddps_ymmqq_ymmqq_memqq(temp0_92, var_380_1)
                zmm3 = __vaddps_ymmqq_ymmqq_memqq(zmm3, var_360_1)
                var_3a0_1 = _mm256_maskstore_ps(arg5, temp0_94)
                var_380_1 = _mm256_maskstore_ps(arg5, temp0_95)
                var_360_1 = _mm256_maskstore_ps(arg5, zmm3)
                zmm0 = _mm256_add_ps(var_4e0, arg15)
                arg6 = _mm256_blendv_ps(var_4e0, zmm0, arg5)
                var_4e0 = arg6
            
            rsi += 8
            rbx_1 += 0x60
        while (rsi s< rdi_4)
        
        if (rsi s< arg20)
            goto label_140087583
        
        zmm4 = var_4e0
    arg5 = var_380_1
    float temp0_495[0x8] = _mm256_hadd_ps(var_3a0_1, var_3a0_1)
    float temp0_496[0x8] = _mm256_hadd_ps(temp0_495, temp0_495)
    zmm3[0].o = _mm256_extractf128_ps(temp0_496[0].o, 1)
    arg5 = _mm256_hadd_ps(arg5, arg5)
    temp0_496[0].o = temp0_496[0].o f+ zmm3[0].d
    arg5 = _mm256_hadd_ps(arg5, arg5)
    zmm3[0].o = _mm256_extractf128_ps(arg5[0].o, 1)
    float temp0_501[0x8] = _mm256_hadd_ps(var_360_1, var_360_1)
    arg5[0].o = arg5[0].o f+ zmm3[0].d
    float temp0_502[0x8] = _mm256_hadd_ps(temp0_501, temp0_501)
    temp0_496[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(temp0_496[0].o, arg5[0].o, 0x10)
    arg5[0].o = _mm256_extractf128_ps(temp0_502[0].o, 1)
    arg5[0].o = temp0_502[0].o f+ arg5[0].d
    temp0_496[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(temp0_496[0].o, arg5[0].o, 0x20)
    arg5[0].o = __vmovsd_xmmdq_memq(*r11)
    arg5[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, r11[1].d, 0x20)
    temp0_496[0].o = __vaddps_xmmdq_xmmdq_xmmdq(temp0_496[0].o, arg5[0].o)
    *r11 = temp0_496[0]
    *(r11 + 4) = __vextractps_memd_xmmdq_immb(temp0_496[0].o, 1)
    r11[1].d = __vextractps_memd_xmmdq_immb(temp0_496[0].o, 2)
    zmm0 = _mm256_hadd_ps(zmm4, zmm4)
float temp0_539[0x8] = _mm256_hadd_ps(zmm0, zmm0)
arg5[0].o = _mm256_extractf128_ps(temp0_539[0].o, 1)
temp0_539[0].o = temp0_539[0].o f+ arg5[0].d
temp0_539[0].o = temp0_539[0].o f+ *arg18
*arg18 = temp0_539[0]
arg7[0].o = var_e8
arg8[0].o = var_d8
arg9[0].o = var_c8
arg10[0].o = var_b8
arg11[0].o = var_a8
arg12[0].o = var_98
arg13[0].o = var_88
arg14[0].o = var_78
arg15[0].o = var_68
arg16[0].o = var_58
_mm256_zeroupper()
return arg18
