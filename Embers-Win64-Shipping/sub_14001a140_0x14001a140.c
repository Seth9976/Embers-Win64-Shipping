// 函数: sub_14001a140
// 地址: 0x14001a140
// 来自: E:\Embers\Embers\Binaries\Win64\Embers-Win64-Shipping.exe

int128_t var_88 = arg5[0].o
uint32_t zmm0[0x8]
zmm0[0].o = __vmovsd_xmmdq_memq(*arg2)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, arg2[1].d, 0x20)
int32_t zmm1[0x8]
zmm1[0].o = __vmovsd_xmmdq_memq(arg1[8].q)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0x88), 0x20)
arg3[0].o = _mm_permute_ps(arg3[0].o, 0xc0)
float zmm4[0x8]
zmm4[0].o = __vmovsd_xmmdq_memq(arg1[0xc].q)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(arg1 + 0xc8), 0x20)
zmm0[0].o = __vdivps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
float zmm9[0x4] = __vsubps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
zmm1[0].o = arg1[0x12].d
arg3[0].o = *(arg1 + 0x118)
zmm4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x11c))
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, arg3[0].o, 0x20)
float zmm5[0x4] = _mm_permute_ps(zmm9, 0xd2)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg3[0].o, 0x10)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0x11c), 0x20)
arg3[0].o = _mm_permute_ps(zmm9, 0xc9)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
arg3[0].o = __vmovsd_xmmdq_memq(arg1[0x10].q)
arg3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg3[0].o, *(arg1 + 0x108), 0x20)
zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm1[0].o)
zmm4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x8c))
zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(arg1 + 0x94), 0x20)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
arg3[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
arg3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg3[0].o, *(arg1 + 0xd4), 0x20)
arg3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
zmm4[0].o = *(arg1 + 0x12c)
zmm5 = *(arg1 + 0x124)
float zmm7[0x4] =
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vmovsd_xmmdq_memq(*(arg1 + 0x128)), zmm5, 0x20)
float zmm6[0x4] = __vmulps_xmmdq_xmmdq_xmmdq(_mm_permute_ps(arg3[0].o, 0xd2), zmm7)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm5, 0x10)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(arg1 + 0x128), 0x20)
arg3[0].o = _mm_permute_ps(arg3[0].o, 0xc9)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm4[0].o)
arg3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm6, arg3[0].o)
zmm4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x10c))
zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(arg1 + 0x114), 0x20)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
arg3[0].o = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
zmm4[0].o = _mm_permute_pd(zmm1[0].o, 1)
zmm1[0].o = zmm1[0].o f+ arg3[0].d
zmm1[0].o = zmm4[0].o f+ zmm1[0]
zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0xc0)
float zmm13[0x4] = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm0[0].o = __vmovsd_xmmdq_memq(arg1[6].q)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg1 + 0x68), 0x20)
zmm1[0].o = _mm_permute_ps(arg4[0].o, 0xc0)
zmm6 = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
zmm0[0].o = arg1[0xe]
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
arg3[0].o = _mm_broadcast_ss(*(arg1 + 0xe4))
zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
arg3[0].o = _mm_broadcast_ss(*(arg1 + 0xec))
zmm4[0].o = _mm_permute_pd(zmm1[0].o, 1)
zmm7 = zmm4[0].o f* *(arg1 + 0xe8)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5, 0x88)
zmm4[0].o = _mm_permute_ps(arg3[0].o, 0xd8)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
zmm4[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm1[0].o)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5, 0x4e)
zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm0[0].o, 0xc)
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0x78)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7, zmm7, 0x1c)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm5, 0x60)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
float zmm8[0x8]
zmm8[0].o = data_142d3f670
zmm5 = __vsubps_xmmdq_xmmdq_xmmdq(zmm8[0].o, zmm0[0].o)
arg5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
zmm7 = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm6, 1)
zmm1[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm6, 2)
zmm6 = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm6, 4)
zmm0[0].o = _mm_permute_ps(zmm5, 0xc0)
float zmm11[0x8]
zmm11[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7, zmm0[0].o)
zmm0[0].o = _mm_permute_ps(zmm4[0].o, 0xea)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
zmm11[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm11[0].o)
zmm0[0].o = _mm_permute_ps(arg3[0].o, 0xd5)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, zmm0[0].o)
zmm11[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm11[0].o)
zmm0[0].o = _mm_permute_ps(arg3[0].o, 0xea)
float zmm12[0x4] = __vmulps_xmmdq_xmmdq_xmmdq(zmm7, zmm0[0].o)
zmm0[0].o = _mm_permute_ps(zmm5, 0xd5)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
zmm12 = __vaddps_xmmdq_xmmdq_xmmdq(zmm12, zmm0[0].o)
zmm0[0].o = _mm_permute_ps(zmm4[0].o, 0xc0)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, zmm0[0].o)
zmm12 = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm12)
zmm0[0].o = _mm_permute_ps(zmm4[0].o, 0xd5)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7, zmm0[0].o)
zmm7 = _mm_permute_ps(arg3[0].o, 0xc0)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm7)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm1[0].o = _mm_permute_ps(zmm5, 0xea)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, zmm1[0].o)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5, arg3[0].o, 0x9c)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm4[0].o, 0x60)
zmm6 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5, zmm4[0].o, 0x8c), arg3[0].o, 0x20)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg3[0].o, 0x4e)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm4[0].o, 0x14)
arg3[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm5, 4)
zmm4[0].o = _mm_permute_ps(zmm11[0].o, 0xc0)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, _mm_permute_ps(zmm11[0].o, 0xd5))
zmm4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5)
zmm5 = _mm_permute_ps(zmm11[0].o, 0xea)
float zmm14[0x4] =
    __vaddps_xmmdq_xmmdq_xmmdq(__vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm5), zmm4[0].o)
zmm4[0].o = _mm_permute_ps(zmm12, 0xc0)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, _mm_permute_ps(zmm12, 0xd5))
zmm4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5)
zmm5 = _mm_permute_ps(zmm12, 0xea)
float zmm15[0x4] =
    __vaddps_xmmdq_xmmdq_xmmdq(__vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm5), zmm4[0].o)
zmm4[0].o = _mm_permute_ps(zmm0[0].o, 0xc0)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
zmm4[0].o = _mm_permute_ps(zmm0[0].o, 0xd5)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, zmm4[0].o)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xea)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm0[0].o)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
int32_t temp0_126 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 0)
int32_t temp0_127 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 1)
int32_t temp0_128 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 2)
zmm0[0].o = arg1[0xf]
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
arg3[0].o = _mm_broadcast_ss(*(arg1 + 0xf4))
zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
arg3[0].o = _mm_broadcast_ss(*(arg1 + 0xfc))
zmm4[0].o = _mm_permute_pd(zmm1[0].o, 1)
zmm6 = zmm4[0].o f* *(arg1 + 0xf8)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5, 0x88)
zmm4[0].o = _mm_permute_ps(arg3[0].o, 0xd8)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
zmm4[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm1[0].o)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5, 0x4e)
zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm0[0].o, 0xc)
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0x78)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6, zmm6, 0x1c)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm5, 0x60)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm8[0].o, zmm0[0].o)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, arg1[7].d, 0x10)
zmm5 = __vinsertps_xmmdq_xmmdq_memd_immb(arg5[0].o, *(arg1 + 0x74), 0x20)
zmm6 = *(arg1 + 0x6c)
zmm8[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, _mm_permute_ps(zmm1[0].o, 0xc0))
zmm7 = _mm_permute_ps(zmm4[0].o, 0xea)
zmm8[0].o = __vaddps_xmmdq_xmmdq_xmmdq(__vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm7), zmm8[0].o)
zmm8[0].o = __vaddps_xmmdq_xmmdq_xmmdq(
    __vmulps_xmmdq_xmmdq_xmmdq(zmm5, _mm_permute_ps(arg3[0].o, 0xd5)), zmm8[0].o)
arg5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, _mm_permute_ps(arg3[0].o, 0xea))
zmm7 = _mm_permute_ps(zmm1[0].o, 0xd5)
zmm7 = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm7)
arg5[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg5[0].o, zmm7)
zmm11[0].o = __vaddps_xmmdq_xmmdq_xmmdq(
    __vmulps_xmmdq_xmmdq_xmmdq(zmm5, _mm_permute_ps(zmm4[0].o, 0xc0)), arg5[0].o)
zmm6 = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, _mm_permute_ps(zmm4[0].o, 0xd5))
zmm7 = _mm_permute_ps(arg3[0].o, 0xc0)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm7)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm6, zmm0[0].o)
zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(zmm5, _mm_permute_ps(zmm1[0].o, 0xea))
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm5)
zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg3[0].o, 0x9c), zmm4[0].o, 0x60)
zmm6 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm4[0].o, 0x8c), arg3[0].o, 0x20)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg3[0].o, 0x4e)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm4[0].o, 0x14)
zmm1[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm1[0].o, 4)
arg3[0].o = _mm_permute_ps(zmm8[0].o, 0xc0)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5, arg3[0].o)
zmm4[0].o = _mm_permute_ps(zmm8[0].o, 0xd5)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, zmm4[0].o)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm4[0].o)
zmm4[0].o = _mm_permute_ps(zmm8[0].o, 0xea)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
float var_108[0x4] = arg3[0].o
arg3[0].o = _mm_permute_ps(zmm11[0].o, 0xc0)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5, arg3[0].o)
zmm4[0].o = _mm_permute_ps(zmm11[0].o, 0xd5)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, zmm4[0].o)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm4[0].o)
zmm8 = *arg7
zmm4[0].o = _mm_permute_ps(zmm11[0].o, 0xea)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
float var_f8[0x4] = arg3[0].o
arg3[0].o = _mm_permute_ps(zmm0[0].o, 0xc0)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5, arg3[0].o)
zmm4[0].o = _mm_permute_ps(zmm0[0].o, 0xd5)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, zmm4[0].o)
int32_t temp0_201 = _mm256_movemask_ps(zmm8)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm4[0].o)
zmm4[0].o = _mm_permute_pd(zmm9, 1)
zmm5 = arg4[0].o f* *(arg1 + 0x78)
int32_t temp0_204 = __vextractps_gpr32_xmmdq_immb(zmm14, 0)
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xea)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
zmm7 = __vmovshdup_xmmdq_xmmdq(zmm9)
int32_t temp0_208 = __vextractps_gpr32_xmmdq_immb(zmm15, 0)
zmm12 = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
zmm0[0].o = *(arg1 + 0x7c)
float var_118[0x4] = zmm0[0].o
int32_t temp0_210 = __vextractps_gpr32_xmmdq_immb(zmm15, 1)
float var_d8[0x4] = zmm12
float var_178[0x4] = zmm13
float var_e8[0x4] = zmm5
float var_188[0x4] = zmm14
float var_168[0x4] = zmm15
float var_158[0x8]

if (temp0_201 != 0xff)
    var_158 = zmm8
    zmm14 = __vxorps_xmmdq_xmmdq_xmmdq(zmm14, zmm14)
    bool cond:1_1 = zmm5 f<= zmm14[0]
    zmm15 = __vxorps_xmmdq_xmmdq_xmmdq(zmm15, zmm15)
    zmm12 = __vxorps_xmmdq_xmmdq_xmmdq(zmm12, zmm12)
    zmm8[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm8[0].o, zmm8[0].o)
    zmm13 = __vxorps_xmmdq_xmmdq_xmmdq(zmm13, zmm13)
    zmm11[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm11[0].o)
    
    if (not(cond:1_1))
        zmm0[0].o = data_142d3f780
        zmm6 = __vxorpd_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
        zmm14 = __vxorps_xmmdq_xmmdq_xmmdq(zmm7, zmm0[0].o)
        zmm8[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm9, zmm0[0].o)
        zmm13 = _mm_permute_ps(zmm6, 0x80)
        zmm12 = zx.o(temp0_210)
        arg4[0].o = zx.o(temp0_208)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm12, arg4[0].o, 0x10)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x20)
        zmm11[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm13, zmm0[0].o)
        zmm1[0].o = _mm_permute_ps(zmm9, 0xd5)
        arg5[0].o = zx.o(temp0_127)
        zmm0[0].o = zx.o(temp0_126)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm0[0].o, 0x10)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm0[0].o, 0x20)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
        arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm11[0].o)
        zmm11[0].o = __vmulps_xmmdq_xmmdq_xmmdq(
            __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vunpcklps_xmmdq_xmmdq_xmmdq(zmm6, zmm4[0].o), 
                zmm14, 0x20), 
            arg3[0].o)
        zmm6 = __vmulps_xmmdq_xmmdq_xmmdq(zmm13, 
            __vinsertps_xmmdq_xmmdq_xmmdq_immb(_mm_permute_ps(arg5[0].o, 0xe0), zmm12, 0x20))
        zmm13 = zx.o(temp0_128)
        arg3[0].o = __vpshufd_xmmdq_xmmdq_immb(zmm13, 0xe0)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, arg5[0].o, 0x20)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm6, zmm1[0].o)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7, zmm8[0].o, 0x10)
        arg3[0].o = arg3[0] | zmm9[0].q << 0x40
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm11[0].o)
        arg3[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg3[0].o)
        zmm15 = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm5, 1)
        zmm11[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm15, zmm1[0].o)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm4[0].o, 0x10)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm14, 0x20)
        zmm6 = __vpshufd_xmmdq_xmmdq_immb(zx.o(temp0_204), 0xc0)
        zmm6 = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm6)
        zmm7 = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm8[0].o, zmm9, 0x40)
        arg3[0].o = _mm_permute_ps(zmm0[0].o, 0xe0)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, arg4[0].o, 0x20)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7, arg3[0].o)
        arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm6)
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm14, 0x10)
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm14, 0x20)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x10)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x20)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm13, arg5[0].o, 0x10)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm12, 0x20)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7, zmm1[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm1[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm8[0].o, zmm9, 0)
        zmm1[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm9, 0xc8)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
        zmm1[0].o = __vmovddup_xmmdq_xmmq(zmm15[0].q)
        zmm8[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
        zmm13 = __vmovshdup_xmmdq_xmmdq(zmm11[0].o)
        zmm12 = _mm_permute_pd(zmm11[0].o, 1)
        zmm15 = __vmovshdup_xmmdq_xmmdq(zmm8[0].o)
        zmm14 = _mm_permute_pd(zmm8[0].o, 1)
    
    int128_t var_128_2 = zmm11[0].o
    zmm4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x8c))
    zmm6 = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(arg1 + 0x94), 0x20)
    zmm6 = __vinsertps_xmmdq_xmmdq_memd_immb(zmm6, *(arg1 + 0xd4), 0x20)
    zmm4[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6)
    zmm7 = data_142d3f780
    zmm9 = _mm_permute_pd(zmm4[0].o, 1)
    arg4[0].o = __vxorpd_xmmdq_xmmdq_xmmdq(zmm9, zmm7)
    zmm5 = __vmovshdup_xmmdq_xmmdq(zmm4[0].o)
    arg3[0].o = _mm_permute_ps(arg4[0].o, 0x80)
    zmm11[0].o = var_f8
    zmm1[0].o = _mm_permute_ps(zmm11[0].o, 0xc1)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
    zmm0[0].o = _mm_permute_ps(zmm4[0].o, 0xd5)
    arg5[0].o = var_d8
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(
        __vmulps_xmmdq_xmmdq_xmmdq(_mm_permute_ps(arg5[0].o, 0xc1), zmm0[0].o), zmm1[0].o)
    zmm6 = __vxorps_xmmdq_xmmdq_xmmdq(zmm5, zmm7)
    zmm7 = __vxorps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm7)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm4[0].o, 0x9c)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm6, 0x20)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm1[0].o)
    arg4[0].o = __vmovshdup_xmmdq_xmmdq(arg5[0].o)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm11[0].o, 0x68)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg3[0].o)
    arg4[0].o = _mm_permute_ps(arg5[0].o, 0xda)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm0[0].o)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5, zmm7, 0x10)
    arg3[0].o = arg3[0] | zmm4[0].q << 0x40
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm0[0].o)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
    zmm1[0].o = _mm_permute_ps(zmm4[0].o, 0x4a)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm6, 0x20)
    arg3[0].o = __vpermilps_xmmdq_memdq_immb(var_108, 0xc0)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
    arg4[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm7, zmm4[0].o, 0x40)
    zmm5 = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm11[0].o, arg5[0].o, 0x30)
    zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(__vshufps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm5, 0x80), 
        arg4[0].o)
    arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm5)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(
        __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm9, zmm6, 0x10), 
            zmm6, 0x20), 
        arg3[0].o)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(
        _mm_permute_ps(__vshufps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm11[0].o, 0xc), 0x78), 
        zmm1[0].o)
    arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(
        __vinsertps_xmmdq_xmmdq_xmmdq_immb(_mm_permute_ps(arg5[0].o, 0x46), zmm11[0].o, 0x68), 
        arg4[0].o)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
    arg4[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm7, zmm4[0].o, 0)
    arg4[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm4[0].o, 0xc8)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm1[0].o)
    arg5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
    arg3[0].o = __vblendps_xmmdq_xmmdq_memdq_immb(arg5[0].o, var_118, 1)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm0[0].o)
    arg3[0].o = __vmovddup_xmmdq_xmmq(arg3[0])
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
    arg3[0].o = var_128_2
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm13, 0x10)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm13, zmm8[0].o, 0x10)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm12, 0x20)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm15, 0x20)
    zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
        __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm12, zmm15, 0x10), zmm14, 0x20)
    zmm6 = __vshufps_xmmdq_xmmdq_xmmdq_immb(
        __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 5), zmm1[0].o, 0xd8)
    zmm6 = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm6)
    arg4[0].o = _mm_permute_pd(zmm0[0].o, 1)
    zmm1[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg4[0].o, 1)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5, zmm1[0].o)
    zmm11[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm0[0].o)
    int32_t temp0_580 = __vextractps_gpr32_xmmdq_immb(zmm1[0].o, 0)
    int32_t temp0_581 = __vextractps_gpr32_xmmdq_immb(zmm1[0].o, 1)
    int32_t temp0_582 = __vextractps_gpr32_xmmdq_immb(zmm1[0].o, 2)
    zmm0 = __vandps_ymmqq_ymmqq_memqq(
        __vblendps_ymmqq_ymmqq_memqq_immb(zmm11, data_142fc9300, 0xf8), data_142fc9320)
    arg3 = __vcmpps_ymmqq_ymmqq_memqq_immb(zmm0, data_142fc9340, 2)
    zmm4 = var_158
    arg4[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg5[0].o)
    zmm0[0].o = _mm256_extractf128_ps(zmm4[0].o, 1)
    zmm1[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg5[0].o)
    arg4[0].o = __vorps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
    arg3[0].o = _mm256_extractf128_ps(arg3[0].o, 1)
    arg3[0].o |= zmm1[0].o
    arg3[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(arg4[0].o, arg3[0].o)
    arg3[0].o = __vpacksswb_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm0[0].o)
    
    if (__vpmovmskb_gpr32d_xmmdq(arg3[0].o) != 0xff)
    label_14001b0d1:
        zmm0[0].o = __vmovshdup_xmmdq_xmmdq(zmm11[0].o)
        arg3[0].o = _mm_permute_pd(zmm11[0].o, 1)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6, zmm11[0].o, 0x8a)
        zmm13 = zx.o(temp0_580)
        zmm4[0].o = zx.o(temp0_581)
        zmm15 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm13, zmm4[0].o, 0x1c)
        arg4[0].o = zx.o(temp0_582)
        zmm5 = data_142d3f660
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm11[0].o, arg3[0].o, 0x10)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm13, 0x20)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, arg4[0].o, 0x30)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(
            __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5, zmm6, 0x46), arg3[0].o)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm4[0].o, 0x2a)
        zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6, zmm6, 0x9c)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm5)
        zmm0[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm0[0].o)
        zmm8[0].o = _mm_permute_ps(zmm0[0].o, 0)
        zmm9 = _mm_permute_ps(zmm0[0].o, 0x55)
        zmm12 = _mm_permute_ps(zmm0[0].o, 0xaa)
        zmm14 = _mm_permute_ps(zmm0[0].o, 0xff)
        zmm7 = 0x3f800000
        zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(0x3f800000, arg4[0].o, 0)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm15)
        arg3[0].o = __vunpcklps_xmmdq_xmmdq_xmmdq(zmm13, zmm4[0].o)
        arg3[0].o = arg5[0].q | arg3[0] << 0x40
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg5[0].o)
        zmm4[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
        zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm6, zmm11[0].o, 5)
        var_158[0].o = zmm1[0].o
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
        arg3[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm11[0].o, zmm6, 5)
        zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm11[0].o, zmm6, 0x8a)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm5)
        arg3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm8[0].o, zmm14)
        zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(zmm9, zmm12)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm5)
        zmm5 = _mm_permute_ps(zmm4[0].o, 0xd8)
        zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm5)
        arg5[0].o = _mm_permute_pd(zmm5, 1)
        zmm5 = __vaddps_xmmdq_xmmdq_xmmdq(zmm5, arg5[0].o)
        zmm1[0].o = _mm_permute_ps(zmm5, 0x39)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5, zmm1[0].o)
        arg5[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm1[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm1[0].o)
        arg5[0].o f- zmm1[0]
        
        if (arg5[0].o f!= zmm1[0] || not(is_ordered.d(arg5[0].o, zmm1[0])))
            zmm1[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm11[0].o, zmm6, 0x20)
            zmm5 = __vmovddup_xmmdq_xmmq(zmm4[0].q)
            zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm5)
            zmm5 = zmm11[0].q | zmm6[0].q << 0x40
            zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(
                __vmulps_xmmdq_xmmdq_xmmdq(zmm5, _mm_permute_ps(zmm4[0].o, 0x20)), zmm1[0].o)
            zmm7 = __vinsertps_xmmdq_xmmdq_memd_immb(arg4[0].o, 1f, 0x36)
            float var_128_3[0x4] = zmm14
            zmm14 = var_158[0].o
            zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm14, zmm12)
            zmm12 = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
            zmm1[0].o = data_142d4cc30
            zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg4[0].o, 0x10)
            arg4[0].o = _mm_permute_ps(arg3[0].o, 0x66)
            zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
            arg4[0].o = _mm_permute_ps(arg3[0].o, 0x33)
            arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7, arg4[0].o)
            zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm1[0].o)
            arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm15, zmm9)
            zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm1[0].o)
            arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7, zmm8[0].o)
            zmm7 = __vmulps_xmmdq_xmmdq_xmmdq(zmm15, _mm_permute_pd(arg3[0].o, 3))
            zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm13, 0x1d)
            arg3[0].o = __vmovddup_xmmdq_xmmq(arg3[0])
            zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
            zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm7, zmm0[0].o)
            zmm0[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm0[0].o)
            arg3[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm5, var_128_3)
            arg4[0].o = _mm_permute_ps(zmm4[0].o, 0xcc)
            arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm14, arg4[0].o)
            zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
                __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm11[0].o, 0x9d), zmm6, 0xb0)
            zmm4[0].o = _mm_permute_ps(zmm4[0].o, 0x66)
            zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5, zmm4[0].o)
            arg4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm4[0].o)
            arg3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
            arg4[0].o = data_142fc92f0
            arg4[0].o = __vdivps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg5[0].o)
            arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
            zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
            zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm12, arg4[0].o)
            zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg4[0].o)
            zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm0[0].o, 0x77)
            zmm7 = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm1[0].o, 0x77)
            zmm1[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm1[0].o, 0x22)
        else
            zmm1[0].o = data_142d4cc20
            zmm0[0].o = data_142d4cc30
        
        zmm6 = var_178
        zmm9 = var_188
        arg5[0].o = __vmovshdup_xmmdq_xmmdq(zmm7)
        zmm11[0].o = _mm_permute_pd(zmm7, 1)
        zmm13 = _mm_permute_ps(zmm7, 0xe7)
        zmm14 = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
        zmm8[0].o = _mm_permute_pd(zmm1[0].o, 1)
        arg3[0].o = _mm_permute_ps(zmm1[0].o, 0xe7)
        zmm4[0].o = __vmovshdup_xmmdq_xmmdq(zmm0[0].o)
        zmm5 = _mm_permute_pd(zmm0[0].o, 1)
        arg4[0].o = _mm_permute_ps(zmm0[0].o, 0xe7)
    else
        arg3[0].o = 0x800000
        arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6, 0x800000, 0x30)
        arg3[0].o = _mm_permute_ps(0x800000, 0)
        arg3 = __vcmpps_ymmqq_ymmqq_memqq_immb(
            __vandps_ymmqq_ymmqq_memqq(_mm256_insertf128_ps(arg4, arg3[0].o, 1), data_142fc9320), 
            data_142fc9340, 2)
        arg4[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg5[0].o)
        zmm1[0].o |= _mm256_extractf128_ps(arg3[0].o, 1)
        arg3[0].o = __vorps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
        zmm1[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
        zmm1[0].o = __vpacksswb_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
        
        if (__vpmovmskb_gpr32d_xmmdq(zmm1[0].o) != 0xff)
            goto label_14001b0d1
        
        zmm1[0].o = zx.o(temp0_580)
        arg3[0].o = zx.o(temp0_581)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg3[0].o, 0x10)
        arg3[0].o = zx.o(temp0_582)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg3[0].o, 0x20)
        arg3[0].o = 0x800000
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, 0x800000, 0x30)
        arg3[0].o = _mm_permute_ps(0x800000, 0)
        zmm1 = _mm256_insertf128_ps(zmm1, arg3[0].o, 1)
        arg3[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg3[0].o)
        arg4[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
        zmm1 = __vcmpps_ymmqq_ymmqq_memqq_immb(__vandps_ymmqq_ymmqq_memqq(zmm1, data_142fc9320), 
            data_142fc9340, 2)
        zmm0[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
        arg3[0].o = _mm256_extractf128_ps(zmm1[0].o, 1)
        zmm0[0].o |= arg3[0].o
        zmm1[0].o = __vorps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
        zmm0[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
        zmm0[0].o = __vpacksswb_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        
        if (__vpmovmskb_gpr32d_xmmdq(zmm0[0].o) != 0xff)
            goto label_14001b0d1
        
        zmm5 = 0x3f800000
        arg4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
        zmm4[0].o = zx.o(0)
        zmm0[0].o = zx.o(0)
        arg3[0].o = zx.o(0)
        zmm8[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm8[0].o, zmm8[0].o)
        zmm14 = 0x3f800000
        zmm1[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm1[0].o)
        zmm13 = __vxorps_xmmdq_xmmdq_xmmdq(zmm13, zmm13)
        zmm11[0].o = __vxorpd_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm11[0].o)
        arg5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
        zmm7 = 0x3f800000
        zmm6 = var_178
        zmm9 = var_188
    
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm4[0].o, 0x10)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5, 0x20)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x30)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm14, 0x10)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm8[0].o, 0x20)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg3[0].o, 0x30)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7, arg5[0].o, 0x10)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm11[0].o, 0x20)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm13, 0x30)
    zmm0[0].o = _mm_broadcast_ss(*arg2)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    zmm4[0].o = _mm_broadcast_ss(*(arg2 + 4))
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm4[0].o)
    zmm4[0].o = _mm_broadcast_ss(arg2[1].d)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm4[0].o)
    zmm8[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm4[0].o)
    zmm4[0].o = _mm_permute_ps(zmm6, 0)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
    zmm4[0].o = _mm_permute_ps(zmm6, 0x55)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm1[0].o)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
    arg3[0].o = _mm_permute_ps(zmm6, 0xaa)
else
    zmm15 = __vxorps_xmmdq_xmmdq_xmmdq(zmm15, zmm15)
    bool cond:2_1 = zmm5 f<= zmm15[0]
    zmm11[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm11[0].o)
    zmm13 = __vxorps_xmmdq_xmmdq_xmmdq(zmm13, zmm13)
    zmm8[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm8[0].o, zmm8[0].o)
    zmm14 = __vxorps_xmmdq_xmmdq_xmmdq(zmm14, zmm14)
    arg5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
    
    if (not(cond:2_1))
        zmm0[0].o = data_142d3f780
        zmm6 = __vxorps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
        zmm14 = __vxorps_xmmdq_xmmdq_xmmdq(zmm7, zmm0[0].o)
        zmm8[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm9, zmm0[0].o)
        zmm13 = _mm_permute_ps(zmm6, 0x80)
        zmm11[0].o = zx.o(temp0_210)
        arg4[0].o = zx.o(temp0_208)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm11[0].o, arg4[0].o, 0x10)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x20)
        arg5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm13, zmm0[0].o)
        zmm1[0].o = _mm_permute_ps(zmm9, 0xd5)
        zmm5 = zx.o(temp0_127)
        zmm0[0].o = zx.o(temp0_126)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5, zmm0[0].o, 0x10)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm0[0].o, 0x20)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
        arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg5[0].o)
        arg5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(
            __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vunpcklps_xmmdq_xmmdq_xmmdq(zmm6, zmm4[0].o), 
                zmm14, 0x20), 
            arg3[0].o)
        zmm6 = __vmulps_xmmdq_xmmdq_xmmdq(zmm13, 
            __vinsertps_xmmdq_xmmdq_xmmdq_immb(_mm_permute_ps(zmm5, 0xe0), zmm11[0].o, 0x20))
        zmm13 = zx.o(temp0_128)
        arg3[0].o = __vpshufd_xmmdq_xmmdq_immb(zmm13, 0xe0)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm5, 0x20)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm6, zmm1[0].o)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7, zmm8[0].o, 0x10)
        arg3[0].o = arg3[0] | zmm9[0].q << 0x40
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg5[0].o)
        arg3[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg3[0].o)
        zmm15 = __vblendps_xmmdq_xmmdq_memdq_immb(arg3[0].o, var_e8, 1)
        arg5[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm15, zmm1[0].o)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm4[0].o, 0x10)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm14, 0x20)
        zmm6 = __vpshufd_xmmdq_xmmdq_immb(zx.o(temp0_204), 0xc0)
        zmm6 = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm6)
        zmm7 = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm8[0].o, zmm9, 0x40)
        arg3[0].o = _mm_permute_ps(zmm0[0].o, 0xe0)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, arg4[0].o, 0x20)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7, arg3[0].o)
        arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm6)
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm14, 0x10)
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm14, 0x20)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x10)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x20)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm13, zmm5, 0x10)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm11[0].o, 0x20)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7, zmm1[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm1[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm8[0].o, zmm9, 0)
        zmm1[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm9, 0xc8)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
        zmm1[0].o = __vmovddup_xmmdq_xmmq(zmm15[0].q)
        zmm8[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
        zmm14 = __vmovshdup_xmmdq_xmmdq(arg5[0].o)
        zmm13 = _mm_permute_pd(arg5[0].o, 1)
        zmm11[0].o = __vmovshdup_xmmdq_xmmdq(zmm8[0].o)
        zmm15 = _mm_permute_pd(zmm8[0].o, 1)
    
    var_158[0].o = arg5[0].o
    zmm4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x8c))
    zmm6 = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(arg1 + 0x94), 0x20)
    zmm6 = __vinsertps_xmmdq_xmmdq_memd_immb(zmm6, *(arg1 + 0xd4), 0x20)
    zmm4[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6)
    zmm7 = data_142d3f780
    zmm9 = _mm_permute_pd(zmm4[0].o, 1)
    arg4[0].o = __vxorpd_xmmdq_xmmdq_xmmdq(zmm9, zmm7)
    zmm5 = __vmovshdup_xmmdq_xmmdq(zmm4[0].o)
    arg3[0].o = _mm_permute_ps(arg4[0].o, 0x80)
    arg5[0].o = var_f8
    zmm1[0].o = _mm_permute_ps(arg5[0].o, 0xc1)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
    zmm0[0].o = _mm_permute_ps(zmm4[0].o, 0xd5)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(
        __vmulps_xmmdq_xmmdq_xmmdq(_mm_permute_ps(zmm12, 0xc1), zmm0[0].o), zmm1[0].o)
    zmm6 = __vxorps_xmmdq_xmmdq_xmmdq(zmm5, zmm7)
    zmm7 = __vxorps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm7)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm4[0].o, 0x9c)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm6, 0x20)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm1[0].o)
    arg4[0].o = __vmovshdup_xmmdq_xmmdq(zmm12)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, arg5[0].o, 0x68)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg3[0].o)
    arg4[0].o = _mm_permute_ps(zmm12, 0xda)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm0[0].o)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5, zmm7, 0x10)
    arg3[0].o = arg3[0] | zmm4[0].q << 0x40
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm0[0].o)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
    zmm0[0].o = _mm_permute_ps(zmm4[0].o, 0x4a)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm6, 0x20)
    arg3[0].o = __vpermilps_xmmdq_memdq_immb(var_108, 0xc0)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm0[0].o)
    arg4[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm7, zmm4[0].o, 0x40)
    zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(
        __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm12, 
            __vshufps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm12, 0x30), 0x80), 
        arg4[0].o)
    arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm5)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(
        __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm9, zmm6, 0x10), 
            zmm6, 0x20), 
        arg3[0].o)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(
        _mm_permute_ps(__vshufps_xmmdq_xmmdq_xmmdq_immb(zmm12, arg5[0].o, 0xc), 0x78), zmm0[0].o)
    arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(
        __vinsertps_xmmdq_xmmdq_xmmdq_immb(_mm_permute_ps(zmm12, 0x46), arg5[0].o, 0x68), arg4[0].o)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg4[0].o)
    arg4[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm7, zmm4[0].o, 0)
    arg4[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm4[0].o, 0xc8)
    arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm0[0].o)
    zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
    arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
    arg4[0].o = __vblendps_xmmdq_xmmdq_memdq_immb(zmm0[0].o, var_118, 1)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm1[0].o)
    arg4[0].o = __vmovddup_xmmdq_xmmq(arg4[0])
    arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg3[0].o)
    zmm0[0].o = var_158[0].o
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm14, 0x10)
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm14, zmm8[0].o, 0x10)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm13, 0x20)
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm11[0].o, 0x20)
    zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
        __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm13, zmm11[0].o, 0x10), zmm15, 0x20)
    zmm6 = __vshufps_xmmdq_xmmdq_xmmdq_immb(
        __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg3[0].o, 5), arg3[0].o, 0xd8)
    zmm6 = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6)
    zmm4[0].o = _mm_permute_pd(zmm1[0].o, 1)
    arg3[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm4[0].o, 1)
    arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5, arg3[0].o)
    arg5[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm1[0].o)
    int32_t temp0_344 = __vextractps_gpr32_xmmdq_immb(arg3[0].o, 0)
    int32_t temp0_345 = __vextractps_gpr32_xmmdq_immb(arg3[0].o, 1)
    int32_t temp0_346 = __vextractps_gpr32_xmmdq_immb(arg3[0].o, 2)
    zmm1 = __vcmpps_ymmqq_ymmqq_memqq_immb(
        __vandps_ymmqq_ymmqq_memqq(__vblendps_ymmqq_ymmqq_memqq_immb(arg5, data_142fc9300, 0xf8), 
            data_142fc9320), 
        data_142fc9340, 2)
    
    if (_mm256_movemask_ps(zmm1) != 0xff)
    label_14001aaa2:
        zmm4[0].o = __vmovshdup_xmmdq_xmmdq(arg5[0].o)
        zmm5 = _mm_permute_pd(arg5[0].o, 1)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6, arg5[0].o, 0x8a)
        zmm8[0].o = zx.o(temp0_344)
        zmm7 = zx.o(temp0_345)
        zmm11[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm8[0].o, zmm7, 0x1c)
        arg4[0].o = zx.o(temp0_346)
        zmm0[0].o = data_142d3f660
        zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
            __vinsertps_xmmdq_xmmdq_xmmdq_immb(
                __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm5, 0x10), zmm8[0].o, 0x20), 
            arg4[0].o, 0x30)
        zmm1[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm1[0].o)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm6, 0x46)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm5)
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm7, 0x2a)
        zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6, zmm6, 0x9c)
        zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5)
        zmm0[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm4[0].o)
        zmm13 = _mm_permute_ps(zmm0[0].o, 0)
        zmm15 = _mm_permute_ps(zmm0[0].o, 0x55)
        zmm9 = _mm_permute_ps(zmm0[0].o, 0xaa)
        zmm14 = _mm_permute_ps(zmm0[0].o, 0xff)
        zmm4[0].o = 0x3f800000
        zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(0x3f800000, arg4[0].o, 0)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm11[0].o)
        zmm5 = __vunpcklps_xmmdq_xmmdq_xmmdq(zmm8[0].o, zmm7)
        zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].q | zmm5[0].q << 0x40, zmm1[0].o)
        zmm7 = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm5)
        zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm6, arg5[0].o, 5)
        var_158[0].o = arg3[0].o
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm0[0].o)
        zmm5 = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm6, 5)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm6, 0x8a)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5, arg3[0].o)
        zmm5 = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm13, zmm14)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm15, zmm9)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
        arg3[0].o = _mm_permute_ps(zmm7, 0xd8)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5, arg3[0].o)
        zmm12 = _mm_permute_pd(arg3[0].o, 1)
        arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm12)
        zmm1[0].o = _mm_permute_ps(arg3[0].o, 0x39)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
        zmm12 = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm1[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm1[0].o)
        zmm12 f- zmm1[0]
        
        if (zmm12 f!= zmm1[0] || not(is_ordered.d(zmm12, zmm1[0])))
            zmm1[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm6, 0x20)
            arg3[0].o = __vmovddup_xmmdq_xmmq(zmm7[0].q)
            zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
            arg3[0].o = arg5[0].q | zmm6[0].q << 0x40
            zmm4[0].o = _mm_permute_ps(zmm7, 0x20)
            zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm4[0].o)
            zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm1[0].o)
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg4[0].o, 1f, 0x36)
            float var_128_1[0x4] = zmm14
            zmm14 = var_158[0].o
            zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm14, zmm9)
            zmm9 = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
            zmm1[0].o = data_142d4cc30
            zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg4[0].o, 0x10)
            arg4[0].o = _mm_permute_ps(zmm5, 0x66)
            zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
            arg4[0].o = _mm_permute_ps(zmm5, 0x33)
            arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg4[0].o)
            zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm1[0].o)
            arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm15)
            zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm1[0].o)
            arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm13)
            zmm4[0].o = _mm_permute_pd(zmm5, 3)
            zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm4[0].o)
            zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm8[0].o, 0x1d)
            zmm5 = __vmovddup_xmmdq_xmmq(zmm5[0].q)
            zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm5)
            zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
            zmm0[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm0[0].o)
            arg3[0].o = __vmulps_xmmdq_xmmdq_memdq(arg3[0].o, var_128_1)
            arg4[0].o = _mm_permute_ps(zmm7, 0xcc)
            arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm14, arg4[0].o)
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg5[0].o, 0x9d)
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm6, 0xb0)
            zmm5 = _mm_permute_ps(zmm7, 0x66)
            zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5)
            arg4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm4[0].o)
            arg3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
            arg4[0].o = data_142fc92f0
            arg4[0].o = __vdivps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm12)
            arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
            zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
            zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm9, arg4[0].o)
            zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg4[0].o)
            zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm0[0].o, 0x77)
            zmm4[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm1[0].o, 0x77)
            zmm1[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm1[0].o, 0x22)
        else
            zmm1[0].o = data_142d4cc20
            zmm0[0].o = data_142d4cc30
        
        zmm7 = var_178
        zmm9 = var_188
        arg5[0].o = __vmovshdup_xmmdq_xmmdq(zmm4[0].o)
        zmm11[0].o = _mm_permute_pd(zmm4[0].o, 1)
        zmm13 = _mm_permute_ps(zmm4[0].o, 0xe7)
        zmm14 = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
        zmm8[0].o = _mm_permute_pd(zmm1[0].o, 1)
        arg3[0].o = _mm_permute_ps(zmm1[0].o, 0xe7)
        zmm5 = __vmovshdup_xmmdq_xmmdq(zmm0[0].o)
        zmm6 = _mm_permute_pd(zmm0[0].o, 1)
        arg4[0].o = _mm_permute_ps(zmm0[0].o, 0xe7)
    else
        zmm1[0].o = 0x800000
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6, 0x800000, 0x30)
        zmm1[0].o = _mm_permute_ps(0x800000, 0)
        zmm1 = __vcmpps_ymmqq_ymmqq_memqq_immb(
            __vandps_ymmqq_ymmqq_memqq(_mm256_insertf128_ps(arg3, zmm1[0].o, 1), data_142fc9320), 
            data_142fc9340, 2)
        
        if (_mm256_movemask_ps(zmm1) != 0xff)
            goto label_14001aaa2
        
        zmm1[0].o = zx.o(temp0_344)
        arg3[0].o = zx.o(temp0_345)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg3[0].o, 0x10)
        arg3[0].o = zx.o(temp0_346)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg3[0].o, 0x20)
        arg3[0].o = 0x800000
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, 0x800000, 0x30)
        arg3[0].o = _mm_permute_ps(0x800000, 0)
        zmm1 = __vcmpps_ymmqq_ymmqq_memqq_immb(
            __vandps_ymmqq_ymmqq_memqq(_mm256_insertf128_ps(zmm1, arg3[0].o, 1), data_142fc9320), 
            data_142fc9340, 2)
        
        if (_mm256_movemask_ps(zmm1) != 0xff)
            goto label_14001aaa2
        
        zmm6 = 0x3f800000
        arg4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
        zmm5 = __vxorps_xmmdq_xmmdq_xmmdq(zmm5, zmm5)
        zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        arg3[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg3[0].o)
        zmm8[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm8[0].o, zmm8[0].o)
        zmm14 = 0x3f800000
        zmm1[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm1[0].o)
        zmm13 = __vxorps_xmmdq_xmmdq_xmmdq(zmm13, zmm13)
        zmm11[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm11[0].o)
        arg5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
        zmm4[0].o = 0x3f800000
        zmm7 = var_178
        zmm9 = var_188
    
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5, 0x10)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm6, 0x20)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x30)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm14, 0x10)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm8[0].o, 0x20)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg3[0].o, 0x30)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, arg5[0].o, 0x10)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm11[0].o, 0x20)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm13, 0x30)
    zmm0[0].o = _mm_broadcast_ss(*arg2)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    zmm4[0].o = _mm_broadcast_ss(*(arg2 + 4))
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm4[0].o)
    zmm4[0].o = _mm_broadcast_ss(arg2[1].d)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm4[0].o)
    zmm8[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm4[0].o)
    zmm4[0].o = _mm_permute_ps(zmm7, 0)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
    zmm4[0].o = _mm_permute_ps(zmm7, 0x55)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm1[0].o)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
    arg3[0].o = _mm_permute_ps(zmm7, 0xaa)

zmm4[0].o = __vmovsd_xmmdq_memq(arg1[8].q)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(arg1 + 0x88), 0x20)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
arg4[0].o = __vmovsd_xmmdq_memq(arg1[0xc].q)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg4[0].o, *(arg1 + 0xc8), 0x20)
float var_178_1[0x4] = zmm0[0].o
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
zmm12 = _mm_permute_ps(zmm1[0].o, 0xc9)
zmm4[0].o = _mm_permute_ps(zmm8[0].o, 0xd2)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm12)
zmm14 = _mm_permute_ps(zmm1[0].o, 0xd2)
zmm1[0].o = _mm_permute_ps(zmm8[0].o, 0xc9)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm14)
zmm4[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm1[0].o)
zmm1[0].o = zx.o(temp0_204)
zmm15 = __vpblendw_xmmdq_xmmdq_xmmdq_immb(zmm9, zmm1[0].o, 3)
zmm1[0].o = zx.o(temp0_208)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zx.o(temp0_210), 0x10)
zmm11[0].o = __vblendps_xmmdq_xmmdq_memdq_immb(zmm1[0].o, var_168, 0xc)
zmm1[0].o = zx.o(temp0_126)
arg4[0].o = zx.o(temp0_127)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg4[0].o, 0x10)
arg4[0].o = zx.o(temp0_128)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg4[0].o, 0x20)
arg4[0].o = _mm_permute_ps(zmm4[0].o, 0xc0)
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm15, arg4[0].o)
zmm5 = _mm_permute_ps(zmm4[0].o, 0xd5)
zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm5)
arg4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm5)
zmm4[0].o = _mm_permute_ps(zmm4[0].o, 0xea)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg4[0].o)
float var_188_1[0x4] = zmm0[0].o
arg4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x8c))
arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg4[0].o, *(arg1 + 0x94), 0x20)
zmm4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
zmm13 = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(arg1 + 0xd4), 0x20)
arg4[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm13)
zmm4[0].o = _mm_permute_ps(arg4[0].o, 0xc9)
zmm5 = __vxorps_xmmdq_xmmdq_memdq(zmm8[0].o, data_142d3f780)
zmm7 = __vmulps_xmmdq_xmmdq_xmmdq(_mm_permute_ps(zmm5, 0xd2), zmm4[0].o)
arg4[0].o = _mm_permute_ps(arg4[0].o, 0xd2)
zmm5 = __vsubps_xmmdq_xmmdq_xmmdq(zmm7, 
    __vmulps_xmmdq_xmmdq_xmmdq(_mm_permute_ps(zmm5, 0xc9), arg4[0].o))
zmm7 = _mm_permute_ps(zmm5, 0xc0)
zmm0[0].o = var_108
zmm6 = __vaddps_xmmdq_xmmdq_xmmdq(__vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm7), 
    __vmulps_xmmdq_xmmdq_xmmdq(var_f8, _mm_permute_ps(zmm5, 0xd5)))
zmm5 = _mm_permute_ps(zmm5, 0xea)
arg5[0].o = var_d8
var_158[0].o = __vaddps_xmmdq_xmmdq_xmmdq(__vmulps_xmmdq_xmmdq_xmmdq(arg5[0].o, zmm5), zmm6)
zmm5 = __vsubps_xmmdq_xmmdq_xmmdq(
    __vmulps_xmmdq_xmmdq_xmmdq(_mm_permute_ps(arg3[0].o, 0xd2), zmm12), 
    __vmulps_xmmdq_xmmdq_xmmdq(_mm_permute_ps(arg3[0].o, 0xc9), zmm14))
zmm6 = __vmulps_xmmdq_xmmdq_xmmdq(zmm15, _mm_permute_ps(zmm5, 0xc0))
zmm7 = _mm_permute_ps(zmm5, 0xd5)
zmm6 = __vaddps_xmmdq_xmmdq_xmmdq(zmm6, __vmulps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm7))
zmm5 = _mm_permute_ps(zmm5, 0xea)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm5)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm6)
float var_168_1[0x4] = zmm1[0].o
zmm1[0].o = __vpermilps_xmmdq_memdq_immb(var_e8, 0xc0)
zmm6 = _mm_permute_ps(__vxorps_xmmdq_xmmdq_memdq(var_118, data_142d3f780), 0xc0)
zmm7 = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm8[0].o)
zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, zmm8[0].o)
zmm12 = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
zmm15 = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, arg3[0].o)
zmm1[0].o = __vxorps_xmmdq_xmmdq_memdq(arg3[0].o, data_142d3f780)
arg3[0].o = _mm_permute_ps(zmm1[0].o, 0xd2)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm4[0].o)
zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0xc9)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
arg3[0].o = _mm_permute_ps(zmm1[0].o, 0xc0)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
arg4[0].o = arg6
arg4[0].o = _mm_permute_ps(arg4[0].o, 0xc0)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm7)
zmm4[0].o = __vaddps_xmmdq_xmmdq_memdq(zmm4[0].o, var_178_1)
arg1[0xc].d = zmm4[0]
*(arg1 + 0xc4) = __vextractps_memd_xmmdq_immb(zmm4[0].o, 1)
zmm6 = _mm_permute_ps(zmm1[0].o, 0xd5)
*(arg1 + 0xc8) = __vextractps_memd_xmmdq_immb(zmm4[0].o, 2)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(var_f8, zmm6)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm4[0].o)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm5)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm13)
*(arg1 + 0xcc) = zmm0[0]
arg1[0xd].d = __vextractps_memd_xmmdq_immb(zmm0[0].o, 1)
zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0xea)
*(arg1 + 0xd4) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 2)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg5[0].o, zmm1[0].o)
zmm14 = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
zmm6 = arg1[0xe]
zmm9 = arg1[0xf]
zmm7 = __vmulps_xmmdq_xmmdq_memdq(arg4[0].o, var_188_1)
zmm1[0].o = __vmulps_xmmdq_xmmdq_memdq(arg4[0].o, var_158[0].o)
zmm8[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm8[0].o, zmm8[0].o)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, zmm8[0].o)
arg3[0].o = _mm_permute_ps(zmm7, 0)
zmm4[0].o = _mm_permute_ps(zmm6, 0x1b)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm4[0].o)
arg5[0].o = __vmovddup_xmmdq_memq(-0.007812501848093234)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg5[0].o)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm4[0].o)
zmm4[0].o = _mm_permute_ps(zmm7, 0x55)
zmm5 = _mm_permute_pd(zmm6, 1)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5)
arg3[0].o = data_142d3f7d0
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
zmm4[0].o = _mm_permute_ps(zmm7, 0xaa)
zmm5 = _mm_permute_ps(zmm6, 0xb1)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5)
zmm13 = data_142d3f7b0
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm13)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
zmm11[0].o = data_142d3f640
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm11[0].o)
zmm7 = __vmulps_xmmdq_xmmdq_xmmdq(zmm9, zmm8[0].o)
zmm4[0].o = _mm_permute_ps(zmm1[0].o, 0)
zmm5 = _mm_permute_ps(zmm9, 0x1b)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg5[0].o)
zmm4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm7, zmm4[0].o)
zmm4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(
    __vmulps_xmmdq_xmmdq_xmmdq(
        __vmulps_xmmdq_xmmdq_xmmdq(_mm_permute_ps(zmm1[0].o, 0x55), _mm_permute_pd(zmm9, 1)), 
        arg3[0].o), 
    zmm4[0].o)
zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0xaa)
zmm5 = _mm_permute_ps(zmm9, 0xb1)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm5)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm13)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm11[0].o)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm6, zmm0[0].o)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
zmm5 = _mm_permute_pd(zmm4[0].o, 1)
zmm4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5)
zmm5 = __vmovshdup_xmmdq_xmmdq(zmm4[0].o)
zmm4[0].o = zmm4[0].o f+ zmm5[0]
zmm5 = zmm4[0].o f* 0.5f
zmm7 = __vrsqrtss_xmmdq_xmmdq_xmmd(zmm4[0].o, zmm4[0])
arg3[0].o = zmm7 f* zmm7[0]
arg3[0].o = arg3[0].o f* zmm5[0]
arg3[0].o = 0x3f000000 f- arg3[0].d
arg3[0].o = zmm7 f* arg3[0].d
arg3[0].o = zmm7 f+ arg3[0].d
arg3[0].o =
    arg3[0].o f+ (arg3[0].o f* (0x3f000000 - (zmm5 f* (arg3[0].o f* arg3[0].d)[0])[0])[0])[0]
zmm4[0].o = _mm_cmp_ss(0x322bcc77, zmm4[0], 6)
arg3[0].o = _mm_permute_ps(arg3[0].o, 0)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
arg3[0].o = 0xffffffff
zmm4[0].o = __vandnps_xmmdq_xmmdq_xmmdq(zmm4[0].o, 0xffffffff)
zmm4[0].o = _mm_permute_ps(zmm4[0].o, 0)
zmm0 = _mm256_and_ps(zmm0, zmm4)
zmm4 = __vandnps_ymmqq_ymmqq_memqq(zmm4, data_142fc9280)
zmm11 = _mm256_or_ps(zmm0, zmm4)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm9, zmm1[0].o)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
zmm4[0].o = _mm_permute_pd(zmm1[0].o, 1)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
zmm4[0].o = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
zmm1[0].o = zmm1[0].o f+ zmm4[0]
zmm4[0].o = _mm_cmp_ss(0x322bcc77, zmm1[0], 6)
arg3[0].o = __vandnps_xmmdq_xmmdq_xmmdq(zmm4[0].o, 0xffffffff)
zmm4[0].o = zmm1[0].o f* 0.5f
zmm1[0].o = __vrsqrtss_xmmdq_xmmdq_xmmd(zmm1[0].o, zmm1[0])
zmm1[0].o = zmm1[0].o f+ (zmm1[0].o f* (0x3f000000 - (zmm1[0].o f* zmm1[0] f* zmm4[0])[0])[0])[0]
zmm4[0].o = zmm4[0].o f* (zmm1[0].o f* zmm1[0])[0]
int64_t rax_7 = __vpextrq_gpr64q_xmmdq_immb(zmm11[0].o, 1)
zmm4[0].o = 0x3f000000 f- zmm4[0]
zmm4[0].o = zmm1[0].o f* zmm4[0]
zmm1[0].o = zmm1[0].o f+ zmm4[0]
zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm1[0].o = _mm_permute_ps(arg3[0].o, 0)
zmm0[0].o = __vandps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm1[0].o = __vandnps_xmmdq_xmmdq_memdq(zmm1[0].o, data_142d3f660)
zmm0[0].o = __vorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm0[0].o)
arg3[0].o = _mm_permute_pd(zmm1[0].o, 1)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
arg3[0].o = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
zmm1[0].o = zmm1[0].o f+ arg3[0].d
arg3[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg3[0].o)
zmm1[0].o = _mm_cmp_ss(arg3[0].o, zmm1[0], 2)
zmm1[0].o = __vblendvps_xmmdq_xmmdq_xmmdq_xmmdq(0xbf800000, 0x3f800000, zmm1[0].o)
zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
arg1[0xe] = zmm11[0].o
arg1[0xf] = zmm0[0].o
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm12)
zmm1[0].o = __vmovsd_xmmdq_memq(arg1[0x10].q)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0x108), 0x20)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
arg1[0x10].d = zmm0[0]
*(arg1 + 0x104) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 1)
*(arg1 + 0x108) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 2)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm15)
zmm1[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x10c))
zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0x114), 0x20)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
*(arg1 + 0x10c) = zmm0[0]
arg1[0x11].d = __vextractps_memd_xmmdq_immb(zmm0[0].o, 1)
*(arg1 + 0x114) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 2)
zmm0[0].o = __vmulps_xmmdq_xmmdq_memdq(arg4[0].o, var_168_1)
zmm1[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x118))
zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, arg1[0x12].d, 0x20)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
*(arg1 + 0x118) = zmm0[0]
*(arg1 + 0x11c) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 1)
arg1[0x12].d = __vextractps_memd_xmmdq_immb(zmm0[0].o, 2)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm14)
zmm1[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x124))
zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0x12c), 0x20)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
*(arg1 + 0x124) = zmm0[0]
*(arg1 + 0x128) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 1)
int64_t rdx = zmm11[0].q
*(arg1 + 0x12c) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 2)
zmm0[0].o = zx.o(rdx.d)
zmm1[0].o = zx.o(rax_7.d)
uint32_t result = (rax_7 u>> 0x20).d
arg3[0].o = zx.o((rdx u>> 0x20).d)
arg4[0].o = *(arg1 + 0x18)
zmm5 = arg1[1].d
zmm6 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm1[0].o, 0x10), zmm0[0].o, 0x20)
zmm7 = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, 
    __vinsertps_xmmdq_xmmdq_memd_immb(__vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm5, 0x10), 
        *(arg1 + 0x14), 0x20))
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm0[0].o, 0x10)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg3[0].o, 0x20)
zmm1[0].o = __vmovsd_xmmdq_memq(arg1[1].q)
arg3[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x14))
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm5, 0x20)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
arg3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm7, arg3[0].o)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg3[0].o)
zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(__vpshufd_xmmdq_xmmdq_immb(zx.o(result), 0xc0), arg3[0].o)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg4[0].o, 0x20)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm5)
arg4[0].o = _mm_permute_ps(arg3[0].o, 0xd2)
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, arg4[0].o)
arg3[0].o = _mm_permute_ps(arg3[0].o, 0xc9)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
zmm0[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm0[0].o)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
zmm1[0].o = __vmovsd_xmmdq_memq(arg1[0xc].q)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0xc8), 0x20)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
arg1[8].d = zmm0[0]
*(arg1 + 0x84) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 1)
*(arg1 + 0x88) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 2)
zmm0[0].o = arg1[0xf].d
zmm1[0].o = *(arg1 + 0xf8)
arg3[0].o = *(arg1 + 0x48)
arg4[0].o = arg1[4].d
zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vmovsd_xmmdq_memq(*(arg1 + 0xf4)), zmm0[0].o, 0x20)
zmm6 = __vmulps_xmmdq_xmmdq_xmmdq(zmm5, 
    __vinsertps_xmmdq_xmmdq_memd_immb(
        __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, arg4[0].o, 0x10), *(arg1 + 0x44), 0x20))
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm0[0].o, 0x10)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg1 + 0xf4), 0x20)
zmm7 = __vmovsd_xmmdq_memq(arg1[4].q)
zmm0[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x44))
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x20)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
zmm0[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm6, zmm0[0].o)
arg4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
zmm0[0].o = _mm_broadcast_ss(*(arg1 + 0xfc))
zmm6 = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg4[0].o)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7, arg3[0].o, 0x20)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm6)
zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(zmm5, _mm_permute_ps(arg4[0].o, 0xd2))
arg4[0].o = _mm_permute_ps(arg4[0].o, 0xc9)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm5, zmm1[0].o)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
arg3[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
arg3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg3[0].o, *(arg1 + 0xd4), 0x20)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
*(arg1 + 0x8c) = zmm1[0]
arg1[9].d = __vextractps_memd_xmmdq_immb(zmm1[0].o, 1)
*(arg1 + 0x94) = __vextractps_memd_xmmdq_immb(zmm1[0].o, 2)
zmm1[0].o = _mm_broadcast_ss(*(arg1 + 0xec))
arg3[0].o = *arg1
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
arg4[0].o = _mm_broadcast_ss(arg1[0xe].d)
zmm5 = *(arg1 + 0xc)
zmm6 = *(arg1 + 8)
zmm7 = *(arg1 + 4)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5, zmm6, 0x10)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm7, 0x20)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, arg3[0].o, 0x30)
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm4[0].o)
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg5[0].o)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
arg4[0].o = _mm_broadcast_ss(*(arg1 + 0xe4))
zmm4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 8))
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, arg3[0].o, 0x20)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm7, 0x30)
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg4[0].o)
zmm11[0].o = data_142d3f7d0
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm11[0].o)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
arg4[0].o = _mm_broadcast_ss(*(arg1 + 0xe8))
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7, arg3[0].o, 0x10)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm5, 0x20)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm6, 0x30)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm13)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
arg3[0].o = arg1[3]
arg4[0].o = _mm_broadcast_ss(arg1[0xf].d)
zmm4[0].o = *(arg1 + 0x3c)
zmm5 = *(arg1 + 0x38)
zmm6 = *(arg1 + 0x34)
zmm7 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm5, 0x10), 
        zmm6, 0x20), 
    arg3[0].o, 0x30)
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm7)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm0[0].o)
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg5[0].o)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg4[0].o)
arg4[0].o = _mm_broadcast_ss(*(arg1 + 0xf4))
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(
        __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vmovsd_xmmdq_memq(*(arg1 + 0x38)), arg3[0].o, 0x20), 
        zmm6, 0x30), 
    arg4[0].o)
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm11[0].o)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg4[0].o)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6, arg3[0].o, 0x10)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm4[0].o, 0x20)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm5, 0x30)
arg4[0].o = _mm_broadcast_ss(*(arg1 + 0xf8))
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm13)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm0[0].o)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
arg4[0].o = _mm_permute_pd(arg3[0].o, 1)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
arg4[0].o = __vmovshdup_xmmdq_xmmdq(arg3[0].o)
arg3[0].o = arg3[0].o f+ arg4[0].d
arg4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
arg3[0].o = _mm_cmp_ss(arg4[0].o, arg3[0].d, 2)
arg3[0].o = __vblendvps_xmmdq_xmmdq_xmmdq_xmmdq(0xbf800000, 0x3f800000, arg3[0].o)
arg3[0].o = _mm_permute_ps(arg3[0].o, 0)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
arg1[0xa] = zmm1[0].o
arg1[0xb] = zmm0[0].o
arg5[0].o = var_88
_mm256_zeroupper()
return result
