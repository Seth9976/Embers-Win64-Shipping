// 函数: sub_140014250
// 地址: 0x140014250
// 来自: E:\Embers\Embers\Binaries\Win64\Embers-Win64-Shipping.exe

float var_38[0x4] = arg7
int128_t var_88 = arg6[0].o
float var_98[0x4] = arg5
float zmm0[0x8]
zmm0[0].o = __vmovsd_xmmdq_memq(*arg2)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, arg2[1].d, 0x20)
float zmm1[0x4] = _mm_broadcastss_ps(arg3[0].o)
arg3[0].o = __vmovsd_xmmdq_memq(arg1[8][0].q)
arg3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg3[0].o, (*arg1)[0x22], 0x20)
float zmm6[0x4] = __vdivps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1)
zmm0[0].o = __vmovsd_xmmdq_memq(arg1[0xc][0].q)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, (*arg1)[0x32], 0x20)
float zmm11[0x4] = __vsubps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm0[0].o)
zmm0[0].o = arg1[0x12][0]
zmm1 = (*arg1)[0x46]
arg3[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x11c))
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm1, 0x20)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1, 0x10)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, (*arg1)[0x47], 0x20)
zmm1 = _mm_permute_ps(zmm11, 0xd2)
double zmm4[0x4]
zmm4[0].o = _mm_permute_ps(zmm11, 0xc9)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
zmm0[0].o = _mm_fmsub_ps(zmm0[0].o, arg3[0].o, zmm1)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(
    __vinsertps_xmmdq_xmmdq_memd_immb(__vmovsd_xmmdq_memq(arg1[0x10][0].q), (*arg1)[0x42], 0x20), 
    zmm0[0].o)
zmm1 = __vinsertps_xmmdq_xmmdq_memd_immb(__vmovsd_xmmdq_memq(*(arg1 + 0x8c)), (*arg1)[0x25], 0x20)
arg3[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
arg3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg3[0].o, (*arg1)[0x35], 0x20)
zmm1 = __vsubps_xmmdq_xmmdq_xmmdq(zmm1, arg3[0].o)
arg3[0].o = (*arg1)[0x4b]
zmm4[0].o = (*arg1)[0x49]
float zmm5[0x4] =
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vmovsd_xmmdq_memq(*(arg1 + 0x128)), zmm4[0].o, 0x20)
float zmm7[0x4] = _mm_permute_ps(zmm1, 0xd2)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm4[0].o, 0x10)
arg3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg3[0].o, (*arg1)[0x4a], 0x20)
zmm1 = __vmulps_xmmdq_xmmdq_xmmdq(_mm_permute_ps(zmm1, 0xc9), arg3[0].o)
arg3[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x10c))
arg3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg3[0].o, (*arg1)[0x45], 0x20)
zmm1 = _mm_fmsub_ps(zmm1, zmm5, zmm7)
zmm0[0].o = __vsubps_xmmdq_xmmdq_xmmdq(__vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1), zmm0[0].o)
float var_178[0x4] = zmm6
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, zmm0[0].o)
zmm1 = __vmovshdup_xmmdq_xmmdq(zmm0[0].o)
arg3[0].o = _mm_permute_pd(zmm0[0].o, 1)
zmm0[0].o = zmm0[0].o f+ zmm1[0]
zmm0[0].o = arg3[0].o f+ zmm0[0]
zmm0[0].o = _mm_broadcastss_ps(zmm0[0].o)
int128_t var_108 = zmm0[0].o
float zmm8[0x8]
zmm8[0].o = arg4[0].o f* (*arg1)[0x1e]
zmm0[0].o = _mm_broadcastss_ps(arg4[0].o)
zmm1 = __vinsertps_xmmdq_xmmdq_memd_immb(__vmovsd_xmmdq_memq(arg1[6][0].q), (*arg1)[0x1a], 0x20)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1)
zmm0[0].o = arg1[0xe]
zmm1 = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
zmm0[0].o = _mm_broadcastss_ps(zmm0[0].o)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1, zmm0[0].o)
arg3[0].o = _mm_broadcast_ss((*arg1)[0x39])
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1, arg3[0].o)
arg3[0].o = _mm_broadcast_ss((*arg1)[0x3b])
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1, arg3[0].o)
zmm5 = _mm_permute_pd(zmm1, 1) f* (*arg1)[0x3a]
zmm7 = _mm_permute_ps(__vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x88), 0xd8)
zmm1 = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm7)
arg3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm7, arg3[0].o)
zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x4e), zmm0[0].o, 0xc)
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0x78)
arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5, zmm5, 0x1c), arg4[0].o, 0x60)
arg4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg4[0].o)
float zmm12[0x4] = _mm_broadcast_ss(1f)
arg4[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm12, arg4[0].o)
arg5 = __vxorps_xmmdq_xmmdq_xmmdq(arg5, arg5)
zmm5 = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg5, zmm4[0].o, 1)
zmm7 = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg5, zmm4[0].o, 2)
float zmm14[0x4] = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg5, zmm4[0].o, 4)
zmm6 = __vmulps_xmmdq_xmmdq_xmmdq(zmm5, _mm_broadcastss_ps(arg4[0].o))
zmm0[0].o = _mm_permute_ps(arg3[0].o, 0xea)
zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm7, zmm6)
zmm6 = _mm_fmadd_ps(_mm_permute_ps(zmm1, 0xd5), zmm14, zmm0[0].o)
float zmm13[0x4] = _mm_permute_ps(zmm1, 0xea)
zmm0[0].o = _mm_permute_ps(arg4[0].o, 0xd5)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7, zmm0[0].o)
zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm5, zmm13)
zmm4[0].o = _mm_broadcastss_ps(arg3[0].o)
zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm14, zmm0[0].o)
zmm13 = _mm_permute_ps(arg3[0].o, 0xd5)
zmm0[0].o = _mm_broadcastss_ps(zmm1)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7, zmm0[0].o)
zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm5, zmm13)
zmm5 = _mm_fmadd_ps(_mm_permute_ps(arg4[0].o, 0xea), zmm14, zmm0[0].o)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm1, 0x9c)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg3[0].o, 0x60)
zmm7 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, arg3[0].o, 0x8c), zmm1, 0x20)
zmm1 = __vblendps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1, 0x4e), 
        arg3[0].o, 0x14), 
    arg4[0].o, 4)
arg3[0].o = _mm_broadcastss_ps(zmm6)
arg4[0].o = _mm_permute_ps(zmm6, 0xd5)
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7, arg4[0].o)
arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm0[0].o, arg3[0].o)
zmm6 = _mm_fmadd_ps(_mm_permute_ps(zmm6, 0xea), zmm1, arg4[0].o)
arg3[0].o = _mm_broadcastss_ps(zmm4[0].o)
arg4[0].o = _mm_permute_ps(zmm4[0].o, 0xd5)
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7, arg4[0].o)
arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm0[0].o, arg3[0].o)
zmm4[0].o = _mm_permute_ps(zmm4[0].o, 0xea)
zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm1, arg4[0].o)
arg3[0].o = _mm_broadcastss_ps(zmm5)
arg4[0].o = _mm_permute_ps(zmm5, 0xd5)
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7, arg4[0].o)
arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm0[0].o, arg3[0].o)
zmm0[0].o = _mm_permute_ps(zmm5, 0xea)
zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm1, arg4[0].o)
float var_138[0x4] = zmm6
int32_t temp0_112 = __vextractps_gpr32_xmmdq_immb(zmm6, 0)
int32_t temp0_113 = __vextractps_gpr32_xmmdq_immb(zmm4[0].o, 0)
float var_148[0x4] = zmm4[0].o
int32_t temp0_114 = __vextractps_gpr32_xmmdq_immb(zmm4[0].o, 1)
int32_t temp0_115 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 0)
int32_t temp0_116 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 1)
float var_158[0x4] = (*arg1)[0x1f]
int32_t temp0_117 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 2)
zmm0[0].o = arg1[0xf]
zmm1 = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
zmm0[0].o = _mm_broadcastss_ps(zmm0[0].o)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1, zmm0[0].o)
arg3[0].o = _mm_broadcast_ss((*arg1)[0x3d])
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1, arg3[0].o)
arg3[0].o = _mm_broadcast_ss((*arg1)[0x3f])
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1, arg3[0].o)
zmm4[0].o = _mm_permute_pd(zmm1, 1) f* (*arg1)[0x3e]
zmm5 = _mm_permute_ps(__vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x88), 0xd8)
zmm1 = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm5)
arg3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm5, arg3[0].o)
zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x4e), zmm0[0].o, 0xc)
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0x78)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm4[0].o, 0x1c)
arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, arg4[0].o, 0x60)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg4[0].o)
zmm0[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm12, zmm0[0].o)
arg4[0].o = (*arg1)[0x1b]
zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5, arg1[7][0], 0x10)
zmm5 = _mm_broadcastss_ps(zmm0[0].o)
zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm5)
zmm6 = _mm_fmadd_ps(_mm_permute_ps(arg3[0].o, 0xea), zmm4[0].o, zmm5)
zmm5 = _mm_permute_ps(zmm1, 0xea)
zmm7 = _mm_permute_ps(zmm0[0].o, 0xd5)
zmm7 = _mm_fmadd_ps(__vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm7), arg4[0].o, zmm5)
zmm5 = _mm_broadcastss_ps(zmm1)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5)
zmm5 = _mm_permute_ps(arg3[0].o, 0xd5)
zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, arg4[0].o, zmm5)
arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5, (*arg1)[0x1d], 0x20)
zmm5 = _mm_fmadd_ps(_mm_permute_ps(zmm1, 0xd5), arg4[0].o, zmm6)
zmm6 = _mm_fmadd_ps(_mm_broadcastss_ps(arg3[0].o), arg4[0].o, zmm7)
zmm7 = _mm_fmadd_ps(_mm_permute_ps(zmm0[0].o, 0xea), arg4[0].o, zmm4[0].o)
arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1, 0x9c)
arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, arg3[0].o, 0x60)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg3[0].o, 0x8c)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm1, 0x20)
zmm0[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1, 0x4e), 
        arg3[0].o, 0x14), 
    zmm0[0].o, 4)
zmm1 = _mm_broadcastss_ps(zmm5)
arg3[0].o = _mm_permute_ps(zmm5, 0xd5)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
arg3[0].o = _mm_fmadd_ps(arg3[0].o, arg4[0].o, zmm1)
float var_168[0x4] = _mm_fmadd_ps(_mm_permute_ps(zmm5, 0xea), zmm0[0].o, arg3[0].o)
zmm1 = _mm_broadcastss_ps(zmm6)
arg3[0].o = _mm_permute_ps(zmm6, 0xd5)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
arg3[0].o = _mm_fmadd_ps(arg3[0].o, arg4[0].o, zmm1)
arg5 = _mm_fmadd_ps(_mm_permute_ps(zmm6, 0xea), zmm0[0].o, arg3[0].o)
zmm1 = _mm_broadcastss_ps(zmm7)
arg3[0].o = _mm_permute_ps(zmm7, 0xd5)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
arg3[0].o = _mm_fmadd_ps(arg3[0].o, arg4[0].o, zmm1)
zmm1 = _mm_fmadd_ps(_mm_permute_ps(zmm7, 0xea), zmm0[0].o, arg3[0].o)
zmm0[0].o = zmm1
arg7 = __vxorpd_xmmdq_xmmdq_xmmdq(arg7, arg7)
float var_e8[0x4] = zmm8[0].o
bool cond:0 = zmm8[0].o f<= arg7[0]
zmm12 = __vxorps_xmmdq_xmmdq_xmmdq(zmm12, zmm12)
zmm13 = __vxorps_xmmdq_xmmdq_xmmdq(zmm13, zmm13)
zmm1 = __vxorps_xmmdq_xmmdq_xmmdq(zmm1, zmm1)
zmm14 = __vxorps_xmmdq_xmmdq_xmmdq(zmm14, zmm14)
arg6[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg6[0].o, arg6[0].o)
float var_f8[0x4] = zmm0[0].o

if (not(cond:0))
    arg7 = _mm_permute_pd(zmm11, 1)
    zmm12 = __vmovshdup_xmmdq_xmmdq(zmm11)
    arg3[0].o = _mm_broadcast_ss(-0f)
    zmm6 = __vxorpd_xmmdq_xmmdq_xmmdq(arg7, arg3[0].o)
    zmm1 = __vxorps_xmmdq_xmmdq_xmmdq(zmm12, arg3[0].o)
    zmm13 = __vxorps_xmmdq_xmmdq_xmmdq(zmm11, arg3[0].o)
    arg3[0].o = _mm_permute_ps(zmm6, 0x80)
    zmm14 = zx.o(temp0_114)
    zmm4[0].o = zx.o(temp0_113)
    zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
        __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm14, zmm4[0].o, 0x10), zmm4[0].o, 0x20)
    arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm5)
    zmm0[0].o = _mm_permute_ps(zmm11, 0xd5)
    zmm5 = zx.o(temp0_116)
    zmm7 = zx.o(temp0_115)
    zmm8[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5, zmm7, 0x10)
    zmm8[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm8[0].o, zmm7, 0x20)
    zmm8[0].o = _mm_fmadd_ps(zmm8[0].o, zmm0[0].o, arg4[0].o)
    arg4[0].o = __vunpcklps_xmmdq_xmmdq_xmmdq(zmm6, arg7)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm1, 0x20)
    zmm8[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm8[0].o)
    arg6[0].o = zx.o(temp0_117)
    arg4[0].o = __vpshufd_xmmdq_xmmdq_immb(arg6[0].o, 0xe0)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm5, 0x20)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg4[0].o)
    arg4[0].o = _mm_permute_ps(zmm5, 0xe0)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm14, 0x20)
    zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, arg3[0].o, arg4[0].o)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm12, zmm13, 0x10)
    arg3[0].o = arg3[0].q | zmm11[0].q << 0x40
    arg3[0].o = _mm_fmadd_ps(arg3[0].o, zmm0[0].o, zmm8[0].o)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg7, arg7, 0x10)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1, 0x20)
    arg4[0].o = zx.o(temp0_112)
    arg4[0].o = _mm_broadcastd_epi32(arg4[0])
    zmm8[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg4[0].o)
    arg4[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm13, zmm11, 0x40)
    zmm6 = _mm_fmadd_ps(
        __vinsertps_xmmdq_xmmdq_xmmdq_immb(_mm_permute_ps(zmm7, 0xe0), zmm4[0].o, 0x20), arg4[0].o, 
        zmm8[0].o)
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
        __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7, zmm4[0].o, 0x10), zmm4[0].o, 0x20)
    zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
        __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg6[0].o, zmm5, 0x10), zmm14, 0x20)
    arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm5)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm0[0].o, zmm4[0].o)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg7, zmm1, 0x10)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1, 0x20)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm6)
    zmm4[0].o = __vblendps_xmmdq_xmmdq_memdq_immb(__vxorps_xmmdq_xmmdq_xmmdq(zmm1, zmm1), var_e8, 1)
    arg6[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
    arg3[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm13, zmm11, 0)
    arg3[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm11, 0xc8)
    arg3[0].o = _mm_fmadd_ps(arg3[0].o, arg4[0].o, zmm0[0].o)
    zmm0[0].o = __vmovddup_xmmdq_xmmq(zmm4[0])
    zmm1 = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    zmm0[0].o = var_f8
    zmm14 = __vmovshdup_xmmdq_xmmdq(arg6[0].o)
    zmm13 = _mm_permute_pd(arg6[0].o, 1)
    zmm12 = __vmovshdup_xmmdq_xmmdq(zmm1)
    arg7 = _mm_permute_pd(zmm1, 1)

float var_128 = zmm1[0]
arg4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x8c))
arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg4[0].o, (*arg1)[0x25], 0x20)
zmm6 = __vinsertps_xmmdq_xmmdq_memd_immb(__vmovsd_xmmdq_memq(*(arg1 + 0xcc)), (*arg1)[0x35], 0x20)
arg4[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm6)
zmm6 = _mm_broadcast_ss(-0f)
zmm11 = _mm_permute_pd(arg4[0].o, 1)
zmm7 = __vxorpd_xmmdq_xmmdq_xmmdq(zmm11, zmm6)
zmm5 = _mm_permute_ps(zmm7, 0x80)
zmm1 = __vmulps_xmmdq_xmmdq_xmmdq(_mm_permute_ps(arg5, 0xc1), zmm5)
zmm4[0].o = _mm_permute_ps(arg4[0].o, 0xd5)
zmm8[0].o = zmm0[0].o
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xc1)
zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm4[0].o, zmm1)
zmm1 = __vmulps_xmmdq_xmmdq_xmmdq(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vmovshdup_xmmdq_xmmdq(zmm8[0].o), arg5, 0x68), zmm5)
zmm5 = _mm_fmadd_ps(_mm_permute_ps(zmm8[0].o, 0xda), zmm4[0].o, zmm1)
zmm1 = __vmovshdup_xmmdq_xmmdq(arg4[0].o)
zmm4[0].o = __vxorpd_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm6)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1, zmm4[0].o, 0x10)
arg3[0].o = arg3[0].q | arg4[0].q << 0x40
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm5)
float var_118[0x4] = zmm6
zmm1 = __vxorps_xmmdq_xmmdq_xmmdq(zmm1, zmm6)
zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7, arg4[0].o, 0x9c), zmm1, 0x20)
arg3[0].o = _mm_fmadd_ps(arg3[0].o, zmm0[0].o, zmm5)
zmm0[0].o = _mm_permute_ps(arg4[0].o, 0x4a)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1, 0x20)
zmm5 = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg5, zmm8[0].o, 0x30)
zmm5 = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm8[0].o, zmm5, 0x80)
zmm7 = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, arg4[0].o, 0x40)
zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(zmm5, zmm7)
zmm6 = _mm_fmadd_ps(__vpermilps_xmmdq_memdq_immb(var_168, 0xc0), zmm0[0].o, zmm5)
zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(_mm_permute_ps(zmm8[0].o, 0x46), arg5, 0x68), zmm7)
float var_d8[0x4] = arg5
zmm7 = _mm_permute_ps(__vshufps_xmmdq_xmmdq_xmmdq_immb(zmm8[0].o, arg5, 0xc), 0x78)
zmm5 = _mm_fmadd_ps(zmm5, zmm0[0].o, zmm7)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm11, zmm1, 0x10)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1, 0x20)
zmm1 = _mm_fmadd_ps(
    __vmulps_xmmdq_xmmdq_xmmdq(
        __vshufps_xmmdq_xmmdq_xmmdq_immb(__vshufps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, arg4[0].o, 0), 
            arg4[0].o, 0xc8), 
        zmm5), 
    zmm6, zmm0[0].o)
zmm0[0].o = var_108
zmm0[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm0[0].o, var_178)
int128_t var_178_1 = zmm0[0].o
zmm0[0].o = var_158
zmm0[0].o = __vblendps_xmmdq_xmmdq_memdq_immb(zmm0[0].o, data_142d8f750, 0xe)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
zmm0[0].o = __vmovddup_xmmdq_xmmq(zmm0[0].q)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1)
zmm1 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg6[0].o, zmm14, 0x10)
arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm14, var_128, 0x10)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1, zmm13, 0x20)
zmm1 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm12, 0x20)
arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm13, zmm12, 0x10)
arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, arg7, 0x20)
zmm1 = __vaddps_xmmdq_xmmdq_xmmdq(zmm1, 
    __vshufps_xmmdq_xmmdq_xmmdq_immb(__vshufps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm0[0].o, 5), 
        zmm0[0].o, 0xd8))
zmm5 = _mm_permute_pd(arg3[0].o, 1)
zmm0[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5, 1)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm0[0].o)
int32_t temp0_306 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 0)
int32_t temp0_307 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 1)
zmm8[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
int32_t temp0_309 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 2)
arg3 = _mm256_blend_ps(_mm256_broadcast_ss(1.17549435e-38f), zmm8, 7)
zmm0 = _mm256_broadcast_ss(nanf)
arg4 = _mm256_and_ps(arg3, zmm0)
arg3 = _mm256_broadcast_ss(9.99999994e-09f)
arg4 = _mm256_cmp_ps(arg4, arg3, 2)

if (_mm256_movemask_ps(arg4) != 0xff)
label_140014b18:
    arg3[0].o = __vmovshdup_xmmdq_xmmdq(zmm8[0].o)
    arg4[0].o = _mm_permute_pd(zmm8[0].o, 1)
    zmm11 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1, zmm8[0].o, 0x8a)
    zmm0[0].o = zx.o(temp0_306)
    zmm4[0].o = zx.o(temp0_307)
    zmm14 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm4[0].o, 0x1c)
    arg5 = zx.o(temp0_309)
    zmm5 = data_142d3f660
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm8[0].o, arg4[0].o, 0x10)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm0[0].o, 0x20)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, arg5, 0x30)
    zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5, zmm1, 0x46)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm4[0].o, 0x2a)
    zmm6 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1, zmm1, 0x9c)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm6)
    arg3[0].o = _mm_fmsub_ps(arg3[0].o, arg4[0].o, zmm5)
    zmm13 = _mm_broadcastss_ps(arg3[0].o)
    zmm12 = _mm_permute_ps(arg3[0].o, 0x55)
    arg6[0].o = _mm_permute_ps(arg3[0].o, 0xaa)
    zmm6 = _mm_permute_ps(arg3[0].o, 0xff)
    arg3[0].o = 0x3f800000
    zmm7 = __vshufps_xmmdq_xmmdq_xmmdq_immb(0x3f800000, arg5, 0)
    zmm4[0].o = __vunpcklps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm4[0].o)
    arg4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
    zmm4[0].o = arg4[0].q | zmm4[0] << 0x40
    arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg4[0].o)
    zmm7 = _mm_fmsub_ps(zmm7, zmm14, arg4[0].o)
    arg4[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1, zmm8[0].o, 5)
    zmm4[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm8[0].o, zmm1, 5)
    zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm8[0].o, zmm1, 0x8a)
    arg7 = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5)
    zmm5 = zmm11
    arg7 = _mm_fmsub_ps(arg7, zmm11, arg4[0].o)
    arg4[0].o = _mm_permute_ps(zmm7, 0xd8)
    arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg7, arg4[0].o)
    zmm4[0].o = _mm_permute_pd(arg4[0].o, 1)
    arg4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm4[0].o)
    zmm4[0].o = _mm_permute_ps(arg4[0].o, 0x39)
    arg4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm4[0].o)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm12, arg6[0].o)
    zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm13, zmm6)
    zmm11 = __vsubps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg4[0].o)
    arg4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
    zmm11 f- arg4[0]
    
    if (zmm11 f!= arg4[0] || not(is_ordered.d(zmm11, arg4[0])))
        arg3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg5, 1f, 0x36)
        arg4[0].o = data_142d4cc30
        arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, arg5, 0x10)
        zmm4[0].o = _mm_permute_ps(arg7, 0x66)
        arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm4[0].o)
        zmm4[0].o = _mm_permute_ps(arg7, 0x33)
        arg4[0].o = _mm_fmsub_ps(arg4[0].o, arg3[0].o, zmm4[0].o)
        zmm12 = _mm_fmsub_ps(zmm12, zmm14, arg4[0].o)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm0[0].o, 0x1d)
        arg4[0].o = __vmovddup_xmmdq_xmmq(arg7[0].q)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg4[0].o)
        arg4[0].o = _mm_permute_pd(arg7, 3)
        zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm14, arg4[0].o)
        arg4[0].o = zmm8[0].q | zmm1[0].q << 0x40
        zmm13 = _mm_fmsub_ps(zmm13, arg3[0].o, zmm0[0].o)
        zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm8[0].o, zmm1, 0x20)
        arg3[0].o = __vmovddup_xmmdq_xmmq(zmm7[0].q)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
        arg3[0].o = _mm_broadcastss_ps(zmm7)
        zmm0[0].o = _mm_fmsub_ps(zmm0[0].o, arg4[0].o, arg3[0].o)
        arg6[0].o = _mm_fmsub_ps(arg6[0].o, zmm5, zmm0[0].o)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm8[0].o, 0x9d)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1, 0xb0)
        zmm1 = _mm_permute_ps(zmm7, 0x66)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1)
        zmm1 = _mm_permute_ps(zmm7, 0xcc)
        zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm5, zmm1)
        zmm6 = _mm_fmsub_ps(zmm6, arg4[0].o, zmm0[0].o)
        zmm0[0].o = data_142fc92f0
        zmm0[0].o = __vdivps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm11)
        zmm1 = __vmulps_xmmdq_xmmdq_xmmdq(zmm6, zmm0[0].o)
        arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm12, zmm0[0].o)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg6[0].o, zmm0[0].o)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm13, zmm0[0].o)
        zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm0[0].o, 0x77)
        arg3[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1, arg4[0].o, 0x77)
        zmm1 = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1, arg4[0].o, 0x22)
    else
        zmm1 = data_142d4cc20
        zmm0[0].o = data_142d4cc30
    
    arg7 = var_168
    zmm11 = var_138
    zmm14 = var_148
    zmm8[0].o = __vmovshdup_xmmdq_xmmdq(arg3[0].o)
    arg5 = _mm_permute_pd(arg3[0].o, 1)
    zmm12 = _mm_permute_ps(arg3[0].o, 0xe7)
    arg6[0].o = __vmovshdup_xmmdq_xmmdq(zmm1)
    zmm7 = _mm_permute_pd(zmm1, 1)
    arg4[0].o = _mm_permute_ps(zmm1, 0xe7)
    zmm5 = __vmovshdup_xmmdq_xmmdq(zmm0[0].o)
    zmm6 = _mm_permute_pd(zmm0[0].o, 1)
    zmm4[0].o = _mm_permute_ps(zmm0[0].o, 0xe7)
else
    arg4[0].o = 0x800000
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1, 0x800000, 0x30)
    arg4[0].o = _mm_broadcastss_ps(0x800000)
    arg4 = _mm256_cmp_ps(_mm256_and_ps(_mm256_insertf128_ps(zmm4, arg4[0].o, 1), zmm0), arg3, 2)
    
    if (_mm256_movemask_ps(arg4) != 0xff)
        goto label_140014b18
    
    arg4[0].o = zx.o(temp0_306)
    zmm4[0].o = zx.o(temp0_307)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm4[0].o, 0x10)
    zmm4[0].o = zx.o(temp0_309)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm4[0].o, 0x20)
    zmm4[0].o = 0x800000
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, 0x800000, 0x30)
    zmm4[0].o = _mm_broadcastss_ps(0x800000)
    arg4 = _mm256_insertf128_ps(arg4, zmm4[0].o, 1)
    zmm0 = _mm256_cmp_ps(_mm256_and_ps(arg4, zmm0), arg3, 2)
    
    if (_mm256_movemask_ps(zmm0) != 0xff)
        goto label_140014b18
    
    zmm6 = 0x3f800000
    zmm4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm4[0].o)
    zmm5 = __vxorps_xmmdq_xmmdq_xmmdq(zmm5, zmm5)
    zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
    arg4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
    zmm7 = __vxorps_xmmdq_xmmdq_xmmdq(zmm7, zmm7)
    arg6[0].o = 0x3f800000
    zmm1 = __vxorps_xmmdq_xmmdq_xmmdq(zmm1, zmm1)
    zmm12 = __vxorps_xmmdq_xmmdq_xmmdq(zmm12, zmm12)
    arg5 = __vxorps_xmmdq_xmmdq_xmmdq(arg5, arg5)
    zmm8[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm8[0].o, zmm8[0].o)
    arg3[0].o = 0x3f800000
    arg7 = var_168
    zmm11 = var_138
    zmm14 = var_148

zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5, 0x10)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm6, 0x20)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm4[0].o, 0x30)
zmm1 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1, arg6[0].o, 0x10), 
        zmm7, 0x20), 
    arg4[0].o, 0x30)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm8[0].o, 0x10)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, arg5, 0x20)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm12, 0x30)
arg4[0].o = _mm_broadcast_ss(*arg2)
zmm4[0].o = _mm_broadcast_ss(*(arg2 + 4))
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1, zmm4[0].o)
arg4[0].o = _mm_fmadd_ps(arg4[0].o, arg3[0].o, zmm4[0].o)
zmm13 = _mm_fmadd_ps(_mm_broadcast_ss(arg2[1].d), zmm0[0].o, arg4[0].o)
zmm4[0].o = var_178_1
arg4[0].o = _mm_broadcastss_ps(zmm4[0].o)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg3[0].o)
arg4[0].o = _mm_permute_ps(zmm4[0].o, 0x55)
arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm1, arg3[0].o)
zmm12 = _mm_fmadd_ps(_mm_permute_ps(zmm4[0].o, 0xaa), zmm0[0].o, arg4[0].o)
zmm0[0].o = __vmovsd_xmmdq_memq(arg1[8][0].q)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, (*arg1)[0x22], 0x20)
arg6[0].o = __vxorps_xmmdq_xmmdq_memdq(var_118, var_158)
arg5 = __vinsertps_xmmdq_xmmdq_memd_immb(__vmovsd_xmmdq_memq(arg1[0xc][0].q), (*arg1)[0x32], 0x20)
arg3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg5)
zmm1 = _mm_permute_ps(arg3[0].o, 0xc9)
zmm0[0].o = _mm_permute_ps(zmm13, 0xd2)
zmm4[0].o = _mm_permute_ps(arg3[0].o, 0xd2)
arg3[0].o = _mm_permute_ps(zmm13, 0xc9)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm4[0].o)
zmm0[0].o = _mm_fmsub_ps(zmm0[0].o, zmm1, arg3[0].o)
arg3[0].o = zx.o(temp0_112)
arg3[0].o = _mm_blend_epi32(zmm11, arg3[0].o, 1)
arg4[0].o = zx.o(temp0_113)
arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zx.o(temp0_114), 0x10)
arg4[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm14, arg4[0].o, 3)
zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(zx.o(temp0_115), zx.o(temp0_116), 0x10), zx.o(temp0_117), 
    0x20)
float var_128_1[0x4] = zmm5
zmm6 = __vmulps_xmmdq_xmmdq_xmmdq(_mm_permute_ps(zmm12, 0xc9), zmm4[0].o)
zmm4[0].o = _mm_permute_ps(zmm12, 0xd2)
zmm6 = _mm_fmsub_ps(zmm6, zmm1, zmm4[0].o)
zmm1 = _mm_broadcastss_ps(zmm0[0].o)
zmm4[0].o = _mm_permute_ps(zmm0[0].o, 0xd5)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm4[0].o)
zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, arg3[0].o, zmm1)
zmm1 = __vmovsd_xmmdq_memq(*(arg1 + 0x8c))
zmm14 = _mm_fmadd_ps(_mm_permute_ps(zmm0[0].o, 0xea), zmm5, zmm4[0].o)
zmm1 = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1, (*arg1)[0x25], 0x20)
zmm4[0].o = _mm_permute_ps(zmm6, 0xd5)
zmm8[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm4[0].o)
arg4[0].o = _mm_broadcastss_ps(zmm6)
zmm8[0].o = _mm_fmadd_ps(zmm8[0].o, arg3[0].o, arg4[0].o)
arg3[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg3[0].o, (*arg1)[0x35], 0x20)
zmm1 = __vsubps_xmmdq_xmmdq_xmmdq(zmm1, arg4[0].o)
zmm0[0].o = _mm_permute_ps(zmm1, 0xc9)
float var_178_2[0x4] = zmm0[0].o
zmm1 = _mm_permute_ps(zmm1, 0xd2)
float var_108_1[0x4] = zmm1
arg3[0].o = _mm_broadcast_ss(-0f)
int32_t var_118_1[0x4] = arg3[0].o
arg3[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm13, arg3[0].o)
zmm7 = _mm_permute_ps(arg3[0].o, 0xd2)
arg3[0].o = _mm_permute_ps(arg3[0].o, 0xc9)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1)
zmm7 = _mm_fmsub_ps(zmm7, zmm0[0].o, arg3[0].o)
zmm11 = _mm_broadcast_ss(var_e8[0])
zmm0[0].o = arg8
arg3[0].o = _mm_broadcastss_ps(zmm0[0].o)
zmm5 = _mm_fmadd_ps(__vmulps_xmmdq_xmmdq_xmmdq(zmm11, zmm13), arg3[0].o, arg5)
zmm4[0].o = _mm_permute_ps(zmm7, 0xd5)
zmm4[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm4[0].o, var_d8)
arg1[0xc][0] = zmm5[0]
zmm1 = _mm_broadcastss_ps(zmm7)
zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, arg7, zmm1)
(*arg1)[0x31] = __vextractps_memd_xmmdq_immb(zmm5, 1)
(*arg1)[0x32] = __vextractps_memd_xmmdq_immb(zmm5, 2)
arg5 = _mm_broadcastss_ps(arg6[0].o)
zmm1 = _mm_fmadd_ps(__vmulps_xmmdq_xmmdq_xmmdq(arg5, zmm13), arg3[0].o, arg4[0].o)
(*arg1)[0x33] = zmm1[0]
zmm0[0].o = _mm_permute_ps(zmm7, 0xea)
arg1[0xd][0] = __vextractps_memd_xmmdq_immb(zmm1, 1)
zmm0[0].o = __vfmadd132ps_xmmdq_xmmdq_memdq(zmm0[0].o, zmm4[0].o, var_f8)
(*arg1)[0x35] = __vextractps_memd_xmmdq_immb(zmm1, 2)
arg7 = arg1[0xe]
zmm1 = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm14)
arg4[0].o = _mm_broadcastss_ps(zmm1)
zmm4[0].o = _mm_permute_ps(arg7, 0x1b)
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm4[0].o)
zmm4[0].o = _mm_permute_ps(zmm1, 0x55)
zmm5 = _mm_permute_pd(arg7, 1)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5)
zmm5 = __vpbroadcastq_xmmdq_memq(-0x407fffffc0800000)
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm5)
zmm7 = zmm5
float var_158_1[0x4] = zmm5
zmm13 = __vxorps_xmmdq_xmmdq_xmmdq(zmm13, zmm13)
arg4[0].o = _mm_fmadd_ps(arg4[0].o, arg7, zmm13)
zmm5 = data_142d3f7d0
zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm5, arg4[0].o)
zmm14 = zmm5
zmm1 = _mm_permute_ps(zmm1, 0xaa)
arg4[0].o = _mm_permute_ps(arg7, 0xb1)
arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1, arg4[0].o)
zmm5 = data_142d3f7b0
arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm5, zmm4[0].o)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm0[0].o)
arg6[0].o = arg1[0xf]
zmm1 = _mm_broadcastss_ps(zmm0[0].o)
zmm4[0].o = _mm_permute_ps(arg6[0].o, 0x1b)
zmm1 = _mm_fmadd_ps(__vmulps_xmmdq_xmmdq_xmmdq(__vmulps_xmmdq_xmmdq_xmmdq(zmm1, zmm4[0].o), zmm7), 
    arg6[0].o, zmm13)
zmm4[0].o = _mm_permute_ps(zmm0[0].o, 0x55)
zmm7 = _mm_permute_pd(arg6[0].o, 1)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm7)
zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm14, zmm1)
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xaa)
zmm1 = _mm_permute_ps(arg6[0].o, 0xb1)
zmm14 = _mm_fmadd_ps(__vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1), zmm5, zmm4[0].o)
zmm0[0].o = 0x3f000000
zmm13 = _mm_broadcastss_ps(0x3f000000)
arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm13, arg7)
zmm14 = _mm_fmadd_ps(zmm14, zmm13, arg6[0].o)
zmm1 = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
zmm4[0].o = _mm_permute_pd(zmm1, 1)
zmm1 = __vaddps_xmmdq_xmmdq_xmmdq(zmm1, zmm4[0].o)
zmm4[0].o = __vmovshdup_xmmdq_xmmdq(zmm1)
arg6[0].o = zmm1 f+ zmm4[0].d
zmm4[0].o = arg6[0].o f* 0.5f
zmm7 = __vrsqrtss_xmmdq_xmmdq_xmmd(arg6[0].o, arg6[0])
zmm1 = _mm_fmadd_ss(_mm_fnmadd_ss(zmm7 f* zmm7[0], zmm4[0].d, 0.5f), zmm7[0], zmm7[0])
zmm7 = _mm_fnmadd_ss(zmm1 f* zmm1[0], zmm4[0].d, 0.5f)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm14, zmm14)
zmm5 = _mm_permute_pd(zmm4[0].o, 1)
zmm4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5)
zmm5 = __vmovshdup_xmmdq_xmmdq(zmm4[0].o)
arg7 = zmm4[0].o f+ zmm5[0]
zmm13 = arg7 f* 0.5f
zmm5 = __vrsqrtss_xmmdq_xmmdq_xmmd(arg7, arg7[0])
zmm4[0].o = zmm5 f* zmm5[0]
zmm4[0].o = _mm_fnmadd_ss(zmm4[0].o, zmm13[0], 0.5f)
zmm4[0].o = _mm_fmadd_ss(zmm4[0].o, zmm5[0], zmm5[0])
zmm5 = _mm_fnmadd_ss(zmm4[0].o f* zmm4[0].d, zmm13[0], 0.5f)
zmm0[0].o = _mm_broadcastss_ps(_mm_fmadd_ss(zmm7, zmm1[0], zmm1[0]))
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm0[0].o)
arg4[0].o = _mm_cmp_ss(0x322bcc77, arg6[0], 6)
arg4[0].o = __vandnps_xmmdq_xmmdq_xmmdq(arg4[0].o, 0xffffffff)
arg4 = _mm256_broadcastss_ps(arg4[0].o)
arg6 = __vandnps_ymmqq_ymmqq_memqq(arg4, data_142fc9280)
zmm0 = _mm256_and_ps(zmm0, arg4)
arg4 = _mm256_or_ps(zmm0, arg6)
zmm0[0].o = _mm_cmp_ss(0x322bcc77, arg7[0], 6)
zmm0[0].o = __vandnps_xmmdq_xmmdq_xmmdq(zmm0[0].o, 0xffffffff)
zmm5 = _mm_fmadd_ss(zmm5, zmm4[0].d, zmm4[0].d)
zmm1 = __vmulps_xmmdq_xmmdq_xmmdq(zmm14, _mm_broadcastss_ps(zmm5))
zmm0[0].o = _mm_broadcastss_ps(zmm0[0].o)
zmm1 = __vandps_xmmdq_xmmdq_xmmdq(zmm1, zmm0[0].o)
zmm0[0].o = __vandnps_xmmdq_xmmdq_memdq(zmm0[0].o, data_142d3f660)
zmm0[0].o = __vorps_xmmdq_xmmdq_xmmdq(zmm1, zmm0[0].o)
zmm1 = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm0[0].o)
zmm4[0].o = _mm_permute_pd(zmm1, 1)
zmm1 = __vaddps_xmmdq_xmmdq_xmmdq(zmm1, zmm4[0].o)
zmm4[0].o = __vmovshdup_xmmdq_xmmdq(zmm1)
zmm1 = zmm1 f+ zmm4[0].d
zmm4[0].o = _mm_permute_ps(zmm6, 0xea)
zmm1 = _mm_cmp_ss(__vxorps_xmmdq_xmmdq_xmmdq(zmm5, zmm5), zmm1[0], 2)
arg6[0].o = 0xbf800000
zmm1 = __vblendvps_xmmdq_xmmdq_xmmdq_xmmdq(0xbf800000, 0x3f800000, zmm1)
zmm4[0].o = __vfmadd132ps_xmmdq_xmmdq_memdq(zmm4[0].o, zmm8[0].o, var_128_1)
zmm1 = _mm_broadcastss_ps(zmm1)
arg1[0xe] = arg4[0].o
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1)
arg1[0xf] = zmm0[0].o
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm11, zmm12)
zmm1 = __vmulps_xmmdq_xmmdq_xmmdq(arg5, zmm12)
zmm5 = __vinsertps_xmmdq_xmmdq_memd_immb(__vmovsd_xmmdq_memq(arg1[0x10][0].q), (*arg1)[0x42], 0x20)
zmm7 = __vxorps_xmmdq_xmmdq_memdq(zmm12, var_118_1)
zmm5 = _mm_fmadd_ps(zmm5, arg3[0].o, zmm0[0].o)
arg1[0x10][0] = zmm5[0]
(*arg1)[0x41] = __vextractps_memd_xmmdq_immb(zmm5, 1)
zmm0[0].o = _mm_permute_ps(zmm7, 0xc9)
zmm0[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm0[0].o, var_108_1)
(*arg1)[0x42] = __vextractps_memd_xmmdq_immb(zmm5, 2)
zmm5 = _mm_fmadd_ps(
    __vinsertps_xmmdq_xmmdq_memd_immb(__vmovsd_xmmdq_memq(*(arg1 + 0x10c)), (*arg1)[0x45], 0x20), 
    arg3[0].o, zmm1)
int64_t rax_3 = __vpextrq_gpr64q_xmmdq_immb(arg4[0].o, 1)
(*arg1)[0x43] = zmm5[0]
zmm1 = _mm_permute_ps(zmm7, 0xd2)
arg1[0x11][0] = __vextractps_memd_xmmdq_immb(zmm5, 1)
zmm0[0].o = __vfmsub231ps_xmmdq_xmmdq_memdq(zmm0[0].o, zmm1, var_178_2)
(*arg1)[0x45] = __vextractps_memd_xmmdq_immb(zmm5, 2)
zmm1 = _mm_broadcastss_ps(zmm0[0].o)
zmm5 = __vinsertps_xmmdq_xmmdq_memd_immb(__vmovsd_xmmdq_memq(*(arg1 + 0x118)), arg1[0x12][0], 0x20)
zmm7 = _mm_permute_ps(zmm0[0].o, 0xd5)
zmm5 = _mm_fmadd_ps(zmm5, arg3[0].o, zmm4[0].o)
(*arg1)[0x46] = zmm5[0]
zmm4[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm7, var_d8)
(*arg1)[0x47] = __vextractps_memd_xmmdq_immb(zmm5, 1)
zmm4[0].o = __vfmadd231ps_xmmdq_xmmdq_memdq(zmm4[0].o, zmm1, var_168)
arg1[0x12][0] = __vextractps_memd_xmmdq_immb(zmm5, 2)
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xea)
zmm1 = __vinsertps_xmmdq_xmmdq_memd_immb(__vmovsd_xmmdq_memq(*(arg1 + 0x124)), (*arg1)[0x4b], 0x20)
zmm0[0].o = __vfmadd132ps_xmmdq_xmmdq_memdq(zmm0[0].o, zmm4[0].o, var_f8)
zmm1 = _mm_fmadd_ps(zmm1, arg3[0].o, zmm0[0].o)
(*arg1)[0x49] = zmm1[0]
(*arg1)[0x4a] = __vextractps_memd_xmmdq_immb(zmm1, 1)
(*arg1)[0x4b] = __vextractps_memd_xmmdq_immb(zmm1, 2)
int64_t rdx = arg4[0].q
zmm0[0].o = zx.o(rdx.d)
zmm1 = zx.o(rax_3.d)
uint32_t result = (rax_3 u>> 0x20).d
arg3[0].o = zx.o((rdx u>> 0x20).d)
arg4[0].o = (*arg1)[6]
zmm4[0].o = arg1[1][0]
zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm1, 0x10)
zmm7 = __vinsertps_xmmdq_xmmdq_memd_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm4[0].o, 0x10), (*arg1)[5], 0x20)
zmm5 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5, zmm0[0].o, 0x20)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1, zmm0[0].o, 0x10)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg3[0].o, 0x20)
zmm1 = __vmovsd_xmmdq_memq(arg1[1][0].q)
arg3[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x14))
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm4[0].o, 0x20)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
arg3[0].o = _mm_fmsub_ps(arg3[0].o, zmm5, zmm7)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg3[0].o)
zmm4[0].o = zx.o(result)
zmm4[0].o = _mm_broadcastd_epi32(zmm4[0].d)
zmm1 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1, arg4[0].o, 0x20)
zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, arg3[0].o, zmm1)
zmm1 = _mm_permute_ps(arg3[0].o, 0xd2)
arg3[0].o = _mm_permute_ps(arg3[0].o, 0xc9)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
zmm0[0].o = _mm_fmsub_ps(zmm0[0].o, zmm5, zmm1)
zmm1 = __vinsertps_xmmdq_xmmdq_memd_immb(__vmovsd_xmmdq_memq(arg1[0xc][0].q), (*arg1)[0x32], 0x20)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1, zmm0[0].o)
arg1[8][0] = zmm0[0]
(*arg1)[0x21] = __vextractps_memd_xmmdq_immb(zmm0[0].o, 1)
(*arg1)[0x22] = __vextractps_memd_xmmdq_immb(zmm0[0].o, 2)
zmm0[0].o = arg1[0xf][0]
zmm1 = (*arg1)[0x3e]
arg3[0].o = (*arg1)[0x12]
arg4[0].o = arg1[4][0]
zmm4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xf4))
zmm5 = __vinsertps_xmmdq_xmmdq_memd_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, arg4[0].o, 0x10), (*arg1)[0x11], 0x20)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm0[0].o, 0x20)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1, zmm0[0].o, 0x10)
zmm1 = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, (*arg1)[0x3d], 0x20)
zmm7 = __vmovsd_xmmdq_memq(arg1[4][0].q)
zmm0[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x44))
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x20)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1, zmm0[0].o)
zmm0[0].o = _mm_fmsub_ps(zmm0[0].o, zmm4[0].o, zmm5)
arg4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
zmm8[0].o = _mm_broadcast_ss((*arg1)[0x3f])
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7, arg3[0].o, 0x20)
arg3[0].o = _mm_fmadd_ps(arg3[0].o, zmm8[0].o, arg4[0].o)
zmm5 = _mm_permute_ps(arg4[0].o, 0xd2)
arg4[0].o = _mm_permute_ps(arg4[0].o, 0xc9)
zmm1 = _mm_fmsub_ps(__vmulps_xmmdq_xmmdq_xmmdq(zmm1, arg4[0].o), zmm4[0].o, zmm5)
arg4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg4[0].o, (*arg1)[0x35], 0x20)
zmm1 = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1)
zmm1 = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm1)
(*arg1)[0x23] = zmm1[0]
arg1[9][0] = __vextractps_memd_xmmdq_immb(zmm1, 1)
(*arg1)[0x25] = __vextractps_memd_xmmdq_immb(zmm1, 2)
zmm5 = _mm_broadcast_ss((*arg1)[0x3b])
zmm1 = *arg1
zmm7 = _mm_broadcast_ss(arg1[0xe][0])
arg5 = (*arg1)[3]
zmm11 = (*arg1)[2]
zmm12 = (*arg1)[1]
zmm6 = _mm_fmadd_ps(
    __vmulps_xmmdq_xmmdq_xmmdq(
        __vmulps_xmmdq_xmmdq_xmmdq(zmm7, 
            __vinsertps_xmmdq_xmmdq_xmmdq_immb(
                __vinsertps_xmmdq_xmmdq_xmmdq_immb(
                    __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg5, zmm11, 0x10), zmm12, 0x20), 
                zmm1, 0x30)), 
        var_158_1), 
    zmm1, zmm5)
zmm5 = _mm_broadcast_ss((*arg1)[0x39])
zmm5 = __vmulps_xmmdq_xmmdq_xmmdq(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(
        __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vmovsd_xmmdq_memq(*(arg1 + 8)), zmm1, 0x20), zmm12, 
        0x30), 
    zmm5)
zmm14 = data_142d3f7d0
zmm5 = _mm_fmadd_ps(zmm5, zmm14, zmm6)
zmm13 = (*arg1)[0xf]
zmm7 = (*arg1)[0xe]
zmm0[0].o = (*arg1)[0xd]
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm13, zmm7, 0x10)
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm0[0].o, 0x20)
arg4[0].o = arg1[3]
arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, arg4[0].o, 0x30)
zmm4[0].o = _mm_broadcast_ss(arg1[0xf][0])
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, var_158_1)
arg3[0].o = _mm_fmadd_ps(arg3[0].o, arg4[0].o, zmm8[0].o)
zmm4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x38))
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, arg4[0].o, 0x20)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm0[0].o, 0x30)
zmm6 = _mm_broadcast_ss((*arg1)[0x3d])
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6)
zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm14, arg3[0].o)
zmm1 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(
    __vinsertps_xmmdq_xmmdq_xmmdq_immb(__vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm12, zmm1, 0x10), arg5, 
        0x20), 
    zmm11, 0x30)
arg3[0].o = _mm_broadcast_ss((*arg1)[0x3a])
zmm1 = __vmulps_xmmdq_xmmdq_xmmdq(zmm1, arg3[0].o)
zmm6 = data_142d3f7b0
zmm1 = _mm_fmadd_ps(zmm1, zmm6, zmm5)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x10)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm13, 0x20)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm7, 0x30)
arg3[0].o = _mm_broadcast_ss((*arg1)[0x3e])
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm6, zmm4[0].o)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1, zmm0[0].o)
arg4[0].o = _mm_permute_pd(arg3[0].o, 1)
arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
arg4[0].o = __vmovshdup_xmmdq_xmmdq(arg3[0].o)
arg3[0].o = arg3[0].o f+ arg4[0]
arg4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
arg3[0].o = _mm_cmp_ss(arg4[0].o, arg3[0], 2)
arg3[0].o = __vblendvps_xmmdq_xmmdq_xmmdq_xmmdq(0xbf800000, 0x3f800000, arg3[0].o)
arg3[0].o = _mm_broadcastss_ps(arg3[0].o)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
arg1[0xa] = zmm1
arg1[0xb] = zmm0[0].o
arg6[0].o = var_88
_mm256_zeroupper()
return result
