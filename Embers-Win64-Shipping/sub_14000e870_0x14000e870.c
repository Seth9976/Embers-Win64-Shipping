// 函数: sub_14000e870
// 地址: 0x14000e870
// 来自: E:\Embers\Embers\Binaries\Win64\Embers-Win64-Shipping.exe

int128_t var_168 = arg4[0].o
float var_198 = arg3[0]
float zmm0[0x8]
zmm0[0].o = __vmovsd_xmmdq_memq(arg1[8].q)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg1 + 0x88), 0x20)
int32_t zmm1[0x8]
zmm1[0].o = __vmovsd_xmmdq_memq(arg1[0xc].q)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0xc8), 0x20)
arg3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
int128_t var_188 = arg3[0].o
zmm0[0].o = arg1[0x12].d
zmm1[0].o = *(arg1 + 0x118)
uint32_t zmm4[0x8]
zmm4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x11c))
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm1[0].o, 0x20)
double zmm5[0x4]
zmm5[0].o = _mm_permute_ps(arg3[0].o, 0xd2)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0x10)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg1 + 0x11c), 0x20)
zmm1[0].o = _mm_permute_ps(arg3[0].o, 0xc9)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
zmm0[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x8c))
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg1 + 0x94), 0x20)
zmm1[0].o = _mm_fmsub_ps(zmm1[0].o, zmm4[0].o, zmm5[0].o)
float var_178[0x4] = zmm1[0].o
zmm1[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0xd4), 0x20)
zmm0[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm1[0].o = *(arg1 + 0x12c)
zmm4[0].o = *(arg1 + 0x124)
zmm5[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x128))
zmm5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm4[0].o, 0x20)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm4[0].o, 0x10)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0x128), 0x20)
zmm4[0].o = _mm_permute_ps(zmm0[0].o, 0xd2)
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xc9)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm0[0].o = _mm_fmsub_ps(zmm0[0].o, zmm5[0].o, zmm4[0].o)
float var_158[0x4] = zmm0[0].o
zmm0[0].o = _mm_broadcastss_ps(arg4[0].o)
zmm1[0].o = __vmovsd_xmmdq_memq(arg1[6].q)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0x68), 0x20)
float zmm7[0x8]
zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm0[0].o = arg1[0xe]
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
zmm0[0].o = _mm_broadcastss_ps(zmm0[0].o)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
zmm4[0].o = _mm_broadcast_ss(*(arg1 + 0xe4))
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
zmm5[0].o = _mm_broadcast_ss(*(arg1 + 0xec))
zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm5[0].o)
zmm1[0].o = _mm_permute_pd(zmm1[0].o, 1)
float zmm9[0x4] = zmm1[0].o f* *(arg1 + 0xe8)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm4[0].o, 0x88)
double zmm6[0x4]
zmm6[0].o = _mm_permute_ps(zmm1[0].o, 0xd8)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm6[0].o)
zmm5[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm5[0].o)
zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm4[0].o, 0x4e)
zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm0[0].o, 0xc)
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0x78)
zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm9, zmm9, 0x1c)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm4[0].o, 0x60)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm4[0].o)
float zmm10[0x4] = _mm_broadcast_ss(1f)
zmm6[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm10, zmm0[0].o)
zmm9 = __vxorps_xmmdq_xmmdq_xmmdq(zmm9, zmm9)
float zmm15[0x4] = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm9, zmm7[0].o, 1)
zmm4[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm9, zmm7[0].o, 2)
float zmm13[0x4] = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm9, zmm7[0].o, 4)
zmm7[0].o = _mm_broadcastss_ps(zmm6[0].o)
float zmm14[0x4] = __vmulps_xmmdq_xmmdq_xmmdq(zmm15, zmm7[0].o)
zmm7[0].o = _mm_permute_ps(zmm5[0].o, 0xea)
zmm7[0].o = _mm_fmadd_ps(zmm7[0].o, zmm4[0].o, zmm14)
zmm0[0].o = _mm_permute_ps(zmm1[0].o, 0xd5)
zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm13, zmm7[0].o)
zmm14 = _mm_permute_ps(zmm1[0].o, 0xea)
zmm7[0].o = _mm_permute_ps(zmm6[0].o, 0xd5)
zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm7[0].o)
zmm7[0].o = _mm_fmadd_ps(zmm7[0].o, zmm15, zmm14)
zmm14 = _mm_fmadd_ps(_mm_broadcastss_ps(zmm5[0].o), zmm13, zmm7[0].o)
zmm7[0].o = _mm_broadcastss_ps(zmm1[0].o)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm7[0].o)
zmm7[0].o = _mm_permute_ps(zmm5[0].o, 0xd5)
zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm15, zmm7[0].o)
zmm7[0].o = _mm_permute_ps(zmm6[0].o, 0xea)
zmm7[0].o = _mm_fmadd_ps(zmm7[0].o, zmm13, zmm4[0].o)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm1[0].o, 0x9c)
zmm13 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm5[0].o, 0x60)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm5[0].o, 0x8c)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm1[0].o, 0x20)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0x4e)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm5[0].o, 0x14)
zmm1[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm6[0].o, 4)
zmm5[0].o = _mm_broadcastss_ps(zmm0[0].o)
zmm6[0].o = _mm_permute_ps(zmm0[0].o, 0xd5)
zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6[0].o)
zmm6[0].o = _mm_fmadd_ps(zmm6[0].o, zmm13, zmm5[0].o)
uint32_t zmm11[0x8]
zmm11[0].o = _mm_permute_ps(zmm0[0].o, 0xea)
zmm11[0].o = _mm_fmadd_ps(zmm11[0].o, zmm1[0].o, zmm6[0].o)
zmm0[0].o = _mm_broadcastss_ps(zmm14)
zmm5[0].o = _mm_permute_ps(zmm14, 0xd5)
zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5[0].o)
zmm5[0].o = _mm_fmadd_ps(zmm5[0].o, zmm13, zmm0[0].o)
uint32_t zmm12[0x8]
zmm12[0].o = _mm_permute_ps(zmm14, 0xea)
zmm12[0].o = _mm_fmadd_ps(zmm12[0].o, zmm1[0].o, zmm5[0].o)
zmm0[0].o = _mm_broadcastss_ps(zmm7[0].o)
zmm5[0].o = _mm_permute_ps(zmm7[0].o, 0xd5)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5[0].o)
zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm13, zmm0[0].o)
zmm13 = _mm_fmadd_ps(_mm_permute_ps(zmm7[0].o, 0xea), zmm1[0].o, zmm4[0].o)
zmm0[0].o = arg1[0xf]
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
zmm0[0].o = _mm_broadcastss_ps(zmm0[0].o)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
zmm4[0].o = _mm_broadcast_ss(*(arg1 + 0xf4))
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
zmm5[0].o = _mm_broadcast_ss(*(arg1 + 0xfc))
zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm5[0].o)
zmm1[0].o = _mm_permute_pd(zmm1[0].o, 1)
zmm7[0].o = zmm1[0].o f* *(arg1 + 0xf8)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm4[0].o, 0x88)
zmm6[0].o = _mm_permute_ps(zmm1[0].o, 0xd8)
zmm14 = __vaddps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm6[0].o)
zmm15 = __vsubps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm5[0].o)
zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm4[0].o, 0x4e)
zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm0[0].o, 0xc)
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0x78)
zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm7[0].o, 0x1c)
zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm4[0].o, 0x60)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm4[0].o)
zmm4[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm10, zmm0[0].o)
zmm0[0].o = *(arg1 + 0x6c)
zmm6[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm9, arg1[7].d, 0x10)
zmm7[0].o = _mm_broadcastss_ps(zmm4[0].o)
zmm10 = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm7[0].o)
zmm7[0].o = _mm_permute_ps(zmm15, 0xea)
zmm7[0].o = _mm_fmadd_ps(zmm7[0].o, zmm6[0].o, zmm10)
zmm1[0].o = _mm_permute_ps(zmm4[0].o, 0xd5)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm1[0].o)
zmm5[0].o = _mm_permute_ps(zmm14, 0xea)
zmm1[0].o = _mm_fmadd_ps(zmm1[0].o, zmm0[0].o, zmm5[0].o)
zmm5[0].o = _mm_broadcastss_ps(zmm14)
zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm5[0].o)
zmm6[0].o = _mm_permute_ps(zmm15, 0xd5)
zmm5[0].o = _mm_fmadd_ps(zmm5[0].o, zmm0[0].o, zmm6[0].o)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm9, *(arg1 + 0x74), 0x20)
zmm6[0].o = _mm_permute_ps(zmm14, 0xd5)
zmm6[0].o = _mm_fmadd_ps(zmm6[0].o, zmm0[0].o, zmm7[0].o)
zmm7[0].o = _mm_broadcastss_ps(zmm15)
zmm7[0].o = _mm_fmadd_ps(zmm7[0].o, zmm0[0].o, zmm1[0].o)
zmm1[0].o = _mm_permute_ps(zmm4[0].o, 0xea)
zmm1[0].o = _mm_fmadd_ps(zmm1[0].o, zmm0[0].o, zmm5[0].o)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm14, 0x9c)
zmm9 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm15, 0x60)
zmm5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm15, 0x8c)
zmm10 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm14, 0x20)
zmm0[0].o = _mm_permute_ps(zmm6[0].o, 0xd5)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm10, zmm0[0].o)
zmm5[0].o = _mm_broadcastss_ps(zmm6[0].o)
zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm9, zmm5[0].o)
zmm5[0].o = _mm_permute_ps(zmm7[0].o, 0xd5)
zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm10, zmm5[0].o)
float zmm8[0x4] = _mm_broadcastss_ps(zmm7[0].o)
zmm5[0].o = _mm_fmadd_ps(zmm5[0].o, zmm9, zmm8)
arg3[0].o = _mm_permute_ps(zmm1[0].o, 0xd5)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm10, arg3[0].o)
arg4[0].o = _mm_broadcastss_ps(zmm1[0].o)
arg3[0].o = _mm_fmadd_ps(arg3[0].o, zmm9, arg4[0].o)
arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm14, 0x4e)
arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm15, 0x14)
arg4[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm4[0].o, 4)
zmm4[0].o = _mm_permute_ps(zmm6[0].o, 0xea)
zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, arg4[0].o, zmm0[0].o)
float var_138[0x4] = zmm4[0].o
zmm0[0].o = _mm_permute_ps(zmm7[0].o, 0xea)
zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, arg4[0].o, zmm5[0].o)
float var_148[0x4] = zmm0[0].o
zmm0[0].o = _mm_permute_ps(zmm1[0].o, 0xea)
zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, arg4[0].o, arg3[0].o)
float var_128[0x4] = zmm0[0].o
zmm7 = *arg6
zmm0[0].o = __vmovsd_xmmdq_memq(*arg2)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, arg2[1].d, 0x20)
zmm1[0].o = _mm_broadcast_ss(var_198)
zmm0[0].o = __vdivps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm1[0].o = __vmovsd_xmmdq_memq(arg1[0x10].q)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0x108), 0x20)
zmm1[0].o = __vaddps_xmmdq_xmmdq_memdq(zmm1[0].o, var_178)
arg3[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x10c))
arg3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg3[0].o, *(arg1 + 0x114), 0x20)
arg3[0].o = __vaddps_xmmdq_xmmdq_memdq(arg3[0].o, var_158)
int32_t temp0_171 = _mm256_movemask_ps(zmm7)
zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
arg3[0].o = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
arg3[0].o = zmm1[0].o f+ arg3[0]
zmm1[0].o = _mm_permute_pd(zmm1[0].o, 1)
zmm1[0].o = zmm1[0].o f+ arg3[0]
arg4[0].o = var_188
zmm4[0].o = _mm_permute_pd(arg4[0].o, 1)
zmm1[0].o = _mm_broadcastss_ps(zmm1[0].o)
arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm5[0].o = arg4[0].o
zmm1[0].o = __vmovshdup_xmmdq_xmmdq(arg4[0].o)
zmm0[0].o = var_168
zmm6[0].o = zmm0[0].o f* *(arg1 + 0x78)
uint32_t var_158_1[0x4] = zmm11[0].o
int32_t temp0_180 = __vextractps_gpr32_xmmdq_immb(zmm11[0].o, 0)
int32_t temp0_181 = __vextractps_gpr32_xmmdq_immb(zmm12[0].o, 0)
int32_t temp0_182 = __vextractps_gpr32_xmmdq_immb(zmm13, 0)
int32_t temp0_183 = __vextractps_gpr32_xmmdq_immb(zmm13, 1)
int32_t temp0_184 = __vextractps_gpr32_xmmdq_immb(zmm13, 2)
zmm0[0].o = *(arg1 + 0x7c)
int32_t var_198_1[0x4] = zmm0[0].o
float var_168_1[0x4] = zmm12[0].o
int32_t temp0_185 = __vextractps_gpr32_xmmdq_immb(zmm12[0].o, 1)
float var_178_1[0x4] = zmm6[0].o
int128_t var_1a8 = arg3[0].o
uint32_t result

if (temp0_171 != 0xff)
    zmm8 = __vxorps_xmmdq_xmmdq_xmmdq(zmm8, zmm8)
    bool cond:1_1 = zmm6[0].o f<= zmm8[0]
    zmm13 = __vxorps_xmmdq_xmmdq_xmmdq(zmm13, zmm13)
    zmm11[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm11[0].o)
    arg3[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg3[0].o)
    zmm10 = __vxorps_xmmdq_xmmdq_xmmdq(zmm10, zmm10)
    zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
    float var_118_1[0x4] = zmm0[0].o
    
    if (not(cond:1_1))
        zmm0[0].o = _mm_broadcast_ss(-0f)
        zmm7[0].o = __vxorpd_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
        zmm15 = __vxorps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
        zmm8 = __vxorpd_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
        zmm13 = _mm_permute_ps(zmm7[0].o, 0x80)
        zmm9 = zx.o(temp0_185)
        zmm12[0].o = zx.o(temp0_181)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm9, zmm12[0].o, 0x10)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm12[0].o, 0x20)
        zmm11[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm13, zmm0[0].o)
        arg3[0].o = _mm_permute_ps(zmm5[0].o, 0xd5)
        zmm14 = zx.o(temp0_183)
        zmm0[0].o = zx.o(temp0_182)
        arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm14, zmm0[0].o, 0x10)
        arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm0[0].o, 0x20)
        arg4[0].o = _mm_fmadd_ps(arg4[0].o, arg3[0].o, zmm11[0].o)
        zmm7[0].o = __vunpcklps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm4[0].o)
        zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm15, 0x20)
        zmm11[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, arg4[0].o)
        zmm7[0].o = _mm_permute_ps(zmm14, 0xe0)
        zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm9, 0x20)
        zmm10 = zx.o(temp0_184)
        arg4[0].o = __vpshufd_xmmdq_xmmdq_immb(zmm10, 0xe0)
        arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm14, 0x20)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
        arg3[0].o = _mm_fmadd_ps(arg3[0].o, zmm13, zmm7[0].o)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm8, 0x10)
        zmm1[0].o = zmm1[0].q | zmm5[0] << 0x40
        zmm1[0].o = _mm_fmadd_ps(zmm1[0].o, arg3[0].o, zmm11[0].o)
        arg3[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg3[0].o)
        zmm11[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm6[0].o, 1)
        zmm13 = __vaddps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm1[0].o)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm4[0].o, 0x10)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm15, 0x20)
        arg4[0].o = zx.o(temp0_180)
        arg4[0].o = _mm_broadcastd_epi32(arg4[0])
        arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
        zmm7[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm8, zmm5[0].o, 0x40)
        arg3[0].o = _mm_permute_ps(zmm0[0].o, 0xe0)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm12[0].o, 0x20)
        arg3[0].o = _mm_fmadd_ps(arg3[0].o, zmm7[0].o, arg4[0].o)
        arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm15, 0x10)
        arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm15, 0x20)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg3[0].o)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm12[0].o, 0x10)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm12[0].o, 0x20)
        arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm10, zmm14, 0x10)
        arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm9, 0x20)
        arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, arg4[0].o)
        arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm1[0].o, zmm0[0].o)
        zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm8, zmm5[0].o, 0)
        zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5[0].o, 0xc8)
        zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, arg4[0].o, arg3[0].o)
        zmm1[0].o = __vmovddup_xmmdq_xmmq(zmm11[0].q)
        arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
        zmm10 = __vmovshdup_xmmdq_xmmdq(zmm13)
        var_118_1 = zmm13
        zmm11[0].o = _mm_permute_pd(zmm13, 1)
        zmm13 = __vmovshdup_xmmdq_xmmdq(arg3[0].o)
        zmm8 = _mm_permute_pd(arg3[0].o, 1)
    
    int32_t var_188_2 = arg3[0]
    zmm1[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x8c))
    zmm4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0x94), 0x20)
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(arg1 + 0xd4), 0x20)
    zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
    zmm9 = _mm_broadcast_ss(-0f)
    zmm12[0].o = _mm_permute_pd(zmm1[0].o, 1)
    zmm7[0].o = __vxorpd_xmmdq_xmmdq_xmmdq(zmm12[0].o, zmm9)
    arg4[0].o = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
    zmm15 = __vxorps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm9)
    arg3[0].o = _mm_permute_ps(zmm7[0].o, 0x80)
    zmm6[0].o = _mm_permute_ps(var_148, 0xc1)
    zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, arg3[0].o)
    zmm0[0].o = _mm_permute_ps(zmm1[0].o, 0xd5)
    zmm5[0].o = var_128
    zmm4[0].o = _mm_permute_ps(zmm5[0].o, 0xc1)
    zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm0[0].o, zmm6[0].o)
    zmm6[0].o = __vmovshdup_xmmdq_xmmdq(zmm5[0].o)
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, var_148, 0x68)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, arg3[0].o)
    zmm6[0].o = _mm_permute_ps(zmm5[0].o, 0xda)
    zmm6[0].o = _mm_fmadd_ps(zmm6[0].o, zmm0[0].o, arg3[0].o)
    float var_108_1[0x4] = zmm9
    zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm9)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm1[0].o, 0x9c)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm15, 0x20)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm0[0].o, 0x10)
    arg4[0].o = arg4[0].q | zmm1[0].q << 0x40
    arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm6[0].o)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm4[0].o, arg3[0].o)
    arg3[0].o = _mm_permute_ps(zmm1[0].o, 0x4a)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm15, 0x20)
    zmm4[0].o = __vpermilps_xmmdq_memdq_immb(var_138, 0xc0)
    zmm6[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0x40)
    zmm7[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(var_148, zmm5[0].o, 0x30)
    zmm7[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm7[0].o, 0x80)
    zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm6[0].o)
    zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, arg3[0].o, zmm7[0].o)
    zmm7[0].o = _mm_permute_ps(zmm5[0].o, 0x46)
    zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, var_148, 0x68)
    zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm6[0].o)
    zmm7[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, var_148, 0xc)
    zmm7[0].o = _mm_permute_ps(zmm7[0].o, 0x78)
    zmm6[0].o = _mm_fmadd_ps(zmm6[0].o, arg3[0].o, zmm7[0].o)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm12[0].o, zmm15, 0x10)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm15, 0x20)
    zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0)
    zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0xc8)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm6[0].o)
    zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm4[0].o, arg3[0].o)
    zmm12[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm12[0].o, zmm12[0].o)
    zmm1[0].o = __vblendps_xmmdq_xmmdq_memdq_immb(zmm12[0].o, var_198_1, 1)
    arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
    zmm1[0].o = __vmovddup_xmmdq_xmmq(zmm1[0].q)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
    zmm1[0].o = var_118_1
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm10, 0x10)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm10, var_188_2, 0x10)
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm11[0].o, 0x20)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm13, 0x20)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm11[0].o, zmm13, 0x10)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm8, 0x20)
    zmm5[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm0[0].o, 5)
    zmm5[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm0[0].o, 0xd8)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm5[0].o)
    zmm5[0].o = _mm_permute_pd(arg3[0].o, 1)
    zmm0[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5[0].o, 1)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm0[0].o)
    arg4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
    int32_t temp0_525 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 0)
    int32_t temp0_526 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 1)
    int32_t temp0_527 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 2)
    arg3 = _mm256_blend_ps(_mm256_broadcast_ss(1.17549435e-38f), arg4, 7)
    zmm0 = _mm256_broadcast_ss(nanf)
    zmm4 = _mm256_and_ps(arg3, zmm0)
    arg3 = _mm256_broadcast_ss(9.99999994e-09f)
    zmm5 = _mm256_cmp_ps(zmm4, arg3, 2)
    zmm4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm4[0].o)
    zmm4 = _mm256_cmpeq_epi32(zmm7, zmm4)
    zmm5 = _mm256_or_ps(zmm5, zmm4)
    zmm6[0].o = _mm256_extractf128_ps(zmm5[0].o, 1)
    zmm5[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm6[0].o)
    zmm5[0].o = __vpacksswb_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
    
    if (__vpmovmskb_gpr32d_xmmdq(zmm5[0].o) != 0xff)
    label_14000f740:
        zmm0[0].o = __vmovshdup_xmmdq_xmmdq(arg4[0].o)
        arg3[0].o = _mm_permute_pd(arg4[0].o, 1)
        zmm8 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg4[0].o, 0x8a)
        zmm15 = zx.o(temp0_525)
        zmm7[0].o = zx.o(temp0_526)
        zmm10 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm15, zmm7[0].o, 0x1c)
        zmm6[0].o = zx.o(temp0_527)
        zmm4[0].o = data_142d3f660
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, arg3[0].o, 0x10)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm15, 0x20)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm6[0].o, 0x30)
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm1[0].o, 0x46)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm7[0].o, 0x2a)
        zmm5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm1[0].o, 0x9c)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm5[0].o)
        zmm0[0].o = _mm_fmsub_ps(zmm0[0].o, arg3[0].o, zmm4[0].o)
        zmm14 = _mm_broadcastss_ps(zmm0[0].o)
        zmm11[0].o = _mm_permute_ps(zmm0[0].o, 0x55)
        zmm9 = _mm_permute_ps(zmm0[0].o, 0xaa)
        zmm13 = _mm_permute_ps(zmm0[0].o, 0xff)
        zmm4[0].o = 0x3f800000
        arg3[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(0x3f800000, zmm6[0].o, 0)
        zmm5[0].o = __vunpcklps_xmmdq_xmmdq_xmmdq(zmm15, zmm7[0].o)
        zmm5[0].o = zmm12[0].q | zmm5[0] << 0x40
        zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm12[0].o)
        arg3[0].o = _mm_fmsub_ps(arg3[0].o, zmm10, zmm5[0].o)
        zmm5[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg4[0].o, 5)
        zmm7[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm1[0].o, 5)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm1[0].o, 0x8a)
        zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm0[0].o)
        zmm7[0].o = _mm_fmsub_ps(zmm7[0].o, zmm8, zmm5[0].o)
        zmm0[0].o = _mm_permute_ps(arg3[0].o, 0xd8)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm0[0].o)
        zmm5[0].o = _mm_permute_pd(zmm0[0].o, 1)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm5[0].o)
        zmm5[0].o = _mm_permute_ps(zmm0[0].o, 0x39)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm5[0].o)
        zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm9)
        zmm5[0].o = _mm_fmadd_ps(zmm5[0].o, zmm14, zmm13)
        zmm12[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
        zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        zmm12[0].o f- zmm0[0]
        
        if (zmm12[0].o f!= zmm0[0] || not(is_ordered.d(zmm12[0].o, zmm0[0])))
            zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm6[0].o, 1f, 0x36)
            zmm4[0].o = data_142d4cc30
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm6[0].o, 0x10)
            zmm5[0].o = _mm_permute_ps(zmm7[0].o, 0x66)
            zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5[0].o)
            zmm5[0].o = _mm_permute_ps(zmm7[0].o, 0x33)
            zmm4[0].o = _mm_fmsub_ps(zmm4[0].o, zmm0[0].o, zmm5[0].o)
            zmm11[0].o = _mm_fmsub_ps(zmm11[0].o, zmm10, zmm4[0].o)
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm15, 0x1d)
            zmm5[0].o = __vmovddup_xmmdq_xmmq(zmm7[0].q)
            zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5[0].o)
            zmm5[0].o = _mm_permute_pd(zmm7[0].o, 3)
            zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm10, zmm5[0].o)
            zmm5[0].o = arg4[0].q | zmm1[0].q << 0x40
            zmm14 = _mm_fmsub_ps(zmm14, zmm0[0].o, zmm4[0].o)
            zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm1[0].o, 0x20)
            zmm4[0].o = __vmovddup_xmmdq_xmmq(arg3[0].q)
            zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm4[0].o)
            zmm4[0].o = _mm_broadcastss_ps(arg3[0].o)
            zmm0[0].o = _mm_fmsub_ps(zmm0[0].o, zmm5[0].o, zmm4[0].o)
            zmm9 = _mm_fmsub_ps(zmm9, zmm8, zmm0[0].o)
            zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x9d)
            zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0xb0)
            zmm1[0].o = _mm_permute_ps(arg3[0].o, 0x66)
            zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
            zmm1[0].o = _mm_permute_ps(arg3[0].o, 0xcc)
            zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm8, zmm1[0].o)
            zmm13 = _mm_fmsub_ps(zmm13, zmm5[0].o, zmm0[0].o)
            zmm0[0].o = data_142fc92f0
            zmm0[0].o = __vdivps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm12[0].o)
            zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm13, zmm0[0].o)
            arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm0[0].o)
            arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm9, zmm0[0].o)
            zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm14, zmm0[0].o)
            zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm0[0].o, 0x77)
            zmm4[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg3[0].o, 0x77)
            zmm1[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg3[0].o, 0x22)
        else
            zmm1[0].o = data_142d4cc20
            zmm0[0].o = data_142d4cc30
        
        zmm14 = arg5
        zmm7[0].o = var_1a8
        zmm13 = var_108_1
        zmm8 = __vmovshdup_xmmdq_xmmdq(zmm4[0].o)
        zmm9 = _mm_permute_pd(zmm4[0].o, 1)
        zmm10 = _mm_permute_ps(zmm4[0].o, 0xe7)
        zmm12[0].o = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
        zmm11[0].o = _mm_permute_pd(zmm1[0].o, 1)
        arg3[0].o = _mm_permute_ps(zmm1[0].o, 0xe7)
        zmm5[0].o = __vmovshdup_xmmdq_xmmdq(zmm0[0].o)
        zmm6[0].o = _mm_permute_pd(zmm0[0].o, 1)
        arg4[0].o = _mm_permute_ps(zmm0[0].o, 0xe7)
    else
        zmm5[0].o = 0x800000
        zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, 0x800000, 0x30)
        zmm5[0].o = _mm_broadcastss_ps(0x800000)
        zmm5 = _mm256_cmp_ps(_mm256_and_ps(_mm256_insertf128_ps(zmm6, zmm5[0].o, 1), zmm0), arg3, 2)
        zmm4 = _mm256_or_ps(zmm5, zmm4)
        zmm5[0].o = _mm256_extractf128_ps(zmm4[0].o, 1)
        zmm4[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5[0].o)
        zmm4[0].o = __vpacksswb_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
        
        if (__vpmovmskb_gpr32d_xmmdq(zmm4[0].o) != 0xff)
            goto label_14000f740
        
        zmm4[0].o = zx.o(0)
        zmm4 = _mm256_cmpeq_epi32(zmm7, zmm4)
        zmm5[0].o = zx.o(temp0_525)
        zmm6[0].o = zx.o(temp0_526)
        zmm5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm6[0].o, 0x10)
        zmm6[0].o = zx.o(temp0_527)
        zmm5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm6[0].o, 0x20)
        zmm6[0].o = 0x800000
        zmm5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, 0x800000, 0x30)
        zmm6[0].o = _mm_broadcastss_ps(0x800000)
        zmm5 = _mm256_insertf128_ps(zmm5, zmm6[0].o, 1)
        zmm0 = _mm256_or_ps(_mm256_cmp_ps(_mm256_and_ps(zmm5, zmm0), arg3, 2), zmm4)
        arg3[0].o = _mm256_extractf128_ps(zmm0[0].o, 1)
        zmm0[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
        zmm0[0].o = __vpacksswb_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        
        if (__vpmovmskb_gpr32d_xmmdq(zmm0[0].o) != 0xff)
            goto label_14000f740
        
        zmm6[0].o = 0x3f800000
        arg4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
        zmm5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm5[0].o)
        zmm0[0].o = zx.o(0)
        arg3[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg3[0].o)
        zmm11[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm11[0].o)
        zmm12[0].o = 0x3f800000
        zmm1[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm1[0].o)
        zmm10 = __vxorps_xmmdq_xmmdq_xmmdq(zmm10, zmm10)
        zmm9 = __vxorpd_xmmdq_xmmdq_xmmdq(zmm9, zmm9)
        zmm8 = __vxorps_xmmdq_xmmdq_xmmdq(zmm8, zmm8)
        zmm4[0].o = 0x3f800000
        zmm14 = arg5
        zmm7[0].o = var_1a8
        zmm13 = var_108_1
    
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5[0].o, 0x10)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm6[0].o, 0x20)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x30)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm12[0].o, 0x10)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm11[0].o, 0x20)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg3[0].o, 0x30)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm8, 0x10)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm9, 0x20)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm10, 0x30)
    arg4[0].o = _mm_broadcast_ss(*arg2)
    zmm4[0].o = _mm_broadcast_ss(*(arg2 + 4))
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, arg3[0].o, zmm4[0].o)
    zmm5[0].o = _mm_broadcast_ss(arg2[1].d)
    zmm5[0].o = _mm_fmadd_ps(zmm5[0].o, zmm0[0].o, arg4[0].o)
    arg4[0].o = _mm_broadcastss_ps(zmm7[0].o)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg3[0].o)
    arg4[0].o = _mm_permute_ps(zmm7[0].o, 0x55)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm1[0].o, arg3[0].o)
    zmm9 = _mm_fmadd_ps(_mm_permute_ps(zmm7[0].o, 0xaa), zmm0[0].o, arg4[0].o)
    zmm0[0].o = __vmovsd_xmmdq_memq(arg1[8].q)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg1 + 0x88), 0x20)
    zmm12[0].o = __vxorps_xmmdq_xmmdq_memdq(zmm13, var_198_1)
    zmm0[0].o = __vmovsd_xmmdq_memq(arg1[0xc].q)
    zmm13 = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg1 + 0xc8), 0x20)
    zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm13)
    arg4[0].o = _mm_permute_ps(zmm1[0].o, 0xc9)
    arg3[0].o = _mm_permute_ps(zmm5[0].o, 0xd2)
    zmm6[0].o = _mm_permute_ps(zmm1[0].o, 0xd2)
    zmm1[0].o = _mm_permute_ps(zmm5[0].o, 0xc9)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm6[0].o)
    arg3[0].o = _mm_fmsub_ps(arg3[0].o, arg4[0].o, zmm1[0].o)
    zmm1[0].o = zx.o(temp0_180)
    zmm8 = __vpblendd_xmmdq_xmmdq_memdq_immb(zmm1[0].o, var_158_1, 0xe)
    zmm1[0].o = zx.o(temp0_181)
    zmm7[0].o = zx.o(temp0_185)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm7[0].o, 0x10)
    zmm7[0].o = __vblendps_xmmdq_xmmdq_memdq_immb(zmm1[0].o, var_168_1, 0xc)
    zmm1[0].o = zx.o(temp0_182)
    zmm0[0].o = zx.o(temp0_183)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm0[0].o, 0x10)
    zmm1[0].o = zx.o(temp0_184)
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0x20)
    float var_1a8_2[0x4] = zmm4[0].o
    zmm0[0].o = _mm_permute_ps(zmm9, 0xc9)
    zmm10 = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm6[0].o)
    zmm0[0].o = _mm_permute_ps(zmm9, 0xd2)
    zmm10 = _mm_fmsub_ps(zmm10, arg4[0].o, zmm0[0].o)
    zmm0[0].o = _mm_broadcastss_ps(arg3[0].o)
    zmm1[0].o = _mm_permute_ps(arg3[0].o, 0xd5)
    arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm1[0].o)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm8, zmm0[0].o)
    zmm0[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x8c))
    zmm1[0].o = _mm_permute_ps(arg3[0].o, 0xea)
    zmm1[0].o = _mm_fmadd_ps(zmm1[0].o, zmm4[0].o, arg4[0].o)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg1 + 0x94), 0x20)
    arg3[0].o = _mm_permute_ps(zmm10, 0xd5)
    zmm11[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, arg3[0].o)
    arg3[0].o = _mm_broadcastss_ps(zmm10)
    zmm11[0].o = _mm_fmadd_ps(zmm11[0].o, zmm8, arg3[0].o)
    arg3[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
    arg3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg3[0].o, *(arg1 + 0xd4), 0x20)
    zmm0[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    arg4[0].o = _mm_permute_ps(zmm0[0].o, 0xc9)
    float var_188_4[0x4] = arg4[0].o
    zmm6[0].o = _mm_permute_ps(zmm0[0].o, 0xd2)
    float var_158_3[0x4] = zmm6[0].o
    zmm0[0].o = _mm_broadcast_ss(-0f)
    int32_t var_168_3[0x4] = zmm0[0].o
    zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
    zmm4[0].o = _mm_permute_ps(zmm0[0].o, 0xd2)
    zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xc9)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm6[0].o)
    zmm4[0].o = _mm_fmsub_ps(zmm4[0].o, arg4[0].o, zmm0[0].o)
    zmm15 = _mm_broadcast_ss(var_178_1[0])
    zmm8 = _mm_broadcastss_ps(zmm14)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm15, zmm5[0].o)
    zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm8, zmm13)
    zmm7[0].o = _mm_permute_ps(zmm4[0].o, 0xd5)
    zmm13 = __vmulps_xmmdq_xmmdq_memdq(zmm7[0].o, var_148)
    arg1[0xc].d = zmm0[0]
    zmm7[0].o = _mm_broadcastss_ps(zmm4[0].o)
    zmm13 = __vfmadd231ps_xmmdq_xmmdq_memdq(zmm13, zmm7[0].o, var_138)
    *(arg1 + 0xc4) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 1)
    *(arg1 + 0xc8) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 2)
    zmm14 = _mm_broadcastss_ps(zmm12[0].o)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm14, zmm5[0].o)
    zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm8, arg3[0].o)
    *(arg1 + 0xcc) = zmm0[0]
    arg3[0].o = _mm_permute_ps(zmm4[0].o, 0xea)
    arg1[0xd].d = __vextractps_memd_xmmdq_immb(zmm0[0].o, 1)
    arg3[0].o = __vfmadd132ps_xmmdq_xmmdq_memdq(arg3[0].o, zmm13, var_128)
    *(arg1 + 0xd4) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 2)
    zmm0[0].o = arg1[0xe]
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm8, zmm1[0].o)
    zmm4[0].o = _mm_broadcastss_ps(zmm1[0].o)
    zmm5[0].o = _mm_permute_ps(zmm0[0].o, 0x1b)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5[0].o)
    zmm5[0].o = _mm_permute_ps(zmm1[0].o, 0x55)
    zmm7[0].o = _mm_permute_pd(zmm0[0].o, 1)
    zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm7[0].o)
    arg4[0].o = __vpbroadcastq_xmmdq_memq(-0x407fffffc0800000)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg4[0].o)
    zmm5[0].o = arg4[0].o
    float var_198_3[0x4] = arg4[0].o
    zmm13 = __vxorps_xmmdq_xmmdq_xmmdq(zmm13, zmm13)
    zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm0[0].o, zmm13)
    arg4[0].o = data_142d3f7d0
    zmm7[0].o = _mm_fmadd_ps(zmm7[0].o, arg4[0].o, zmm4[0].o)
    zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0xaa)
    zmm4[0].o = _mm_permute_ps(zmm0[0].o, 0xb1)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
    zmm1[0].o = data_142d3f7b0
    zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm1[0].o, zmm7[0].o)
    zmm6[0].o = zmm1[0].o
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm8, arg3[0].o)
    zmm12[0].o = arg1[0xf]
    arg3[0].o = _mm_broadcastss_ps(zmm1[0].o)
    zmm7[0].o = _mm_permute_ps(zmm12[0].o, 0x1b)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm7[0].o)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm5[0].o)
    arg3[0].o = _mm_fmadd_ps(arg3[0].o, zmm12[0].o, zmm13)
    zmm7[0].o = _mm_permute_ps(zmm1[0].o, 0x55)
    zmm5[0].o = _mm_permute_pd(zmm12[0].o, 1)
    zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm5[0].o)
    zmm5[0].o = _mm_fmadd_ps(zmm5[0].o, arg4[0].o, arg3[0].o)
    zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0xaa)
    arg3[0].o = _mm_permute_ps(zmm12[0].o, 0xb1)
    arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm6[0].o, zmm5[0].o)
    arg3[0].o = _mm_broadcastss_ps(0x3f000000)
    zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, arg3[0].o, zmm0[0].o)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, arg3[0].o, zmm12[0].o)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm4[0].o)
    arg3[0].o = _mm_permute_pd(zmm0[0].o, 1)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    arg3[0].o = __vmovshdup_xmmdq_xmmdq(zmm0[0].o)
    zmm1[0].o = zmm0[0].o f+ arg3[0]
    arg3[0].o = zmm1[0].o f* 0.5f
    zmm5[0].o = __vrsqrtss_xmmdq_xmmdq_xmmd(zmm1[0].o, zmm1[0])
    zmm7[0].o = zmm5[0].o f* zmm5[0].d
    zmm7[0].o = _mm_fnmadd_ss(zmm7[0].o, arg3[0], 0.5f)
    zmm7[0].o = _mm_fmadd_ss(zmm7[0].o, zmm5[0].d, zmm5[0].d)
    zmm5[0].o = zmm7[0].o f* zmm7[0]
    zmm5[0].o = _mm_fnmadd_ss(zmm5[0].o, arg3[0], 0.5f)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
    zmm6[0].o = _mm_permute_pd(arg3[0].o, 1)
    arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm6[0].o)
    zmm6[0].o = __vmovshdup_xmmdq_xmmdq(arg3[0].o)
    arg3[0].o = arg3[0].o f+ zmm6[0].d
    zmm12[0].o = arg3[0].o f* 0.5f
    zmm6[0].o = __vrsqrtss_xmmdq_xmmdq_xmmd(arg3[0].o, arg3[0])
    zmm0[0].o = zmm6[0].o f* zmm6[0].d
    zmm0[0].o = _mm_fnmadd_ss(zmm0[0].o, zmm12[0], 0.5f)
    zmm0[0].o = _mm_fmadd_ss(zmm0[0].o, zmm6[0].d, zmm6[0].d)
    zmm6[0].o = zmm0[0].o f* zmm0[0]
    zmm6[0].o = _mm_fnmadd_ss(zmm6[0].o, zmm12[0], 0.5f)
    zmm5[0].o = _mm_fmadd_ss(zmm5[0].o, zmm7[0], zmm7[0])
    zmm5[0].o = _mm_broadcastss_ps(zmm5[0].o)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5[0].o)
    zmm5[0].o = 0x322bcc77
    zmm1[0].o = _mm_cmp_ss(0x322bcc77, zmm1[0], 6)
    zmm7[0].o = 0xffffffff
    zmm1[0].o = __vandnps_xmmdq_xmmdq_xmmdq(zmm1[0].o, 0xffffffff)
    zmm1 = _mm256_broadcastss_ps(zmm1[0].o)
    zmm12 = __vandnps_ymmqq_ymmqq_memqq(zmm1, data_142fc9280)
    zmm1 = _mm256_and_ps(zmm4, zmm1)
    zmm13 = _mm256_or_ps(zmm1, zmm12).o
    zmm1[0].o = _mm_cmp_ss(0x322bcc77, arg3[0], 6)
    zmm1[0].o = __vandnps_xmmdq_xmmdq_xmmdq(zmm1[0].o, 0xffffffff)
    zmm6[0].o = _mm_fmadd_ss(zmm6[0].o, zmm0[0], zmm0[0])
    zmm0[0].o = _mm_broadcastss_ps(zmm6[0].o)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm0[0].o)
    zmm1[0].o = _mm_broadcastss_ps(zmm1[0].o)
    zmm0[0].o = __vandps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
    zmm1[0].o = __vandnps_xmmdq_xmmdq_memdq(zmm1[0].o, data_142d3f660)
    zmm0[0].o = __vorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm13, zmm0[0].o)
    arg3[0].o = _mm_permute_pd(zmm1[0].o, 1)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
    arg3[0].o = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
    zmm1[0].o = zmm1[0].o f+ arg3[0]
    arg3[0].o = _mm_permute_ps(zmm10, 0xea)
    arg4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
    zmm1[0].o = _mm_cmp_ss(arg4[0].o, zmm1[0], 2)
    arg4[0].o = 0x3f800000
    zmm1[0].o = __vblendvps_xmmdq_xmmdq_xmmdq_xmmdq(0xbf800000, 0x3f800000, zmm1[0].o)
    arg3[0].o = __vfmadd132ps_xmmdq_xmmdq_memdq(arg3[0].o, zmm11[0].o, var_1a8_2)
    zmm1[0].o = _mm_broadcastss_ps(zmm1[0].o)
    arg1[0xe] = zmm13
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
    arg1[0xf] = zmm0[0].o
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm15, zmm9)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm14, zmm9)
    arg4[0].o = __vmovsd_xmmdq_memq(arg1[0x10].q)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg4[0].o, *(arg1 + 0x108), 0x20)
    zmm5[0].o = __vxorps_xmmdq_xmmdq_memdq(zmm9, var_168_3)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm8, zmm0[0].o)
    arg1[0x10].d = arg4[0]
    *(arg1 + 0x104) = __vextractps_memd_xmmdq_immb(arg4[0].o, 1)
    zmm0[0].o = _mm_permute_ps(zmm5[0].o, 0xc9)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm0[0].o, var_158_3)
    *(arg1 + 0x108) = __vextractps_memd_xmmdq_immb(arg4[0].o, 2)
    arg4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x10c))
    arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg4[0].o, *(arg1 + 0x114), 0x20)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm8, zmm1[0].o)
    int64_t rax_8 = __vpextrq_gpr64q_xmmdq_immb(zmm13, 1)
    *(arg1 + 0x10c) = arg4[0]
    zmm1[0].o = _mm_permute_ps(zmm5[0].o, 0xd2)
    arg1[0x11].d = __vextractps_memd_xmmdq_immb(arg4[0].o, 1)
    zmm0[0].o = __vfmsub231ps_xmmdq_xmmdq_memdq(zmm0[0].o, zmm1[0].o, var_188_4)
    *(arg1 + 0x114) = __vextractps_memd_xmmdq_immb(arg4[0].o, 2)
    zmm1[0].o = _mm_broadcastss_ps(zmm0[0].o)
    arg4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x118))
    arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg4[0].o, arg1[0x12].d, 0x20)
    zmm5[0].o = _mm_permute_ps(zmm0[0].o, 0xd5)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm8, arg3[0].o)
    *(arg1 + 0x118) = arg4[0]
    arg3[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm5[0].o, var_148)
    *(arg1 + 0x11c) = __vextractps_memd_xmmdq_immb(arg4[0].o, 1)
    arg3[0].o = __vfmadd231ps_xmmdq_xmmdq_memdq(arg3[0].o, zmm1[0].o, var_138)
    arg1[0x12].d = __vextractps_memd_xmmdq_immb(arg4[0].o, 2)
    zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xea)
    zmm1[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x124))
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0x12c), 0x20)
    zmm0[0].o = __vfmadd132ps_xmmdq_xmmdq_memdq(zmm0[0].o, arg3[0].o, var_128)
    zmm1[0].o = _mm_fmadd_ps(zmm1[0].o, zmm8, zmm0[0].o)
    *(arg1 + 0x124) = zmm1[0]
    *(arg1 + 0x128) = __vextractps_memd_xmmdq_immb(zmm1[0].o, 1)
    *(arg1 + 0x12c) = __vextractps_memd_xmmdq_immb(zmm1[0].o, 2)
    int64_t rdx_2 = zmm13[0].q
    zmm0[0].o = zx.o(rdx_2.d)
    zmm1[0].o = zx.o(rax_8.d)
    result = (rax_8 u>> 0x20).d
    arg3[0].o = zx.o((rdx_2 u>> 0x20).d)
    arg4[0].o = *(arg1 + 0x18)
    zmm5[0].o = arg1[1].d
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm1[0].o, 0x10)
    zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm5[0].o, 0x10)
    zmm7[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm7[0].o, *(arg1 + 0x14), 0x20)
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm0[0].o, 0x20)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm0[0].o, 0x10)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg3[0].o, 0x20)
    zmm1[0].o = __vmovsd_xmmdq_memq(arg1[1].q)
    arg3[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x14))
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm5[0].o, 0x20)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    arg3[0].o = _mm_fmsub_ps(arg3[0].o, zmm6[0].o, zmm7[0].o)
    arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg3[0].o)
    zmm5[0].o = zx.o(result)
    zmm5[0].o = _mm_broadcastd_epi32(zmm5[0].d)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg4[0].o, 0x20)
    zmm5[0].o = _mm_fmadd_ps(zmm5[0].o, arg3[0].o, zmm1[0].o)
    zmm1[0].o = _mm_permute_ps(arg3[0].o, 0xd2)
    arg3[0].o = _mm_permute_ps(arg3[0].o, 0xc9)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    zmm0[0].o = _mm_fmsub_ps(zmm0[0].o, zmm6[0].o, zmm1[0].o)
    zmm1[0].o = __vmovsd_xmmdq_memq(arg1[0xc].q)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0xc8), 0x20)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
    arg1[8].d = zmm0[0]
    *(arg1 + 0x84) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 1)
    *(arg1 + 0x88) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 2)
    zmm0[0].o = arg1[0xf].d
    zmm1[0].o = *(arg1 + 0xf8)
    arg3[0].o = *(arg1 + 0x48)
    arg4[0].o = arg1[4].d
    zmm5[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xf4))
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, arg4[0].o, 0x10)
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm6[0].o, *(arg1 + 0x44), 0x20)
    zmm5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm0[0].o, 0x20)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm0[0].o, 0x10)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg1 + 0xf4), 0x20)
    zmm7[0].o = __vmovsd_xmmdq_memq(arg1[4].q)
    zmm0[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x44))
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x20)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
    zmm0[0].o = _mm_fmsub_ps(zmm0[0].o, zmm5[0].o, zmm6[0].o)
    arg4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
    zmm10 = _mm_broadcast_ss(*(arg1 + 0xfc))
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, arg3[0].o, 0x20)
    arg3[0].o = _mm_fmadd_ps(arg3[0].o, zmm10, arg4[0].o)
    zmm6[0].o = _mm_permute_ps(arg4[0].o, 0xd2)
    arg4[0].o = _mm_permute_ps(arg4[0].o, 0xc9)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
    zmm1[0].o = _mm_fmsub_ps(zmm1[0].o, zmm5[0].o, zmm6[0].o)
    arg4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
    arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg4[0].o, *(arg1 + 0xd4), 0x20)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm1[0].o)
    *(arg1 + 0x8c) = zmm1[0]
    arg1[9].d = __vextractps_memd_xmmdq_immb(zmm1[0].o, 1)
    *(arg1 + 0x94) = __vextractps_memd_xmmdq_immb(zmm1[0].o, 2)
    zmm5[0].o = _mm_broadcast_ss(*(arg1 + 0xec))
    zmm1[0].o = *arg1
    zmm7[0].o = _mm_broadcast_ss(arg1[0xe].d)
    zmm9 = *(arg1 + 0xc)
    zmm11[0].o = *(arg1 + 8)
    zmm13 = *(arg1 + 4)
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm9, zmm11[0].o, 0x10)
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm13, 0x20)
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm1[0].o, 0x30)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm4[0].o)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, var_198_3)
    zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm1[0].o, zmm5[0].o)
    zmm5[0].o = _mm_broadcast_ss(*(arg1 + 0xe4))
    zmm7[0].o = __vmovsd_xmmdq_memq(*(arg1 + 8))
    zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm1[0].o, 0x20)
    zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm13, 0x30)
    zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm5[0].o)
    zmm15 = data_142d3f7d0
    zmm5[0].o = _mm_fmadd_ps(zmm5[0].o, zmm15, zmm4[0].o)
    zmm14 = *(arg1 + 0x3c)
    zmm7[0].o = *(arg1 + 0x38)
    zmm0[0].o = *(arg1 + 0x34)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm14, zmm7[0].o, 0x10)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm0[0].o, 0x20)
    arg4[0].o = arg1[3]
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, arg4[0].o, 0x30)
    zmm6[0].o = _mm_broadcast_ss(arg1[0xf].d)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, arg3[0].o)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, var_198_3)
    arg3[0].o = _mm_fmadd_ps(arg3[0].o, arg4[0].o, zmm10)
    zmm6[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x38))
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, arg4[0].o, 0x20)
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm0[0].o, 0x30)
    zmm4[0].o = _mm_broadcast_ss(*(arg1 + 0xf4))
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm4[0].o)
    zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm15, arg3[0].o)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm13, zmm1[0].o, 0x10)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm9, 0x20)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm11[0].o, 0x30)
    arg3[0].o = _mm_broadcast_ss(*(arg1 + 0xe8))
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
    zmm6[0].o = data_142d3f7b0
    zmm1[0].o = _mm_fmadd_ps(zmm1[0].o, zmm6[0].o, zmm5[0].o)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x10)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm14, 0x20)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm7[0].o, 0x30)
    arg3[0].o = _mm_broadcast_ss(*(arg1 + 0xf8))
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm6[0].o, zmm4[0].o)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
    arg4[0].o = _mm_permute_pd(arg3[0].o, 1)
    arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
    arg4[0].o = __vmovshdup_xmmdq_xmmdq(arg3[0].o)
    arg3[0].o = arg3[0].o f+ arg4[0]
    arg4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
    arg3[0].o = _mm_cmp_ss(arg4[0].o, arg3[0], 2)
    arg3[0].o = __vblendvps_xmmdq_xmmdq_xmmdq_xmmdq(0xbf800000, 0x3f800000, arg3[0].o)
else
    zmm10 = __vxorps_xmmdq_xmmdq_xmmdq(zmm10, zmm10)
    bool cond:2_1 = zmm6[0].o f<= zmm10[0]
    zmm15 = __vxorps_xmmdq_xmmdq_xmmdq(zmm15, zmm15)
    zmm13 = __vxorps_xmmdq_xmmdq_xmmdq(zmm13, zmm13)
    zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
    zmm14 = __vxorps_xmmdq_xmmdq_xmmdq(zmm14, zmm14)
    zmm8 = __vxorps_xmmdq_xmmdq_xmmdq(zmm8, zmm8)
    
    if (not(cond:2_1))
        zmm0[0].o = _mm_broadcast_ss(-0f)
        zmm7[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
        zmm12[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
        zmm9 = __vxorps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
        zmm11[0].o = _mm_permute_ps(zmm7[0].o, 0x80)
        zmm10 = zx.o(temp0_185)
        arg4[0].o = zx.o(temp0_181)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm10, arg4[0].o, 0x10)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x20)
        zmm8 = __vmulps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm0[0].o)
        zmm13 = _mm_permute_ps(zmm5[0].o, 0xd5)
        zmm6[0].o = zx.o(temp0_183)
        zmm0[0].o = zx.o(temp0_182)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm0[0].o, 0x10)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm0[0].o, 0x20)
        arg3[0].o = _mm_fmadd_ps(arg3[0].o, zmm13, zmm8)
        zmm7[0].o = __vunpcklps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm4[0].o)
        zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm12[0].o, 0x20)
        zmm8 = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, arg3[0].o)
        zmm7[0].o = _mm_permute_ps(zmm6[0].o, 0xe0)
        zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm10, 0x20)
        zmm14 = zx.o(temp0_184)
        arg3[0].o = __vpshufd_xmmdq_xmmdq_immb(zmm14, 0xe0)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm6[0].o, 0x20)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm13, arg3[0].o)
        arg3[0].o = _mm_fmadd_ps(arg3[0].o, zmm11[0].o, zmm7[0].o)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm9, 0x10)
        zmm1[0].o = zmm1[0].q | zmm5[0] << 0x40
        zmm1[0].o = _mm_fmadd_ps(zmm1[0].o, arg3[0].o, zmm8)
        arg3[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg3[0].o)
        zmm11[0].o = __vblendps_xmmdq_xmmdq_memdq_immb(arg3[0].o, var_178_1, 1)
        zmm8 = __vaddps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm1[0].o)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm4[0].o, 0x10)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm12[0].o, 0x20)
        zmm7[0].o = zx.o(temp0_180)
        zmm7[0].o = _mm_broadcastd_epi32(zmm7[0])
        zmm13 = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm7[0].o)
        arg3[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm9, zmm5[0].o, 0x40)
        zmm7[0].o = _mm_permute_ps(zmm0[0].o, 0xe0)
        zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, arg4[0].o, 0x20)
        zmm7[0].o = _mm_fmadd_ps(zmm7[0].o, arg3[0].o, zmm13)
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm12[0].o, 0x10)
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm12[0].o, 0x20)
        zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm7[0].o)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x10)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x20)
        arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm14, zmm6[0].o, 0x10)
        arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm10, 0x20)
        arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
        arg3[0].o = _mm_fmadd_ps(arg3[0].o, zmm1[0].o, zmm0[0].o)
        zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm9, zmm5[0].o, 0)
        zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5[0].o, 0xc8)
        zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, arg3[0].o, zmm4[0].o)
        zmm1[0].o = __vmovddup_xmmdq_xmmq(zmm11[0].q)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
        zmm14 = __vmovshdup_xmmdq_xmmdq(zmm8)
        zmm13 = _mm_permute_pd(zmm8, 1)
        zmm15 = __vmovshdup_xmmdq_xmmdq(zmm0[0].o)
        zmm10 = _mm_permute_pd(zmm0[0].o, 1)
    
    float var_188_1 = zmm0[0]
    zmm1[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x8c))
    zmm4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0x94), 0x20)
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm4[0].o, *(arg1 + 0xd4), 0x20)
    zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
    zmm9 = _mm_broadcast_ss(-0f)
    zmm11[0].o = _mm_permute_pd(zmm1[0].o, 1)
    arg4[0].o = __vxorpd_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm9)
    zmm5[0].o = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
    zmm12[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm9)
    zmm6[0].o = _mm_permute_ps(arg4[0].o, 0x80)
    zmm0[0].o = _mm_permute_ps(var_148, 0xc1)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm6[0].o)
    arg3[0].o = _mm_permute_ps(zmm1[0].o, 0xd5)
    zmm7[0].o = var_128
    zmm4[0].o = _mm_permute_ps(zmm7[0].o, 0xc1)
    zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, arg3[0].o, zmm0[0].o)
    zmm0[0].o = __vmovshdup_xmmdq_xmmdq(zmm7[0].o)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, var_148, 0x68)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm6[0].o)
    zmm6[0].o = _mm_permute_ps(zmm7[0].o, 0xda)
    zmm6[0].o = _mm_fmadd_ps(zmm6[0].o, arg3[0].o, zmm0[0].o)
    float var_f8[0x4] = zmm9
    zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm9)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm1[0].o, 0x9c)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm12[0].o, 0x20)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm0[0].o, 0x10)
    arg4[0].o = arg4[0].q | zmm1[0].q << 0x40
    arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm6[0].o)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm4[0].o, arg3[0].o)
    arg3[0].o = _mm_permute_ps(zmm1[0].o, 0x4a)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm12[0].o, 0x20)
    zmm4[0].o = __vpermilps_xmmdq_memdq_immb(var_138, 0xc0)
    zmm5[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0x40)
    zmm6[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(var_148, zmm7[0].o, 0x30)
    zmm6[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm6[0].o, 0x80)
    zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm5[0].o)
    zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, arg3[0].o, zmm6[0].o)
    zmm6[0].o = _mm_permute_ps(zmm7[0].o, 0x46)
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, var_148, 0x68)
    zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm5[0].o)
    zmm6[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, var_148, 0xc)
    zmm6[0].o = _mm_permute_ps(zmm6[0].o, 0x78)
    zmm5[0].o = _mm_fmadd_ps(zmm5[0].o, arg3[0].o, zmm6[0].o)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm11[0].o, zmm12[0].o, 0x10)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm12[0].o, 0x20)
    zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0)
    zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0xc8)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm5[0].o)
    zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm4[0].o, arg3[0].o)
    zmm11[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm11[0].o)
    zmm1[0].o = __vblendps_xmmdq_xmmdq_memdq_immb(zmm11[0].o, var_198_1, 1)
    arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
    zmm1[0].o = __vmovddup_xmmdq_xmmq(zmm1[0].q)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
    zmm1[0].o = zmm8
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm14, 0x10)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm14, var_188_1, 0x10)
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm13, 0x20)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm15, 0x20)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm13, zmm15, 0x10)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm10, 0x20)
    zmm5[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm0[0].o, 5)
    zmm5[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm0[0].o, 0xd8)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm5[0].o)
    zmm5[0].o = _mm_permute_pd(arg3[0].o, 1)
    zmm0[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5[0].o, 1)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm0[0].o)
    arg4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg3[0].o)
    int32_t temp0_309 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 0)
    int32_t temp0_310 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 1)
    int32_t temp0_311 = __vextractps_gpr32_xmmdq_immb(zmm0[0].o, 2)
    arg3 = _mm256_blend_ps(_mm256_broadcast_ss(1.17549435e-38f), arg4, 7)
    zmm0 = _mm256_broadcast_ss(nanf)
    zmm4 = _mm256_and_ps(arg3, zmm0)
    arg3 = _mm256_broadcast_ss(9.99999994e-09f)
    zmm4 = _mm256_cmp_ps(zmm4, arg3, 2)
    
    if (_mm256_movemask_ps(zmm4) != 0xff)
    label_14000f17f:
        zmm0[0].o = __vmovshdup_xmmdq_xmmdq(arg4[0].o)
        arg3[0].o = _mm_permute_pd(arg4[0].o, 1)
        zmm8 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg4[0].o, 0x8a)
        zmm7[0].o = zx.o(temp0_309)
        zmm5[0].o = zx.o(temp0_310)
        zmm14 = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm5[0].o, 0x1c)
        zmm9 = zx.o(temp0_311)
        zmm4[0].o = data_142d3f660
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, arg3[0].o, 0x10)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm7[0].o, 0x20)
        arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm9, 0x30)
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm1[0].o, 0x46)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5[0].o, 0x2a)
        zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm1[0].o, 0x9c)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm6[0].o)
        zmm0[0].o = _mm_fmsub_ps(zmm0[0].o, arg3[0].o, zmm4[0].o)
        zmm15 = _mm_broadcastss_ps(zmm0[0].o)
        zmm10 = _mm_permute_ps(zmm0[0].o, 0x55)
        zmm13 = _mm_permute_ps(zmm0[0].o, 0xaa)
        zmm12[0].o = _mm_permute_ps(zmm0[0].o, 0xff)
        zmm4[0].o = 0x3f800000
        arg3[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(0x3f800000, zmm9, 0)
        zmm5[0].o = __vunpcklps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm5[0].o)
        zmm5[0].o = zmm11[0].q | zmm5[0] << 0x40
        zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm11[0].o)
        arg3[0].o = _mm_fmsub_ps(arg3[0].o, zmm14, zmm5[0].o)
        zmm6[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg4[0].o, 5)
        zmm5[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm1[0].o, 5)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm1[0].o, 0x8a)
        zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
        zmm5[0].o = _mm_fmsub_ps(zmm5[0].o, zmm8, zmm6[0].o)
        zmm0[0].o = _mm_permute_ps(arg3[0].o, 0xd8)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
        zmm6[0].o = _mm_permute_pd(zmm0[0].o, 1)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm6[0].o)
        zmm6[0].o = _mm_permute_ps(zmm0[0].o, 0x39)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm6[0].o)
        zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm10, zmm13)
        zmm6[0].o = _mm_fmadd_ps(zmm6[0].o, zmm15, zmm12[0].o)
        zmm11[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm0[0].o)
        zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        zmm11[0].o f- zmm0[0]
        
        if (zmm11[0].o f!= zmm0[0] || not(is_ordered.d(zmm11[0].o, zmm0[0])))
            zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm9, 1f, 0x36)
            zmm4[0].o = data_142d4cc30
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm9, 0x10)
            zmm6[0].o = _mm_permute_ps(zmm5[0].o, 0x66)
            zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6[0].o)
            zmm6[0].o = _mm_permute_ps(zmm5[0].o, 0x33)
            zmm4[0].o = _mm_fmsub_ps(zmm4[0].o, zmm0[0].o, zmm6[0].o)
            zmm10 = _mm_fmsub_ps(zmm10, zmm14, zmm4[0].o)
            zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm7[0].o, 0x1d)
            zmm6[0].o = __vmovddup_xmmdq_xmmq(zmm5[0])
            zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6[0].o)
            zmm5[0].o = _mm_permute_pd(zmm5[0].o, 3)
            zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm14, zmm5[0].o)
            zmm5[0].o = arg4[0].q | zmm1[0].q << 0x40
            zmm15 = _mm_fmsub_ps(zmm15, zmm0[0].o, zmm4[0].o)
            zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm1[0].o, 0x20)
            zmm4[0].o = __vmovddup_xmmdq_xmmq(arg3[0].q)
            zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm4[0].o)
            zmm4[0].o = _mm_broadcastss_ps(arg3[0].o)
            zmm0[0].o = _mm_fmsub_ps(zmm0[0].o, zmm5[0].o, zmm4[0].o)
            zmm13 = _mm_fmsub_ps(zmm13, zmm8, zmm0[0].o)
            zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x9d)
            zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0xb0)
            zmm1[0].o = _mm_permute_ps(arg3[0].o, 0x66)
            zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
            zmm1[0].o = _mm_permute_ps(arg3[0].o, 0xcc)
            zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm8, zmm1[0].o)
            zmm12[0].o = _mm_fmsub_ps(zmm12[0].o, zmm5[0].o, zmm0[0].o)
            zmm0[0].o = data_142fc92f0
            zmm0[0].o = __vdivps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm11[0].o)
            zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm12[0].o, zmm0[0].o)
            arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm10, zmm0[0].o)
            arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm13, zmm0[0].o)
            zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm15, zmm0[0].o)
            zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm0[0].o, 0x77)
            zmm4[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg3[0].o, 0x77)
            zmm1[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg3[0].o, 0x22)
        else
            zmm1[0].o = data_142d4cc20
            zmm0[0].o = data_142d4cc30
        
        zmm15 = var_178_1
        zmm7[0].o = var_1a8
        zmm13 = var_f8
        zmm8 = __vmovshdup_xmmdq_xmmdq(zmm4[0].o)
        zmm9 = _mm_permute_pd(zmm4[0].o, 1)
        zmm11[0].o = _mm_permute_ps(zmm4[0].o, 0xe7)
        zmm12[0].o = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
        zmm10 = _mm_permute_pd(zmm1[0].o, 1)
        arg3[0].o = _mm_permute_ps(zmm1[0].o, 0xe7)
        zmm5[0].o = __vmovshdup_xmmdq_xmmdq(zmm0[0].o)
        zmm6[0].o = _mm_permute_pd(zmm0[0].o, 1)
        arg4[0].o = _mm_permute_ps(zmm0[0].o, 0xe7)
    else
        zmm4[0].o = 0x800000
        zmm5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, 0x800000, 0x30)
        zmm4[0].o = _mm_broadcastss_ps(0x800000)
        zmm4 = _mm256_cmp_ps(_mm256_and_ps(_mm256_insertf128_ps(zmm5, zmm4[0].o, 1), zmm0), arg3, 2)
        
        if (_mm256_movemask_ps(zmm4) != 0xff)
            goto label_14000f17f
        
        zmm4[0].o = zx.o(temp0_309)
        zmm5[0].o = zx.o(temp0_310)
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm5[0].o, 0x10)
        zmm5[0].o = zx.o(temp0_311)
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm5[0].o, 0x20)
        zmm5[0].o = 0x800000
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, 0x800000, 0x30)
        zmm5[0].o = _mm_broadcastss_ps(0x800000)
        zmm4 = _mm256_insertf128_ps(zmm4, zmm5[0].o, 1)
        zmm0 = _mm256_cmp_ps(_mm256_and_ps(zmm4, zmm0), arg3, 2)
        
        if (_mm256_movemask_ps(zmm0) != 0xff)
            goto label_14000f17f
        
        zmm6[0].o = 0x3f800000
        arg4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
        zmm5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm5[0].o)
        zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        arg3[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg3[0].o)
        zmm10 = __vxorps_xmmdq_xmmdq_xmmdq(zmm10, zmm10)
        zmm12[0].o = 0x3f800000
        zmm1[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm1[0].o)
        zmm11[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm11[0].o)
        zmm9 = __vxorpd_xmmdq_xmmdq_xmmdq(zmm9, zmm9)
        zmm8 = __vxorps_xmmdq_xmmdq_xmmdq(var_148, var_148)
        zmm4[0].o = 0x3f800000
        zmm15 = var_178_1
        zmm7[0].o = var_1a8
        zmm13 = var_f8
    
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5[0].o, 0x10)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm6[0].o, 0x20)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x30)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm12[0].o, 0x10)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm10, 0x20)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg3[0].o, 0x30)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm8, 0x10)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm9, 0x20)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm11[0].o, 0x30)
    arg4[0].o = _mm_broadcast_ss(*arg2)
    zmm4[0].o = _mm_broadcast_ss(*(arg2 + 4))
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, arg3[0].o, zmm4[0].o)
    zmm5[0].o = _mm_broadcast_ss(arg2[1].d)
    zmm5[0].o = _mm_fmadd_ps(zmm5[0].o, zmm0[0].o, arg4[0].o)
    arg4[0].o = _mm_broadcastss_ps(zmm7[0].o)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg3[0].o)
    arg4[0].o = _mm_permute_ps(zmm7[0].o, 0x55)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm1[0].o, arg3[0].o)
    zmm12[0].o = _mm_permute_ps(zmm7[0].o, 0xaa)
    zmm12[0].o = _mm_fmadd_ps(zmm12[0].o, zmm0[0].o, arg4[0].o)
    zmm0[0].o = __vmovsd_xmmdq_memq(arg1[8].q)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg1 + 0x88), 0x20)
    zmm13 = __vxorps_xmmdq_xmmdq_memdq(zmm13, var_198_1)
    zmm0[0].o = __vmovsd_xmmdq_memq(arg1[0xc].q)
    zmm11[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg1 + 0xc8), 0x20)
    zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm11[0].o)
    arg4[0].o = _mm_permute_ps(zmm1[0].o, 0xc9)
    arg3[0].o = _mm_permute_ps(zmm5[0].o, 0xd2)
    zmm4[0].o = _mm_permute_ps(zmm1[0].o, 0xd2)
    zmm1[0].o = _mm_permute_ps(zmm5[0].o, 0xc9)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
    arg3[0].o = _mm_fmsub_ps(arg3[0].o, arg4[0].o, zmm1[0].o)
    zmm1[0].o = zx.o(temp0_180)
    zmm8 = __vpblendd_xmmdq_xmmdq_memdq_immb(zmm1[0].o, var_158_1, 0xe)
    zmm1[0].o = zx.o(temp0_181)
    zmm7[0].o = zx.o(temp0_185)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm7[0].o, 0x10)
    zmm7[0].o = __vblendps_xmmdq_xmmdq_memdq_immb(zmm1[0].o, var_168_1, 0xc)
    zmm1[0].o = zx.o(temp0_182)
    zmm0[0].o = zx.o(temp0_183)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm0[0].o, 0x10)
    zmm1[0].o = zx.o(temp0_184)
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0x20)
    float var_1a8_1[0x4] = zmm6[0].o
    zmm0[0].o = _mm_permute_ps(zmm12[0].o, 0xc9)
    zmm10 = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm4[0].o)
    zmm0[0].o = _mm_permute_ps(zmm12[0].o, 0xd2)
    zmm10 = _mm_fmsub_ps(zmm10, arg4[0].o, zmm0[0].o)
    zmm0[0].o = _mm_broadcastss_ps(arg3[0].o)
    zmm1[0].o = _mm_permute_ps(arg3[0].o, 0xd5)
    arg4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm1[0].o)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm8, zmm0[0].o)
    zmm0[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x8c))
    zmm1[0].o = _mm_permute_ps(arg3[0].o, 0xea)
    zmm1[0].o = _mm_fmadd_ps(zmm1[0].o, zmm6[0].o, arg4[0].o)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg1 + 0x94), 0x20)
    arg3[0].o = _mm_permute_ps(zmm10, 0xd5)
    zmm14 = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, arg3[0].o)
    arg3[0].o = _mm_broadcastss_ps(zmm10)
    zmm14 = _mm_fmadd_ps(zmm14, zmm8, arg3[0].o)
    arg3[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
    arg3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg3[0].o, *(arg1 + 0xd4), 0x20)
    zmm0[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    arg4[0].o = _mm_permute_ps(zmm0[0].o, 0xc9)
    float var_188_3[0x4] = arg4[0].o
    zmm4[0].o = _mm_permute_ps(zmm0[0].o, 0xd2)
    float var_158_2[0x4] = zmm4[0].o
    zmm0[0].o = _mm_broadcast_ss(-0f)
    int32_t var_168_2[0x4] = zmm0[0].o
    zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
    zmm7[0].o = _mm_permute_ps(zmm0[0].o, 0xd2)
    zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xc9)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm4[0].o)
    zmm7[0].o = _mm_fmsub_ps(zmm7[0].o, arg4[0].o, zmm0[0].o)
    arg4[0].o = _mm_broadcastss_ps(zmm15)
    float var_178_2[0x4] = arg4[0].o
    zmm0[0].o = arg5
    zmm8 = _mm_broadcastss_ps(zmm0[0].o)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm5[0].o)
    zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm8, zmm11[0].o)
    zmm6[0].o = _mm_permute_ps(zmm7[0].o, 0xd5)
    zmm6[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm6[0].o, var_148)
    arg1[0xc].d = zmm0[0]
    zmm4[0].o = _mm_broadcastss_ps(zmm7[0].o)
    zmm6[0].o = __vfmadd231ps_xmmdq_xmmdq_memdq(zmm6[0].o, zmm4[0].o, var_138)
    *(arg1 + 0xc4) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 1)
    *(arg1 + 0xc8) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 2)
    zmm15 = _mm_broadcastss_ps(zmm13)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm15, zmm5[0].o)
    zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm8, arg3[0].o)
    *(arg1 + 0xcc) = zmm0[0]
    arg3[0].o = _mm_permute_ps(zmm7[0].o, 0xea)
    arg1[0xd].d = __vextractps_memd_xmmdq_immb(zmm0[0].o, 1)
    arg3[0].o = __vfmadd132ps_xmmdq_xmmdq_memdq(arg3[0].o, zmm6[0].o, var_128)
    *(arg1 + 0xd4) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 2)
    zmm0[0].o = arg1[0xe]
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm8, zmm1[0].o)
    zmm4[0].o = _mm_broadcastss_ps(zmm1[0].o)
    zmm5[0].o = _mm_permute_ps(zmm0[0].o, 0x1b)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5[0].o)
    zmm5[0].o = _mm_permute_ps(zmm1[0].o, 0x55)
    zmm6[0].o = _mm_permute_pd(zmm0[0].o, 1)
    zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm6[0].o)
    arg4[0].o = __vpbroadcastq_xmmdq_memq(-0x407fffffc0800000)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, arg4[0].o)
    float var_198_2[0x4] = arg4[0].o
    zmm13 = __vxorps_xmmdq_xmmdq_xmmdq(zmm13, zmm13)
    zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm0[0].o, zmm13)
    zmm5[0].o = data_142d3f7d0
    zmm6[0].o = _mm_fmadd_ps(zmm6[0].o, zmm5[0].o, zmm4[0].o)
    zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0xaa)
    zmm4[0].o = _mm_permute_ps(zmm0[0].o, 0xb1)
    zmm11[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
    zmm9 = data_142d3f7b0
    zmm11[0].o = _mm_fmadd_ps(zmm11[0].o, zmm9, zmm6[0].o)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm8, arg3[0].o)
    zmm7[0].o = arg1[0xf]
    arg3[0].o = _mm_broadcastss_ps(zmm1[0].o)
    zmm4[0].o = _mm_permute_ps(zmm7[0].o, 0x1b)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm4[0].o)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
    arg3[0].o = _mm_fmadd_ps(arg3[0].o, zmm7[0].o, zmm13)
    zmm4[0].o = _mm_permute_ps(zmm1[0].o, 0x55)
    zmm6[0].o = _mm_permute_pd(zmm7[0].o, 1)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6[0].o)
    zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm5[0].o, arg3[0].o)
    zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0xaa)
    arg3[0].o = _mm_permute_ps(zmm7[0].o, 0xb1)
    zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
    zmm5[0].o = _mm_fmadd_ps(zmm5[0].o, zmm9, zmm4[0].o)
    arg3[0].o = _mm_broadcastss_ps(0x3f000000)
    zmm11[0].o = _mm_fmadd_ps(zmm11[0].o, arg3[0].o, zmm0[0].o)
    zmm5[0].o = _mm_fmadd_ps(zmm5[0].o, arg3[0].o, zmm7[0].o)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm11[0].o)
    arg3[0].o = _mm_permute_pd(zmm0[0].o, 1)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    arg3[0].o = __vmovshdup_xmmdq_xmmdq(zmm0[0].o)
    arg4[0].o = zmm0[0].o f+ arg3[0]
    arg3[0].o = arg4[0].o f* 0.5f
    zmm4[0].o = __vrsqrtss_xmmdq_xmmdq_xmmd(arg4[0].o, arg4[0])
    zmm6[0].o = zmm4[0].o f* zmm4[0]
    zmm6[0].o = _mm_fnmadd_ss(zmm6[0].o, arg3[0], 0.5f)
    zmm6[0].o = _mm_fmadd_ss(zmm6[0].o, zmm4[0], zmm4[0])
    zmm4[0].o = zmm6[0].o f* zmm6[0].d
    zmm4[0].o = _mm_fnmadd_ss(zmm4[0].o, arg3[0], 0.5f)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm5[0].o)
    zmm7[0].o = _mm_permute_pd(arg3[0].o, 1)
    arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm7[0].o)
    zmm7[0].o = __vmovshdup_xmmdq_xmmdq(arg3[0].o)
    arg3[0].o = arg3[0].o f+ zmm7[0]
    zmm7[0].o = arg3[0].o f* 0.5f
    zmm1[0].o = __vrsqrtss_xmmdq_xmmdq_xmmd(arg3[0].o, arg3[0])
    zmm0[0].o = zmm1[0].o f* zmm1[0]
    zmm0[0].o = _mm_fnmadd_ss(zmm0[0].o, zmm7[0], 0.5f)
    zmm0[0].o = _mm_fmadd_ss(zmm0[0].o, zmm1[0], zmm1[0])
    zmm1[0].o = zmm0[0].o f* zmm0[0]
    zmm1[0].o = _mm_fnmadd_ss(zmm1[0].o, zmm7[0], 0.5f)
    zmm4[0].o = _mm_fmadd_ss(zmm4[0].o, zmm6[0].d, zmm6[0].d)
    zmm4[0].o = _mm_broadcastss_ps(zmm4[0].o)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm11[0].o, zmm4[0].o)
    zmm6[0].o = 0x322bcc77
    arg4[0].o = _mm_cmp_ss(0x322bcc77, arg4[0], 6)
    zmm7[0].o = 0xffffffff
    arg4[0].o = __vandnps_xmmdq_xmmdq_xmmdq(arg4[0].o, 0xffffffff)
    arg4 = _mm256_broadcastss_ps(arg4[0].o)
    zmm11 = __vandnps_ymmqq_ymmqq_memqq(arg4, data_142fc9280)
    arg4 = _mm256_and_ps(zmm4, arg4)
    zmm13 = _mm256_or_ps(arg4, zmm11).o
    arg3[0].o = _mm_cmp_ss(0x322bcc77, arg3[0], 6)
    arg3[0].o = __vandnps_xmmdq_xmmdq_xmmdq(arg3[0].o, 0xffffffff)
    zmm1[0].o = _mm_fmadd_ss(zmm1[0].o, zmm0[0], zmm0[0])
    zmm0[0].o = _mm_broadcastss_ps(zmm1[0].o)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
    zmm1[0].o = _mm_broadcastss_ps(arg3[0].o)
    zmm0[0].o = __vandps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
    zmm1[0].o = __vandnps_xmmdq_xmmdq_memdq(zmm1[0].o, data_142d3f660)
    zmm0[0].o = __vorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm13, zmm0[0].o)
    arg3[0].o = _mm_permute_pd(zmm1[0].o, 1)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
    arg3[0].o = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
    zmm1[0].o = zmm1[0].o f+ arg3[0]
    arg3[0].o = _mm_permute_ps(zmm10, 0xea)
    arg4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
    zmm1[0].o = _mm_cmp_ss(arg4[0].o, zmm1[0], 2)
    arg4[0].o = 0x3f800000
    zmm1[0].o = __vblendvps_xmmdq_xmmdq_xmmdq_xmmdq(0xbf800000, 0x3f800000, zmm1[0].o)
    arg3[0].o = __vfmadd132ps_xmmdq_xmmdq_memdq(arg3[0].o, zmm14, var_1a8_1)
    zmm1[0].o = _mm_broadcastss_ps(zmm1[0].o)
    arg1[0xe] = zmm13
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
    arg1[0xf] = zmm0[0].o
    zmm0[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm12[0].o, var_178_2)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm15, zmm12[0].o)
    arg4[0].o = __vmovsd_xmmdq_memq(arg1[0x10].q)
    arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg4[0].o, *(arg1 + 0x108), 0x20)
    zmm5[0].o = __vxorps_xmmdq_xmmdq_memdq(zmm12[0].o, var_168_2)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm8, zmm0[0].o)
    arg1[0x10].d = arg4[0]
    *(arg1 + 0x104) = __vextractps_memd_xmmdq_immb(arg4[0].o, 1)
    zmm0[0].o = _mm_permute_ps(zmm5[0].o, 0xc9)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm0[0].o, var_158_2)
    *(arg1 + 0x108) = __vextractps_memd_xmmdq_immb(arg4[0].o, 2)
    arg4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x10c))
    arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg4[0].o, *(arg1 + 0x114), 0x20)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm8, zmm1[0].o)
    int64_t rax_7 = __vpextrq_gpr64q_xmmdq_immb(zmm13, 1)
    *(arg1 + 0x10c) = arg4[0]
    zmm1[0].o = _mm_permute_ps(zmm5[0].o, 0xd2)
    arg1[0x11].d = __vextractps_memd_xmmdq_immb(arg4[0].o, 1)
    zmm0[0].o = __vfmsub231ps_xmmdq_xmmdq_memdq(zmm0[0].o, zmm1[0].o, var_188_3)
    *(arg1 + 0x114) = __vextractps_memd_xmmdq_immb(arg4[0].o, 2)
    zmm1[0].o = _mm_broadcastss_ps(zmm0[0].o)
    arg4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x118))
    arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg4[0].o, arg1[0x12].d, 0x20)
    zmm5[0].o = _mm_permute_ps(zmm0[0].o, 0xd5)
    arg4[0].o = _mm_fmadd_ps(arg4[0].o, zmm8, arg3[0].o)
    *(arg1 + 0x118) = arg4[0]
    arg3[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm5[0].o, var_148)
    *(arg1 + 0x11c) = __vextractps_memd_xmmdq_immb(arg4[0].o, 1)
    arg3[0].o = __vfmadd231ps_xmmdq_xmmdq_memdq(arg3[0].o, zmm1[0].o, var_138)
    arg1[0x12].d = __vextractps_memd_xmmdq_immb(arg4[0].o, 2)
    zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xea)
    zmm1[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x124))
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0x12c), 0x20)
    zmm0[0].o = __vfmadd132ps_xmmdq_xmmdq_memdq(zmm0[0].o, arg3[0].o, var_128)
    zmm1[0].o = _mm_fmadd_ps(zmm1[0].o, zmm8, zmm0[0].o)
    *(arg1 + 0x124) = zmm1[0]
    *(arg1 + 0x128) = __vextractps_memd_xmmdq_immb(zmm1[0].o, 1)
    *(arg1 + 0x12c) = __vextractps_memd_xmmdq_immb(zmm1[0].o, 2)
    int64_t rdx = zmm13[0].q
    zmm0[0].o = zx.o(rdx.d)
    zmm1[0].o = zx.o(rax_7.d)
    result = (rax_7 u>> 0x20).d
    arg3[0].o = zx.o((rdx u>> 0x20).d)
    arg4[0].o = *(arg1 + 0x18)
    zmm5[0].o = arg1[1].d
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm1[0].o, 0x10)
    zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg4[0].o, zmm5[0].o, 0x10)
    zmm7[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm7[0].o, *(arg1 + 0x14), 0x20)
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm0[0].o, 0x20)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm0[0].o, 0x10)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg3[0].o, 0x20)
    zmm1[0].o = __vmovsd_xmmdq_memq(arg1[1].q)
    arg3[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x14))
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm5[0].o, 0x20)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    arg3[0].o = _mm_fmsub_ps(arg3[0].o, zmm6[0].o, zmm7[0].o)
    arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg3[0].o)
    zmm5[0].o = zx.o(result)
    zmm5[0].o = _mm_broadcastd_epi32(zmm5[0].d)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, arg4[0].o, 0x20)
    zmm5[0].o = _mm_fmadd_ps(zmm5[0].o, arg3[0].o, zmm1[0].o)
    zmm1[0].o = _mm_permute_ps(arg3[0].o, 0xd2)
    arg3[0].o = _mm_permute_ps(arg3[0].o, 0xc9)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    zmm0[0].o = _mm_fmsub_ps(zmm0[0].o, zmm6[0].o, zmm1[0].o)
    zmm1[0].o = __vmovsd_xmmdq_memq(arg1[0xc].q)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg1 + 0xc8), 0x20)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
    zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
    arg1[8].d = zmm0[0]
    *(arg1 + 0x84) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 1)
    *(arg1 + 0x88) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 2)
    zmm0[0].o = arg1[0xf].d
    zmm1[0].o = *(arg1 + 0xf8)
    arg3[0].o = *(arg1 + 0x48)
    arg4[0].o = arg1[4].d
    zmm5[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xf4))
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, arg4[0].o, 0x10)
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm6[0].o, *(arg1 + 0x44), 0x20)
    zmm5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm0[0].o, 0x20)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm0[0].o, 0x10)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg1 + 0xf4), 0x20)
    zmm7[0].o = __vmovsd_xmmdq_memq(arg1[4].q)
    zmm0[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x44))
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x20)
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
    zmm0[0].o = _mm_fmsub_ps(zmm0[0].o, zmm5[0].o, zmm6[0].o)
    arg4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
    zmm8 = _mm_broadcast_ss(*(arg1 + 0xfc))
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, arg3[0].o, 0x20)
    arg3[0].o = _mm_fmadd_ps(arg3[0].o, zmm8, arg4[0].o)
    zmm6[0].o = _mm_permute_ps(arg4[0].o, 0xd2)
    arg4[0].o = _mm_permute_ps(arg4[0].o, 0xc9)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg4[0].o)
    zmm1[0].o = _mm_fmsub_ps(zmm1[0].o, zmm5[0].o, zmm6[0].o)
    arg4[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0xcc))
    arg4[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(arg4[0].o, *(arg1 + 0xd4), 0x20)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, zmm1[0].o)
    zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg4[0].o, zmm1[0].o)
    *(arg1 + 0x8c) = zmm1[0]
    arg1[9].d = __vextractps_memd_xmmdq_immb(zmm1[0].o, 1)
    *(arg1 + 0x94) = __vextractps_memd_xmmdq_immb(zmm1[0].o, 2)
    zmm5[0].o = _mm_broadcast_ss(*(arg1 + 0xec))
    zmm1[0].o = *arg1
    zmm7[0].o = _mm_broadcast_ss(arg1[0xe].d)
    zmm10 = *(arg1 + 0xc)
    zmm12[0].o = *(arg1 + 8)
    zmm13 = *(arg1 + 4)
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm10, zmm12[0].o, 0x10)
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm13, 0x20)
    zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm1[0].o, 0x30)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm4[0].o)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, var_198_2)
    zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm1[0].o, zmm5[0].o)
    zmm5[0].o = _mm_broadcast_ss(*(arg1 + 0xe4))
    zmm7[0].o = __vmovsd_xmmdq_memq(*(arg1 + 8))
    zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm1[0].o, 0x20)
    zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm13, 0x30)
    zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm5[0].o)
    zmm9 = data_142d3f7d0
    zmm5[0].o = _mm_fmadd_ps(zmm5[0].o, zmm9, zmm4[0].o)
    zmm14 = *(arg1 + 0x3c)
    zmm7[0].o = *(arg1 + 0x38)
    zmm0[0].o = *(arg1 + 0x34)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm14, zmm7[0].o, 0x10)
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, zmm0[0].o, 0x20)
    arg4[0].o = arg1[3]
    arg3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3[0].o, arg4[0].o, 0x30)
    zmm6[0].o = _mm_broadcast_ss(arg1[0xf].d)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, arg3[0].o)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg3[0].o, var_198_2)
    arg3[0].o = _mm_fmadd_ps(arg3[0].o, arg4[0].o, zmm8)
    zmm6[0].o = __vmovsd_xmmdq_memq(*(arg1 + 0x38))
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, arg4[0].o, 0x20)
    zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm0[0].o, 0x30)
    zmm4[0].o = _mm_broadcast_ss(*(arg1 + 0xf4))
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm4[0].o)
    zmm4[0].o = _mm_fmadd_ps(zmm4[0].o, zmm9, arg3[0].o)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm13, zmm1[0].o, 0x10)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm10, 0x20)
    zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm12[0].o, 0x30)
    arg3[0].o = _mm_broadcast_ss(*(arg1 + 0xe8))
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3[0].o)
    zmm6[0].o = data_142d3f7b0
    zmm1[0].o = _mm_fmadd_ps(zmm1[0].o, zmm6[0].o, zmm5[0].o)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg4[0].o, 0x10)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm14, 0x20)
    zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm7[0].o, 0x30)
    arg3[0].o = _mm_broadcast_ss(*(arg1 + 0xf8))
    zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
    zmm0[0].o = _mm_fmadd_ps(zmm0[0].o, zmm6[0].o, zmm4[0].o)
    arg3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
    arg4[0].o = _mm_permute_pd(arg3[0].o, 1)
    arg3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(arg3[0].o, arg4[0].o)
    arg4[0].o = __vmovshdup_xmmdq_xmmdq(arg3[0].o)
    arg3[0].o = arg3[0].o f+ arg4[0]
    arg4[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg4[0].o, arg4[0].o)
    arg3[0].o = _mm_cmp_ss(arg4[0].o, arg3[0], 2)
    arg3[0].o = __vblendvps_xmmdq_xmmdq_xmmdq_xmmdq(0xbf800000, 0x3f800000, arg3[0].o)

arg3[0].o = _mm_broadcastss_ps(arg3[0].o)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg3[0].o)
arg1[0xa] = zmm1[0].o
arg1[0xb] = zmm0[0].o
_mm256_zeroupper()
return result
