// 函数: sub_14032bb40
// 地址: 0x14032bb40
// 来自: E:\Embers\Embers\Binaries\Win64\Embers-Win64-Shipping.exe

int128_t var_88 = arg5[0].o
float zmm9[0x8] = *arg10
int32_t zmm0[0x8]
zmm0[0].o = *arg4
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, arg4[1], 0x10)
zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, arg4[2], 0x20)
float zmm1[0x8]
zmm1[0].o = data_142d3f730
float zmm3[0x8]
zmm3[0].o = __vdivps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
float zmm12[0x8]
zmm12[0].o = data_142d3f770
int32_t zmm4[0x8]
zmm4[0].o = __vandps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm12[0].o)
int32_t zmm5[0x8]
zmm5[0].o = data_142d3f740
zmm4[0].o = _mm_cmp_ps(zmm5[0].o, zmm4[0].o, 2)
zmm4[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
zmm4[0].o = __vpmovzxwd_xmmdq_xmmq(zmm4[0].q)
zmm4[0].o = __vpslld_xmmdq_xmmdq_immb(zmm4[0].o, 0x1f)
zmm4[0].o = __vpsrad_xmmdq_xmmdq_immb(zmm4[0].o, 0x1f)
zmm5 = _mm256_cvtepi32_ps(_mm256_cvttps_epi32(zmm3))
zmm3[0].o = __vpblendvb_xmmdq_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm3[0].o, zmm4[0].o)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm1[0].o)
zmm0[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm0[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm0[0].o, data_142d3f5f0)
zmm1[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm0[0].o, data_142d3f620)
zmm1 = _mm256_cvtepi32_ps(_mm256_cvttps_epi32(zmm1))
zmm1[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm1[0].o, data_142d3f6e0)
float zmm13[0x8]
zmm13[0].o = arg9
zmm0[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm1[0].o = data_142d3f780
zmm3[0].o = __vandps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm3[0].o = __vorps_xmmdq_xmmdq_memdq(zmm3[0].o, data_142d3f6d0)
zmm4[0].o = __vandps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm12[0].o)
zmm3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm0[0].o)
zmm5[0].o = data_142d3f6a0
zmm4[0].o = _mm_cmp_ps(zmm5[0].o, zmm4[0].o, 1)
zmm4[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
zmm4[0].o = __vpmovzxwd_xmmdq_xmmq(zmm4[0].q)
zmm4[0].o = __vpslld_xmmdq_xmmdq_immb(zmm4[0].o, 0x1f)
zmm4[0].o = __vpsrad_xmmdq_xmmdq_immb(zmm4[0].o, 0x1f)
zmm0[0].o = __vpblendvb_xmmdq_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm3[0].o, zmm4[0].o)
zmm1[0].o &= zmm4[0].o
zmm1[0].o |= data_142d3f670
zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
zmm4[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm3[0].o, data_143443100)
zmm4[0].o = __vaddps_xmmdq_xmmdq_memdq(zmm4[0].o, data_143443110)
float zmm14[0x8]
zmm14[0].o = arg8
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm4[0].o)
zmm4[0].o = __vaddps_xmmdq_xmmdq_memdq(zmm4[0].o, data_143443120)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm4[0].o)
zmm4[0].o = __vaddps_xmmdq_xmmdq_memdq(zmm4[0].o, data_143443130)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm4[0].o)
zmm4[0].o = __vaddps_xmmdq_xmmdq_memdq(zmm4[0].o, data_143443140)
zmm5[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm3[0].o, data_143443150)
zmm5[0].o = __vaddps_xmmdq_xmmdq_memdq(zmm5[0].o, data_143443160)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm4[0].o)
zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm5[0].o)
zmm5[0].o = __vaddps_xmmdq_xmmdq_memdq(zmm5[0].o, data_143443170)
int224_t zmm15
zmm15.o = data_142d3f670
zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm5[0].o)
zmm5[0].o = __vaddps_xmmdq_xmmdq_memdq(zmm5[0].o, data_143443180)
zmm4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm15.o)
zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm5[0].o)
zmm5[0].o = __vaddps_xmmdq_xmmdq_memdq(zmm5[0].o, data_142d3f790)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm4[0].o)
zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm5[0].o)
zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm15.o)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm3[0].o)
zmm3[0].o = _mm_permute_ps(zmm0[0].o, 0xaa)
zmm4[0].o = _mm_permute_ps(zmm1[0].o, 0xaa)
zmm5[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0)
float zmm7[0x8]
zmm7[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0x55)
zmm7[0].o = _mm_permute_ps(zmm7[0].o, 0xd8)
zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm7[0].o)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5[0].o)
zmm5[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm0[0].o, 0)
zmm0[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm0[0].o, 0x55)
zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xd8)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm0[0].o)
zmm1 = __vxorps_ymmqq_ymmqq_memqq(zmm4, data_143443020)
zmm0 = __vxorps_ymmqq_ymmqq_memqq(zmm0, data_143443040)
int32_t temp0_67 = _mm256_movemask_ps(zmm9)
zmm3[0].o = *arg6
zmm3[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm3[0].o, arg6[1], 0x10)
zmm0 = _mm256_add_ps(zmm1, zmm0)
zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm3[0].o, arg6[2], 0x20)
zmm3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(arg3, arg3, 0x1c)
zmm3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, arg3, 0x20)
double zmm11[0x4]
zmm11[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm1[0].o)
zmm4[0].o = arg2[2]
zmm1[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm1[0].o)
zmm1 = _mm256_cmp_ps(zmm4, zmm1, 1)
uint64_t result
float zmm6[0x8]
float var_1c0[0x8]
int32_t var_1b8
int32_t var_1b0
int32_t var_1ac
int32_t var_1a8
int128_t var_150
float var_134
float var_130
float var_12c
float var_128
float var_124
float zmm10[0x4]

if (temp0_67 != 0xff)
    zmm1 = _mm256_and_ps(zmm1, zmm9)
    result = zx.q(_mm256_movemask_ps(zmm1))
    
    if (result.d == 0)
        zmm10 = *(arg2 + 0xc)
        arg5[0].o = _mm_permute_ps(zmm10, 0)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg5[0].o)
        zmm5[0].o = *arg2
        zmm6[0].o = _mm_permute_ps(zmm5[0].o, 0)
        zmm7[0].o = _mm_permute_ps(zmm0[0].o, 0x1b)
        zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm6[0].o)
        zmm6[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm6[0].o, data_142d3f7c0)
        zmm7[0].o = *(arg2 + 4)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm6[0].o)
        zmm6[0].o = _mm_permute_ps(zmm7[0].o, 0)
        zmm3[0].o = _mm_permute_pd(zmm0[0].o, 1)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm6[0].o)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm3[0].o, data_142d3f7d0)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm1[0].o)
        zmm3[0].o = *(arg2 + 8)
        zmm6[0].o = _mm_permute_ps(zmm3[0].o, 0)
        zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xb1)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm6[0].o)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm0[0].o, data_142d3f7b0)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm11[0].o)
        zmm6[0].o = __vmovsd_xmmdq_memq(*(arg2 + 4))
        zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm5[0].o, 0x20)
        zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm10, 0x30)
        zmm9[0].o = _mm_permute_ps(zmm0[0].o, 0xd2)
        zmm3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, zmm5[0].o, 0x10)
        zmm3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, zmm7[0].o, 0x20)
        zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, zmm10, 0x30)
        zmm3[0].o = _mm_permute_ps(zmm0[0].o, 0xc9)
        zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm9[0].o, zmm6[0].o)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm7[0].o)
        zmm3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm3[0].o)
        zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm3[0].o)
        zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg5[0].o, zmm3[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm5[0].o)
        zmm5[0].o = _mm_permute_ps(zmm3[0].o, 0xd2)
        zmm3[0].o = _mm_permute_ps(zmm3[0].o, 0xc9)
        zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm5[0].o)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm3[0].o)
        zmm3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm3[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm3[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_memdq(zmm0[0].o, arg2[1])
        zmm7[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm4[0].o, data_142e6da00)
        zmm5[0].o = arg7
    else
        float var_c0_1[0x4] = arg3
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm5[0].o = _mm_permute_ps(zmm3[0].o, 1)
        zmm3[0].o = _mm_permute_ps(zmm3[0].o, 0x1a)
        zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm3[0].o)
        zmm3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm15.o, zmm3[0].o)
        zmm10 = data_142e6da00
        arg5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm10)
        zmm3[0].o = _mm_permute_ps(zmm0[0].o, 4)
        zmm5[0].o = _mm_permute_ps(zmm1[0].o, 0x29)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm5[0].o)
        zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xff)
        zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0x12)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm1[0].o)
        zmm14[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm10)
        zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm1[0].o)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm10)
        zmm3[0].o = _mm_permute_pd(zmm14[0].o, 1)
        zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        zmm5[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, arg5[0].o, 1)
        arg3 = __vxorps_xmmdq_xmmdq_xmmdq(arg3, arg3)
        zmm12 = data_143443080
        zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm14[0].o, 0x10)
        zmm7[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm1[0].o, 4)
        zmm7 = _mm256_blend_ps(zmm5, zmm7, 0xf)
        zmm5[0].o = _mm256_extractf128_ps(zmm5[0].o, 1)
        zmm5[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm1[0].o, 1)
        zmm5[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, arg5[0].o, 2)
        zmm5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm14[0].o, 0x60)
        zmm5 = _mm256_insertf128_ps(zmm7, zmm5[0].o, 1)
        zmm3 = _mm256_blend_ps(zmm12, zmm3, 1)
        zmm7[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, zmm1[0].o, 2)
        zmm7[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, arg5[0].o, 4)
        zmm3[0].o = _mm256_extractf128_ps(zmm3[0].o, 1)
        zmm3[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm11[0].o, zmm3[0].o, 8)
        zmm3 = _mm256_insertf128_ps(zmm7, zmm3[0].o, 1)
        int32_t var_140_2[0x8] = zmm5
        zmm3[0].o = *arg2
        zmm5[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm3[0].o)
        zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm5[0].o)
        zmm6[0].o = _mm_permute_ps(zmm7[0].o, 0xc1)
        zmm7[0].o = _mm_permute_ps(zmm7[0].o, 0xda)
        zmm6[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm7[0].o)
        zmm6[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm15.o, zmm6[0].o)
        zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6[0].o)
        zmm7[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm3[0].o, *(arg2 + 4), 0x10)
        zmm3[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm3[0].o, 4)
        zmm7[0].o = _mm_permute_ps(zmm5[0].o, 0x29)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm3[0].o)
        zmm7[0].o = _mm_broadcast_ss(*(arg2 + 0xc))
        zmm5[0].o = _mm_permute_ps(zmm5[0].o, 0x12)
        zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm7[0].o)
        zmm7[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm5[0].o)
        zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm7[0].o)
        zmm0[0].o = __vmovsd_xmmdq_memq(*(arg2 + 0x24))
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm4[0].o, 0x20)
        zmm3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm5[0].o)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm0[0].o)
        zmm11[0].o = arg2[1].d
        zmm3[0].o = _mm_permute_pd(zmm7[0].o, 1)
        zmm4[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg3, zmm6[0].o, 1)
        zmm5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm7[0].o, 0x10)
        zmm5[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm0[0].o, 4)
        zmm4[0].o = _mm256_extractf128_ps(zmm4[0].o, 1)
        zmm4[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm0[0].o, 1)
        zmm4[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm6[0].o, 2)
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm7[0].o, 0x60)
        zmm7 = _mm256_blend_ps(zmm12, zmm3, 1)
        zmm0[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm0[0].o, 2)
        zmm3[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm6[0].o, 4)
        zmm0[0].o = _mm256_extractf128_ps(zmm7[0].o, 1)
        zmm0[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm11[0].o, 1)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg2 + 0x14), 0x10)
        zmm7[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg2 + 0x18), 0x20)
        float temp0_328[0x4] = __vmulps_xmmdq_xmmdq_memdq(zmm10, arg2[2])
        zmm0[0].o = _mm_permute_ps(arg5[0].o, 0)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
        zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0xaa)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm1[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm1[0].o = _mm_permute_ps(zmm14[0].o, 0)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm1[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm1[0].o = _mm_broadcast_ss(var_134)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm1[0].o)
        zmm11[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm0[0].o = _mm_broadcast_ss(var_130)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm0[0].o)
        zmm1[0].o = _mm_broadcast_ss(var_128)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm1[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm1[0].o = _mm_broadcast_ss(var_12c)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm1[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm1[0].o = _mm_broadcast_ss(var_124)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm1[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm1[0].o = _mm_broadcast_ss(zmm3[0])
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm1[0].o)
        zmm6[0].o = _mm_broadcast_ss(zmm3[2])
        zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm6[0].o)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm6[0].o)
        zmm6[0].o = _mm_broadcast_ss(zmm3[1])
        zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6[0].o)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm6[0].o)
        zmm6[0].o = _mm_broadcast_ss(zmm3[3])
        zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm6[0].o)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm6[0].o)
        zmm6[0].o = _mm_broadcast_ss(zmm3[4])
        zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm6[0].o)
        zmm6[0].o = _mm_broadcast_ss(zmm3[6])
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm6[0].o)
        zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm3[0].o)
        zmm5[0].o = _mm_broadcast_ss(zmm3[5])
        zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm5[0].o)
        zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm4[0].o)
        zmm4[0].o = _mm_broadcast_ss(zmm3[7])
        zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm4[0].o)
        zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm4[0].o)
        zmm4[0].o = __vmovshdup_xmmdq_xmmdq(zmm11[0].o)
        arg5[0].o = _mm_permute_pd(zmm11[0].o, 1)
        zmm6[0].o = zx.o(__vextractps_gpr32_xmmdq_immb(zmm0[0].o, 0))
        float temp0_376[0x4] = __vmovshdup_xmmdq_xmmdq(zmm0[0].o)
        int32_t temp0_377 = __vextractps_gpr32_xmmdq_immb(zmm1[0].o, 0)
        zmm15.o = _mm_permute_pd(zmm0[0].o, 1)
        zmm7[0].o = zx.o(temp0_377)
        zmm0[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm6[0].o, 1)
        zmm12 = _mm256_insertf128_ps(zmm11, zmm0[0].o, 1)
        zmm0[0].o = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
        zmm5[0].o = _mm_permute_pd(zmm1[0].o, 1)
        zmm1[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm7[0].o, 1)
        zmm14 = _mm256_insertf128_ps(zmm1, zmm3[0].o, 1)
        var_1c0 = zmm12
        zmm1[0].o = zmm11[0].o f* zmm11[0].d
        zmm3[0].o = zmm4[0].o f* zmm4[0]
        zmm1[0].o = zmm1[0].o f+ zmm3[0]
        zmm3[0].o = arg5[0].o f* arg5[0]
        zmm3[0].o = zmm1[0].o f+ zmm3[0]
        zmm1[0].o = zmm6[0].o f* zmm6[0]
        zmm4[0].o = temp0_376 f* temp0_376[0]
        zmm1[0].o = zmm1[0].o f+ zmm4[0]
        zmm4[0].o = zmm15.o f* zmm15.d
        zmm6[0].o = zmm1[0].o f+ zmm4[0]
        zmm1[0].o = zmm7[0].o f* zmm7[0]
        zmm0[0].o = zmm0[0].o f* zmm0[0]
        zmm0[0].o = zmm1[0].o f+ zmm0[0]
        zmm1[0].o = zmm5[0].o f* zmm5[0]
        zmm1[0].o = zmm0[0].o f+ zmm1[0]
        arg5[0].o = 0xb22bcc77
        zmm0[0].o = zmm3[0].o f+ -9.99999994e-09f
        zmm7[0].o = __vrsqrtss_xmmdq_xmmdq_xmmd(zmm3[0].o, zmm3[0])
        zmm13[0].o = 0x3f000000
        zmm3[0].o = zmm3[0].o f* 0.5f
        zmm4[0].o = zmm7[0].o f* zmm7[0]
        zmm4[0].o = zmm3[0].o f* zmm4[0]
        zmm4[0].o = 0x3f000000 f- zmm4[0]
        zmm4[0].o = zmm7[0].o f* zmm4[0]
        zmm4[0].o = zmm7[0].o f+ zmm4[0]
        zmm7[0].o = zmm4[0].o f* zmm4[0]
        zmm3[0].o = zmm3[0].o f* zmm7[0]
        zmm3[0].o = 0x3f000000 f- zmm3[0]
        zmm3[0].o = zmm4[0].o f* zmm3[0]
        zmm3[0].o = zmm4[0].o f+ zmm3[0]
        zmm10 = __vxorps_xmmdq_xmmdq_xmmdq(temp0_376, temp0_376)
        zmm0[0].o = _mm_cmp_ss(zmm10, zmm0[0], 2)
        zmm7[0].o = 0x3f800000
        zmm3[0].o = __vblendvps_xmmdq_xmmdq_xmmdq_xmmdq(0x3f800000, zmm3[0].o, zmm0[0].o)
        zmm0[0].o = zmm6[0].o f+ -9.99999994e-09f
        zmm4[0].o = __vrsqrtss_xmmdq_xmmdq_xmmd(zmm6[0].o, zmm6[0])
        zmm6[0].o = zmm6[0].o f* 0.5f
        zmm5[0].o = zmm4[0].o f* zmm4[0]
        zmm5[0].o = zmm6[0].o f* zmm5[0]
        zmm5[0].o = 0x3f000000 f- zmm5[0]
        zmm5[0].o = zmm4[0].o f* zmm5[0]
        zmm4[0].o = zmm4[0].o f+ zmm5[0]
        zmm5[0].o = zmm4[0].o f* zmm4[0]
        zmm5[0].o = zmm6[0].o f* zmm5[0]
        zmm5[0].o = 0x3f000000 f- zmm5[0]
        zmm5[0].o = zmm4[0].o f* zmm5[0]
        zmm4[0].o = zmm4[0].o f+ zmm5[0]
        zmm0[0].o = _mm_cmp_ss(zmm10, zmm0[0], 2)
        zmm0[0].o = __vblendvps_xmmdq_xmmdq_xmmdq_xmmdq(0x3f800000, zmm4[0].o, zmm0[0].o)
        zmm4[0].o = zmm1[0].o f+ -9.99999994e-09f
        zmm5[0].o = __vrsqrtss_xmmdq_xmmdq_xmmd(zmm1[0].o, zmm1[0])
        zmm1[0].o = zmm1[0].o f* 0.5f
        zmm6[0].o = zmm5[0].o f* zmm5[0]
        zmm6[0].o = zmm1[0].o f* zmm6[0]
        zmm6[0].o = 0x3f000000 f- zmm6[0]
        zmm6[0].o = zmm5[0].o f* zmm6[0]
        zmm5[0].o = zmm5[0].o f+ zmm6[0]
        zmm6[0].o = zmm5[0].o f* zmm5[0]
        zmm1[0].o = zmm1[0].o f* zmm6[0]
        zmm1[0].o = 0x3f000000 f- zmm1[0]
        zmm1[0].o = zmm5[0].o f* zmm1[0]
        zmm1[0].o = zmm5[0].o f+ zmm1[0]
        zmm4[0].o = _mm_cmp_ss(zmm10, zmm4[0], 2)
        zmm1[0].o = __vblendvps_xmmdq_xmmdq_xmmdq_xmmdq(0x3f800000, zmm1[0].o, zmm4[0].o)
        int224_t var_1a0_2 = zmm14[0].28
        zmm4[0].o = zmm3[0].o f* zmm11[0].d
        zmm5[0].o = zmm3[0].o f* var_1c0[1]
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm5[0].o, 0x10)
        zmm3[0].o = zmm3[0].o f* var_1b8
        zmm3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm3[0].o, 0x20)
        zmm4[0].o = zmm0[0].o f* var_1b0
        zmm5[0].o = zmm0[0].o f* var_1ac
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm5[0].o, 0x10)
        zmm0[0].o = zmm0[0].o f* var_1a8
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm0[0].o, 0x20)
        zmm0[0].o = zmm1[0].o f* var_1a0_2.d
        zmm5[0].o = zmm1[0].o f* var_1a0_2:4.d
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5[0].o, 0x10)
        zmm1[0].o = zmm1[0].o f* var_1a0_2:8.d
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0x20)
        zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        zmm0[0].o = _mm_cmp_ps(zmm0[0].o, temp0_328, 2)
        zmm0[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        zmm0[0].o = __vpmovzxwd_xmmdq_xmmq(zmm0[0].q)
        zmm0[0].o = __vpslld_xmmdq_xmmdq_immb(zmm0[0].o, 0x1f)
        zmm5 = data_1434431a0
        zmm6 = __vblendvps_ymmqq_ymmqq_memqq_ymmqq(zmm5, data_1434430a0, zmm0)
        zmm0[0].o = _mm_permute_ps(zmm6[0].o, 0)
        zmm0 = _mm256_insertf128_ps(zmm0, zmm0[0].o, 1)
        zmm11 = _mm256_mul_ps(zmm0, zmm3)
        zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        float temp0_411[0x4] = __vmovshdup_xmmdq_xmmdq(zmm11[0].o)
        zmm3[0].o = _mm_permute_pd(zmm11[0].o, 1)
        var_1c0[0] = zmm11[0].d
        var_1c0[1] = temp0_411[0]
        float var_1b8_2 = zmm3[0]
        zmm5[0].o = _mm_permute_ps(zmm6[0].o, 0x55)
        zmm5 = _mm256_insertf128_ps(zmm5, zmm5[0].o, 1)
        zmm14 = _mm256_mul_ps(zmm5, zmm4)
        zmm5[0].o = __vmovshdup_xmmdq_xmmdq(zmm14[0].o)
        arg5[0].o = _mm_permute_pd(zmm14[0].o, 1)
        float var_1b0_2 = zmm14[0]
        int32_t var_1ac_2 = zmm5[0]
        float var_1a8_2 = arg5[0]
        zmm4[0].o = _mm_permute_ps(zmm6[0].o, 0xaa)
        zmm4 = _mm256_insertf128_ps(zmm4, zmm4[0].o, 1)
        zmm6 = __vcmpps_ymmqq_ymmqq_memqq_immb(
            __vandps_ymmqq_ymmqq_memqq(
                __vblendps_ymmqq_ymmqq_memqq_immb(zmm11, data_142fc9300, 0xf8), data_142fc9320), 
            data_1434431c0, 2)
        zmm1 = _mm256_mul_ps(zmm4, zmm1)
        zmm4[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(zmm9[0].o, zmm0[0].o)
        zmm7[0].o = _mm256_extractf128_ps(zmm9[0].o, 1)
        zmm15.o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm0[0].o)
        zmm4[0].o = __vorps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm4[0].o)
        zmm6[0].o = _mm256_extractf128_ps(zmm6[0].o, 1)
        zmm6[0].o |= zmm15.o
        zmm4[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6[0].o)
        zmm12[0].o = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
        zmm4[0].o = __vpacksswb_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
        result = zx.q(__vpmovmskb_gpr32d_xmmdq(zmm4[0].o))
        zmm10 = _mm_permute_pd(zmm1[0].o, 1)
        var_1a0_2.d = zmm1[0]
        var_1a0_2:4.d = zmm12[0]
        var_1a0_2:8.d = zmm10[0]
        
        if (result.b == 0xff)
            zmm0[0].o = data_142d3f660
        else
            zmm4 = __vcmpps_ymmqq_ymmqq_memqq_immb(
                __vandps_ymmqq_ymmqq_memqq(
                    __vblendps_ymmqq_ymmqq_memqq_immb(zmm14, data_142fc9300, 0xf8), 
                    data_142fc9320), 
                data_1434431c0, 2)
            zmm6[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(zmm9[0].o, zmm0[0].o)
            zmm0[0].o = _mm256_extractf128_ps(zmm4[0].o, 1)
            zmm0[0].o |= zmm15.o
            zmm4[0].o = __vorps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6[0].o)
            zmm0[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
            zmm0[0].o = __vpacksswb_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
            result = zx.q(__vpmovmskb_gpr32d_xmmdq(zmm0[0].o))
            
            if (result.b == 0xff)
                zmm0[0].o = data_142d3f660
            else
                zmm4[0].o = zx.o(0)
                zmm6[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(zmm9[0].o, zx.o(0))
                zmm9 = __vcmpps_ymmqq_ymmqq_memqq_immb(
                    __vandps_ymmqq_ymmqq_memqq(
                        __vblendps_ymmqq_ymmqq_memqq_immb(zmm1, data_142fc9300, 0xf8), 
                        data_142fc9320), 
                    data_1434431c0, 2)
                zmm4[0].o = __vpcmpeqd_xmmdq_xmmdq_xmmdq(zmm7[0].o, zx.o(0))
                zmm7[0].o = _mm256_extractf128_ps(zmm9[0].o, 1)
                zmm4[0].o |= zmm7[0].o
                zmm6[0].o = __vorps_xmmdq_xmmdq_xmmdq(zmm9[0].o, zmm6[0].o)
                zmm4[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm4[0].o)
                zmm4[0].o = __vpacksswb_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
                result = zx.q(__vpmovmskb_gpr32d_xmmdq(zmm4[0].o))
                
                if (result.b == 0xff)
                    zmm0[0].o = data_142d3f660
                else
                    zmm0[0].o = zmm11[0].o f+ zmm5[0]
                    zmm6[0].o = zmm10 f+ zmm0[0]
                    zmm0[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
                    
                    if (zmm6[0].o f<= zmm0[0])
                        zmm5[0].o f- zmm11[0].d
                        int32_t rdx_12
                        rdx_12.b = zmm5[0].o f> zmm11[0].d
                        int64_t rax_13 = 0x14
                        
                        if (zmm5[0].o f<= zmm11[0].d)
                            rax_13 = 0
                        
                        uint64_t rax_14 = 2
                        
                        if (zmm10 f<= *(&var_1c0 + rax_13))
                            rax_14 = zx.q(rdx_12)
                        
                        int64_t r9_1 = sx.q(*(&data_1434431e0 + (rax_14 << 2)))
                        int64_t r8_2 = sx.q(*(&data_1434431e0 + (r9_1 << 2)))
                        zmm0[0].o = var_1c0[zx.q((rax_14 * 5).d)]
                        zmm0[0].o = zmm0[0].o f- var_1c0[sx.q((r9_1 * 5).d)]
                        zmm0[0].o = zmm0[0].o f- var_1c0[sx.q((r8_2 * 5).d)]
                        zmm1[0].o = 0x3f800000
                        zmm0[0].o = zmm0[0].o f+ 1f
                        zmm0[0].o = __vsqrtss_xmmdq_xmmdq_xmmd(zmm0[0].o, zmm0[0])
                        zmm0[0].o = 0x3f800000 f/ zmm0[0]
                        zmm1[0].o = zmm0[0].o f* 0.5f
                        zmm0[0].o = 0x3f800000 f/ zmm0[0]
                        zmm0[0].o = zmm0[0].o f* 0.5f
                        *(&var_150 + (rax_14 << 2)) = zmm0[0]
                        zmm0[0].o = var_1c0[sx.q((r9_1 + (rax_14 << 2)).d)]
                        zmm0[0].o = zmm0[0].o f+ var_1c0[sx.q((rax_14 + (r9_1 << 2)).d)]
                        zmm0[0].o = zmm0[0].o f* zmm1[0]
                        *(&var_150 + (r9_1 << 2)) = zmm0[0]
                        zmm0[0].o = var_1c0[sx.q((r8_2 + (rax_14 << 2)).d)]
                        zmm0[0].o = zmm0[0].o f+ var_1c0[sx.q((rax_14 + (r8_2 << 2)).d)]
                        zmm0[0].o = zmm1[0].o f* zmm0[0]
                        result = sx.q((r9_1 + (r8_2 << 2)).d)
                        arg3 = var_1c0[sx.q((r8_2 + (r9_1 << 2)).d)] - var_1c0[result]
                        *(&var_150 + (r8_2 << 2)) = zmm0[0]
                        zmm0[0].o = zmm1[0].o f* arg3[0]
                        int32_t var_144_2 = zmm0[0]
                        zmm0[0].o = var_150
                    else
                        zmm0[0].o = 0x3f800000
                        zmm7[0].o = 0x3f800000
                        zmm0[0].o = zmm6[0].o f+ 1f
                        zmm0[0].o = __vsqrtss_xmmdq_xmmdq_xmmd(zmm0[0].o, zmm0[0])
                        zmm0[0].o = 0x3f800000 f/ zmm0[0]
                        zmm4[0].o = zmm0[0].o f* 0.5f
                        zmm5[0].o = arg5[0].o f- zmm12[0]
                        zmm5[0].o = zmm5[0].o f* zmm4[0]
                        zmm1[0].o = zmm1[0].o f- zmm3[0]
                        zmm1[0].o = zmm1[0].o f* zmm4[0]
                        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm1[0].o, 0x10)
                        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, 
                            (temp0_411 f- zmm14[0]) f* zmm4[0], 0x20)
                        zmm0[0].o = 0x3f800000 f/ zmm0[0]
                        zmm0[0].o = zmm0[0].o f* 0.5f
                        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm0[0].o, 0x30)
        
        zmm12[0].o = data_142d3f770
        zmm15.o = data_142d3f670
        zmm14[0].o = arg8
        zmm5[0].o = arg7
        zmm7[0].o = temp0_328
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        arg3 = _mm_permute_pd(zmm1[0].o, 1)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3)
        float temp0_476[0x4] = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
        zmm1[0].o = zmm1[0].o f+ temp0_476[0]
        float temp0_477[0x4] = _mm_cmp_ss(0x322bcc77, zmm1[0], 6)
        zmm3[0].o = 0xffffffff
        arg3 = __vandnps_xmmdq_xmmdq_xmmdq(temp0_477, 0xffffffff)
        zmm3[0].o = zmm1[0].o f* 0.5f
        zmm1[0].o = __vrsqrtss_xmmdq_xmmdq_xmmd(zmm1[0].o, zmm1[0])
        zmm4[0].o = zmm1[0].o f* zmm1[0]
        zmm4[0].o = zmm4[0].o f* zmm3[0]
        zmm4[0].o = 0x3f000000 f- zmm4[0]
        zmm4[0].o = zmm1[0].o f* zmm4[0]
        zmm1[0].o = zmm1[0].o f+ zmm4[0]
        zmm4[0].o = zmm1[0].o f* zmm1[0]
        zmm3[0].o = zmm3[0].o f* zmm4[0]
        zmm3[0].o = 0x3f000000 f- zmm3[0]
        zmm3[0].o = zmm1[0].o f* zmm3[0]
        zmm1[0].o = zmm1[0].o f+ zmm3[0]
        zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm1[0].o = _mm_permute_ps(arg3, 0)
        zmm0[0].o = __vandps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm1[0].o = __vandnps_xmmdq_xmmdq_memdq(zmm1[0].o, data_142d3f660)
        zmm1[0].o = __vorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm0[0].o = __vmovsd_xmmdq_memq(var_1a0_2:0x10.q)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, var_1a0_2:0x18.d, 0x28)
        arg3 = var_c0_1
        zmm13[0].o = arg9
    
    zmm3[0].o = __vunpcklps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm14[0].o)
    zmm3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, zmm13[0].o, 0x28)
    arg3 = __vmulps_xmmdq_xmmdq_xmmdq(_mm_permute_ps(arg3 f* 0.5f, 0), zmm3[0].o)
    zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm1[0].o)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm3[0].o)
    zmm5[0].o = _mm_permute_ps(zmm4[0].o, 1)
    zmm4[0].o = _mm_permute_ps(zmm4[0].o, 0x1a)
    zmm4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm4[0].o)
    zmm4[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm15.o, zmm4[0].o)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm4[0].o)
    zmm5[0].o = _mm_permute_ps(zmm1[0].o, 4)
    zmm6[0].o = _mm_permute_ps(zmm3[0].o, 0x29)
    zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm6[0].o)
    zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0xff)
    zmm3[0].o = _mm_permute_ps(zmm3[0].o, 0x12)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm3[0].o)
    zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm1[0].o)
    zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm3[0].o)
    zmm6[0].o = _mm_permute_ps(zmm7[0].o, 0xc9)
else
    result = zx.q(_mm256_movemask_ps(zmm1))
    
    if (result.b == 0)
        zmm5[0].o = *(arg2 + 0xc)
        arg5[0].o = _mm_permute_ps(zmm5[0].o, 0)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, arg5[0].o)
        zmm6[0].o = *arg2
        zmm7[0].o = _mm_permute_ps(zmm6[0].o, 0)
        zmm3[0].o = _mm_permute_ps(zmm0[0].o, 0x1b)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm7[0].o)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm3[0].o, data_142d3f7c0)
        zmm10 = *(arg2 + 4)
        zmm9[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm3[0].o)
        zmm3[0].o = _mm_permute_ps(zmm10, 0)
        zmm1[0].o = _mm_permute_pd(zmm0[0].o, 1)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm3[0].o)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm1[0].o, data_142d3f7d0)
        zmm9[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm9[0].o)
        zmm3[0].o = *(arg2 + 8)
        zmm1[0].o = _mm_permute_ps(zmm3[0].o, 0)
        zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xb1)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_memdq(zmm0[0].o, data_142d3f7b0)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm9[0].o)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm11[0].o)
        zmm7[0].o = __vmovsd_xmmdq_memq(*(arg2 + 4))
        zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm6[0].o, 0x20)
        zmm7[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm7[0].o, zmm5[0].o, 0x30)
        zmm9[0].o = _mm_permute_ps(zmm0[0].o, 0xd2)
        zmm3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, zmm6[0].o, 0x10)
        zmm3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, zmm10, 0x20)
        zmm3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, zmm5[0].o, 0x30)
        zmm5[0].o = _mm_permute_ps(zmm0[0].o, 0xc9)
        zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm9[0].o, zmm7[0].o)
        zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm3[0].o)
        zmm5[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm5[0].o)
        zmm5[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm5[0].o)
        zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(arg5[0].o, zmm5[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm6[0].o)
        zmm6[0].o = _mm_permute_ps(zmm5[0].o, 0xd2)
        zmm5[0].o = _mm_permute_ps(zmm5[0].o, 0xc9)
        zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm6[0].o)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm5[0].o)
        zmm3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm3[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm3[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_memdq(zmm0[0].o, arg2[1])
        zmm10 = __vmulps_xmmdq_xmmdq_memdq(zmm4[0].o, data_142e6da00)
        zmm6[0].o = arg7
    else
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm6[0].o = _mm_permute_ps(zmm3[0].o, 1)
        zmm3[0].o = _mm_permute_ps(zmm3[0].o, 0x1a)
        zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm3[0].o)
        zmm3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm15.o, zmm3[0].o)
        zmm10 = data_142e6da00
        zmm12[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm10)
        zmm6[0].o = _mm_permute_ps(zmm0[0].o, 4)
        zmm7[0].o = _mm_permute_ps(zmm1[0].o, 0x29)
        zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm7[0].o)
        zmm0[0].o = _mm_permute_ps(zmm0[0].o, 0xff)
        zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0x12)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm1[0].o)
        zmm13[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm10)
        zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm1[0].o)
        zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm10)
        zmm1[0].o = _mm_permute_pd(zmm13[0].o, 1)
        arg5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(arg5[0].o, arg5[0].o)
        zmm6[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm12[0].o, 1)
        zmm9 = data_143443080
        zmm5[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm13[0].o, 0x10)
        zmm5[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm7[0].o, 4)
        zmm5 = _mm256_blend_ps(zmm6, zmm5, 0xf)
        zmm6[0].o = _mm256_extractf128_ps(zmm6[0].o, 1)
        zmm6[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm7[0].o, 1)
        zmm6[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm12[0].o, 2)
        zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm13[0].o, 0x60)
        zmm5 = _mm256_insertf128_ps(zmm5, zmm6[0].o, 1)
        zmm1 = _mm256_blend_ps(zmm9, zmm1, 1)
        zmm6[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm7[0].o, 2)
        zmm6[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm12[0].o, 4)
        zmm1[0].o = _mm256_extractf128_ps(zmm1[0].o, 1)
        zmm1[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm11[0].o, zmm1[0].o, 8)
        zmm1 = _mm256_insertf128_ps(zmm6, zmm1[0].o, 1)
        int32_t var_140_1[0x8] = zmm5
        zmm1[0].o = *arg2
        zmm5[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm1[0].o)
        zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm5[0].o)
        zmm3[0].o = _mm_permute_ps(zmm6[0].o, 0xc1)
        zmm6[0].o = _mm_permute_ps(zmm6[0].o, 0xda)
        zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm6[0].o)
        zmm3[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm15.o, zmm3[0].o)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm3[0].o)
        zmm6[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm1[0].o, *(arg2 + 4), 0x10)
        zmm1[0].o = __vshufps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm1[0].o, 4)
        zmm6[0].o = _mm_permute_ps(zmm5[0].o, 0x29)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm1[0].o)
        zmm6[0].o = _mm_broadcast_ss(*(arg2 + 0xc))
        zmm5[0].o = _mm_permute_ps(zmm5[0].o, 0x12)
        zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm6[0].o)
        zmm6[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm5[0].o)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6[0].o)
        zmm6[0].o = __vmovsd_xmmdq_memq(*(arg2 + 0x24))
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm4[0].o, 0x20)
        zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm5[0].o)
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm4[0].o)
        zmm11[0].o = arg2[1].d
        zmm5[0].o = _mm_permute_pd(zmm0[0].o, 1)
        zmm4[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(arg5[0].o, zmm3[0].o, 1)
        zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm0[0].o, 0x10)
        zmm6[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm1[0].o, 4)
        zmm4[0].o = _mm256_extractf128_ps(zmm4[0].o, 1)
        zmm4[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm1[0].o, 1)
        zmm4[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm3[0].o, 2)
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm0[0].o, 0x60)
        zmm0 = _mm256_blend_ps(zmm9, zmm5, 1)
        zmm1[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 2)
        zmm5[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm3[0].o, 4)
        zmm0[0].o = _mm256_extractf128_ps(zmm0[0].o, 1)
        zmm0[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm11[0].o, 1)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg2 + 0x14), 0x10)
        zmm1[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, *(arg2 + 0x18), 0x20)
        zmm10 = __vmulps_xmmdq_xmmdq_memdq(zmm10, arg2[2])
        zmm0[0].o = _mm_permute_ps(zmm12[0].o, 0)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm0[0].o)
        zmm3[0].o = _mm_permute_ps(zmm7[0].o, 0xaa)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm3[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm3[0].o)
        zmm3[0].o = _mm_permute_ps(zmm13[0].o, 0)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm3[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm3[0].o)
        zmm3[0].o = _mm_broadcast_ss(var_134)
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm3[0].o)
        zmm11[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm3[0].o)
        zmm0[0].o = _mm_broadcast_ss(var_130)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm0[0].o)
        zmm7[0].o = _mm_broadcast_ss(var_128)
        zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm7[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm7[0].o)
        zmm7[0].o = _mm_broadcast_ss(var_12c)
        zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm7[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm7[0].o)
        zmm7[0].o = _mm_broadcast_ss(var_124)
        zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm7[0].o)
        zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm7[0].o)
        zmm7[0].o = _mm_broadcast_ss(zmm1[0])
        zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm7[0].o)
        zmm3[0].o = _mm_broadcast_ss(zmm1[2])
        zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm3[0].o)
        zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm3[0].o)
        zmm7[0].o = _mm_broadcast_ss(zmm1[1])
        zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm7[0].o)
        zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm7[0].o)
        zmm7[0].o = _mm_broadcast_ss(zmm1[3])
        zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm7[0].o)
        zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm3[0].o, zmm7[0].o)
        zmm7[0].o = _mm_broadcast_ss(zmm1[4])
        zmm6[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm7[0].o)
        zmm7[0].o = _mm_broadcast_ss(zmm1[6])
        zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm7[0].o)
        zmm5[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm5[0].o)
        zmm6[0].o = _mm_broadcast_ss(zmm1[5])
        zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6[0].o)
        zmm4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm4[0].o)
        zmm5[0].o = _mm_broadcast_ss(zmm1[7])
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm5[0].o)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm1[0].o)
        zmm4[0].o = __vmovshdup_xmmdq_xmmdq(zmm11[0].o)
        zmm9[0].o = _mm_permute_pd(zmm11[0].o, 1)
        zmm6[0].o = zx.o(__vextractps_gpr32_xmmdq_immb(zmm0[0].o, 0))
        zmm12[0].o = __vmovshdup_xmmdq_xmmdq(zmm0[0].o)
        int32_t temp0_195 = __vextractps_gpr32_xmmdq_immb(zmm3[0].o, 0)
        zmm14[0].o = _mm_permute_pd(zmm0[0].o, 1)
        zmm7[0].o = zx.o(temp0_195)
        zmm0[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm6[0].o, 1)
        zmm13 = _mm256_insertf128_ps(zmm11, zmm0[0].o, 1)
        zmm0[0].o = __vmovshdup_xmmdq_xmmdq(zmm3[0].o)
        zmm5[0].o = _mm_permute_pd(zmm3[0].o, 1)
        zmm3[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, zmm7[0].o, 1)
        zmm15 = _mm256_insertf128_ps(zmm3, zmm1[0].o, 1).28
        var_1c0 = zmm13
        zmm1[0].o = zmm11[0].o f* zmm11[0].d
        zmm4[0].o = zmm4[0].o f* zmm4[0]
        zmm1[0].o = zmm1[0].o f+ zmm4[0]
        zmm4[0].o = zmm9[0].o f* zmm9[0]
        zmm3[0].o = zmm1[0].o f+ zmm4[0]
        zmm1[0].o = zmm6[0].o f* zmm6[0]
        zmm4[0].o = zmm12[0].o f* zmm12[0]
        zmm1[0].o = zmm1[0].o f+ zmm4[0]
        zmm4[0].o = zmm14[0].o f* zmm14[0]
        zmm1[0].o = zmm1[0].o f+ zmm4[0]
        zmm4[0].o = zmm7[0].o f* zmm7[0]
        zmm0[0].o = zmm0[0].o f* zmm0[0]
        zmm0[0].o = zmm4[0].o f+ zmm0[0]
        zmm4[0].o = zmm5[0].o f* zmm5[0]
        zmm4[0].o = zmm0[0].o f+ zmm4[0]
        zmm12[0].o = 0xb22bcc77
        zmm0[0].o = zmm3[0].o f+ -9.99999994e-09f
        zmm5[0].o = __vrsqrtss_xmmdq_xmmdq_xmmd(zmm3[0].o, zmm3[0])
        zmm14[0].o = 0x3f000000
        zmm3[0].o = zmm3[0].o f* 0.5f
        zmm7[0].o = zmm5[0].o f* zmm5[0]
        zmm7[0].o = zmm3[0].o f* zmm7[0]
        zmm7[0].o = 0x3f000000 f- zmm7[0]
        zmm7[0].o = zmm5[0].o f* zmm7[0]
        zmm5[0].o = zmm5[0].o f+ zmm7[0]
        zmm7[0].o = zmm5[0].o f* zmm5[0]
        zmm3[0].o = zmm3[0].o f* zmm7[0]
        zmm3[0].o = 0x3f000000 f- zmm3[0]
        zmm3[0].o = zmm5[0].o f* zmm3[0]
        zmm3[0].o = zmm5[0].o f+ zmm3[0]
        zmm7[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm7[0].o)
        zmm0[0].o = _mm_cmp_ss(zmm7[0].o, zmm0[0], 2)
        zmm5[0].o = __vblendvps_xmmdq_xmmdq_xmmdq_xmmdq(0x3f800000, zmm3[0].o, zmm0[0].o)
        zmm0[0].o = zmm1[0].o f+ -9.99999994e-09f
        zmm3[0].o = __vrsqrtss_xmmdq_xmmdq_xmmd(zmm1[0].o, zmm1[0])
        zmm1[0].o = zmm1[0].o f* 0.5f
        zmm6[0].o = zmm3[0].o f* zmm3[0]
        zmm6[0].o = zmm1[0].o f* zmm6[0]
        zmm6[0].o = 0x3f000000 f- zmm6[0]
        zmm6[0].o = zmm3[0].o f* zmm6[0]
        zmm3[0].o = zmm3[0].o f+ zmm6[0]
        zmm6[0].o = zmm3[0].o f* zmm3[0]
        zmm1[0].o = zmm1[0].o f* zmm6[0]
        zmm1[0].o = 0x3f000000 f- zmm1[0]
        zmm1[0].o = zmm3[0].o f* zmm1[0]
        zmm1[0].o = zmm3[0].o f+ zmm1[0]
        zmm0[0].o = _mm_cmp_ss(zmm7[0].o, zmm0[0], 2)
        zmm0[0].o = __vblendvps_xmmdq_xmmdq_xmmdq_xmmdq(0x3f800000, zmm1[0].o, zmm0[0].o)
        zmm1[0].o = zmm4[0].o f+ -9.99999994e-09f
        zmm3[0].o = __vrsqrtss_xmmdq_xmmdq_xmmd(zmm4[0].o, zmm4[0])
        zmm4[0].o = zmm4[0].o f* 0.5f
        zmm6[0].o = zmm3[0].o f* zmm3[0]
        zmm6[0].o = zmm4[0].o f* zmm6[0]
        zmm6[0].o = 0x3f000000 f- zmm6[0]
        zmm6[0].o = zmm3[0].o f* zmm6[0]
        zmm3[0].o = zmm3[0].o f+ zmm6[0]
        zmm6[0].o = zmm3[0].o f* zmm3[0]
        zmm4[0].o = zmm4[0].o f* zmm6[0]
        zmm4[0].o = 0x3f000000 f- zmm4[0]
        zmm4[0].o = zmm3[0].o f* zmm4[0]
        zmm3[0].o = zmm3[0].o f+ zmm4[0]
        zmm1[0].o = _mm_cmp_ss(zmm7[0].o, zmm1[0], 2)
        zmm1[0].o = __vblendvps_xmmdq_xmmdq_xmmdq_xmmdq(0x3f800000, zmm3[0].o, zmm1[0].o)
        zmm3[0].o = zmm5[0].o f* zmm11[0].d
        zmm4[0].o = zmm5[0].o f* var_1c0[1]
        zmm3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, zmm4[0].o, 0x10)
        zmm4[0].o = zmm5[0].o f* var_1b8
        zmm3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, zmm4[0].o, 0x20)
        zmm4[0].o = zmm0[0].o f* var_1b0
        zmm5[0].o = zmm0[0].o f* var_1ac
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm5[0].o, 0x10)
        zmm0[0].o = zmm0[0].o f* var_1a8
        zmm4[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm4[0].o, zmm0[0].o, 0x20)
        zmm0[0].o = zmm1[0].o f* zmm15.d
        zmm5[0].o = zmm1[0].o f* zmm15:4.d
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm5[0].o, 0x10)
        zmm1[0].o = zmm1[0].o f* zmm15:8.d
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm1[0].o, 0x20)
        zmm1[0].o = _mm_cmp_ps(arg5[0].o, zmm10, 2)
        zmm1[0].o = __vpackssdw_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm0[0].o)
        zmm1[0].o = __vpmovzxwd_xmmdq_xmmq(zmm1[0].q)
        zmm1[0].o = __vpslld_xmmdq_xmmdq_immb(zmm1[0].o, 0x1f)
        zmm5 = data_1434431a0
        zmm1 = __vblendvps_ymmqq_ymmqq_memqq_ymmqq(zmm5, data_1434430a0, zmm1)
        zmm5[0].o = _mm_permute_ps(zmm1[0].o, 0)
        zmm5 = _mm256_insertf128_ps(zmm5, zmm5[0].o, 1)
        zmm3 = _mm256_mul_ps(zmm5, zmm3)
        zmm11[0].o = __vmovshdup_xmmdq_xmmdq(zmm3[0].o)
        zmm12[0].o = _mm_permute_pd(zmm3[0].o, 1)
        var_1c0[0] = zmm3[0]
        var_1c0[1] = zmm11[0].d
        float var_1b8_1 = zmm12[0]
        zmm5[0].o = _mm_permute_ps(zmm1[0].o, 0x55)
        zmm5 = _mm256_insertf128_ps(zmm5, zmm5[0].o, 1)
        zmm7 = _mm256_mul_ps(zmm5, zmm4)
        zmm4[0].o = __vmovshdup_xmmdq_xmmdq(zmm7[0].o)
        zmm6[0].o = _mm_permute_pd(zmm7[0].o, 1)
        float var_1b0_1 = zmm7[0]
        int32_t var_1ac_1 = zmm4[0]
        float var_1a8_1 = zmm6[0]
        zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0xaa)
        arg5 = _mm256_mul_ps(_mm256_insertf128_ps(zmm1, zmm1[0].o, 1), zmm0)
        zmm5[0].o = __vmovshdup_xmmdq_xmmdq(arg5[0].o)
        zmm0[0].o = _mm_permute_pd(arg5[0].o, 1)
        int224_t var_1a0_1
        var_1a0_1.d = arg5[0]
        var_1a0_1:4.d = zmm5[0]
        var_1a0_1:8.d = zmm0[0]
        zmm1 = __vcmpps_ymmqq_ymmqq_memqq_immb(
            __vandps_ymmqq_ymmqq_memqq(
                __vblendps_ymmqq_ymmqq_memqq_immb(zmm3, data_142fc9300, 0xf8), data_142fc9320), 
            data_1434431c0, 2)
        result = zx.q(_mm256_movemask_ps(zmm1))
        
        if (result.b == 0xff)
            zmm0[0].o = data_142d3f660
        else
            zmm1 = __vcmpps_ymmqq_ymmqq_memqq_immb(
                __vandps_ymmqq_ymmqq_memqq(
                    __vblendps_ymmqq_ymmqq_memqq_immb(zmm7, data_142fc9300, 0xf8), data_142fc9320), 
                data_1434431c0, 2)
            result = zx.q(_mm256_movemask_ps(zmm1))
            
            if (result.b == 0xff)
                zmm0[0].o = data_142d3f660
            else
                zmm1 = __vcmpps_ymmqq_ymmqq_memqq_immb(
                    __vandps_ymmqq_ymmqq_memqq(
                        __vblendps_ymmqq_ymmqq_memqq_immb(arg5, data_142fc9300, 0xf8), 
                        data_142fc9320), 
                    data_1434431c0, 2)
                result = zx.q(_mm256_movemask_ps(zmm1))
                
                if (result.b == 0xff)
                    zmm0[0].o = data_142d3f660
                else
                    zmm1[0].o = zmm3[0].o f+ zmm4[0]
                    zmm13[0].o = zmm0[0].o f+ zmm1[0]
                    zmm1[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm1[0].o)
                    
                    if (zmm13[0].o f<= zmm1[0])
                        zmm4[0].o f- zmm3[0]
                        int32_t rdx
                        rdx.b = zmm4[0].o f> zmm3[0]
                        int64_t rax_6 = 0x14
                        
                        if (zmm4[0].o f<= zmm3[0])
                            rax_6 = 0
                        
                        uint64_t rax_7 = 2
                        
                        if (zmm0[0].o f<= *(&var_1c0 + rax_6))
                            rax_7 = zx.q(rdx)
                        
                        int64_t r9 = sx.q(*(&data_1434431e0 + (rax_7 << 2)))
                        int64_t r8_1 = sx.q(*(&data_1434431e0 + (r9 << 2)))
                        zmm0[0].o = var_1c0[zx.q((rax_7 * 5).d)]
                        zmm0[0].o = zmm0[0].o f- var_1c0[sx.q((r9 * 5).d)]
                        zmm0[0].o = zmm0[0].o f- var_1c0[sx.q((r8_1 * 5).d)]
                        zmm0[0].o = zmm0[0].o f+ 1f
                        zmm0[0].o = __vsqrtss_xmmdq_xmmdq_xmmd(zmm0[0].o, zmm0[0])
                        zmm0[0].o = 0x3f800000 f/ zmm0[0]
                        zmm1[0].o = zmm0[0].o f* 0.5f
                        zmm0[0].o = 0x3f800000 f/ zmm0[0]
                        zmm0[0].o = zmm0[0].o f* 0.5f
                        *(&var_150 + (rax_7 << 2)) = zmm0[0]
                        zmm0[0].o = var_1c0[sx.q((r9 + (rax_7 << 2)).d)]
                        zmm0[0].o = zmm0[0].o f+ var_1c0[sx.q((rax_7 + (r9 << 2)).d)]
                        zmm0[0].o = zmm0[0].o f* zmm1[0]
                        *(&var_150 + (r9 << 2)) = zmm0[0]
                        zmm0[0].o = var_1c0[sx.q((r8_1 + (rax_7 << 2)).d)]
                        zmm0[0].o = zmm0[0].o f+ var_1c0[sx.q((rax_7 + (r8_1 << 2)).d)]
                        zmm0[0].o = zmm1[0].o f* zmm0[0]
                        zmm3[0].o = var_1c0[sx.q((r8_1 + (r9 << 2)).d)]
                        result = sx.q((r9 + (r8_1 << 2)).d)
                        zmm3[0].o = zmm3[0].o f- var_1c0[result]
                        *(&var_150 + (r8_1 << 2)) = zmm0[0]
                        zmm0[0].o = zmm1[0].o f* zmm3[0]
                        int32_t var_144_1 = zmm0[0]
                        zmm0[0].o = var_150
                    else
                        zmm0[0].o = zmm13[0].o f+ 1f
                        zmm0[0].o = __vsqrtss_xmmdq_xmmdq_xmmd(zmm0[0].o, zmm0[0])
                        zmm0[0].o = 0x3f800000 f/ zmm0[0]
                        zmm1[0].o = zmm0[0].o f* 0.5f
                        zmm3[0].o = zmm6[0].o f- zmm5[0]
                        zmm3[0].o = zmm3[0].o f* zmm1[0]
                        zmm4[0].o = arg5[0].o f- zmm12[0]
                        zmm4[0].o = zmm4[0].o f* zmm1[0]
                        zmm3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, zmm4[0].o, 0x10)
                        zmm4[0].o = zmm11[0].o f- zmm7[0]
                        zmm1[0].o = zmm4[0].o f* zmm1[0]
                        zmm1[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, zmm1[0].o, 0x20)
                        zmm0[0].o = 0x3f800000 f/ zmm0[0]
                        zmm0[0].o = zmm0[0].o f* 0.5f
                        zmm0[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm1[0].o, zmm0[0].o, 0x30)
        
        zmm12[0].o = data_142d3f770
        zmm13[0].o = arg9
        zmm15.o = data_142d3f670
        zmm6[0].o = arg7
        zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm0[0].o)
        zmm3[0].o = _mm_permute_pd(zmm1[0].o, 1)
        zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm3[0].o)
        zmm3[0].o = __vmovshdup_xmmdq_xmmdq(zmm1[0].o)
        zmm1[0].o = zmm1[0].o f+ zmm3[0]
        zmm3[0].o = 0x322bcc77
        zmm3[0].o = _mm_cmp_ss(0x322bcc77, zmm1[0], 6)
        zmm4[0].o = 0xffffffff
        zmm3[0].o = __vandnps_xmmdq_xmmdq_xmmdq(zmm3[0].o, 0xffffffff)
        zmm4[0].o = zmm1[0].o f* 0.5f
        zmm1[0].o = __vrsqrtss_xmmdq_xmmdq_xmmd(zmm1[0].o, zmm1[0])
        zmm5[0].o = zmm1[0].o f* zmm1[0]
        zmm5[0].o = zmm5[0].o f* zmm4[0]
        zmm5[0].o = 0x3f000000 f- zmm5[0]
        zmm5[0].o = zmm1[0].o f* zmm5[0]
        zmm1[0].o = zmm1[0].o f+ zmm5[0]
        zmm5[0].o = zmm1[0].o f* zmm1[0]
        zmm4[0].o = zmm4[0].o f* zmm5[0]
        zmm4[0].o = 0x3f000000 f- zmm4[0]
        zmm4[0].o = zmm1[0].o f* zmm4[0]
        zmm1[0].o = zmm1[0].o f+ zmm4[0]
        zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0)
        zmm0[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm1[0].o = _mm_permute_ps(zmm3[0].o, 0)
        zmm0[0].o = __vandps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm1[0].o = __vandnps_xmmdq_xmmdq_memdq(zmm1[0].o, data_142d3f660)
        zmm1[0].o = __vorps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
        zmm0[0].o = __vmovsd_xmmdq_memq(var_1a0_1:0x10.q)
        zmm0[0].o = __vinsertps_xmmdq_xmmdq_memd_immb(zmm0[0].o, var_1a0_1:0x18.d, 0x28)
        zmm14[0].o = arg8
    
    zmm3[0].o = __vunpcklps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm14[0].o)
    zmm3[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm3[0].o, zmm13[0].o, 0x28)
    arg3 = __vmulps_xmmdq_xmmdq_xmmdq(_mm_permute_ps(arg3 f* 0.5f, 0), zmm3[0].o)
    zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm1[0].o)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm3[0].o)
    zmm5[0].o = _mm_permute_ps(zmm4[0].o, 1)
    zmm4[0].o = _mm_permute_ps(zmm4[0].o, 0x1a)
    zmm4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm4[0].o)
    zmm4[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm15.o, zmm4[0].o)
    zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm10, zmm4[0].o)
    zmm5[0].o = _mm_permute_ps(zmm1[0].o, 4)
    zmm6[0].o = _mm_permute_ps(zmm3[0].o, 0x29)
    zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm6[0].o)
    zmm1[0].o = _mm_permute_ps(zmm1[0].o, 0xff)
    zmm3[0].o = _mm_permute_ps(zmm3[0].o, 0x12)
    zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm3[0].o)
    zmm3[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm1[0].o)
    zmm3[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm10, zmm3[0].o)
    zmm6[0].o = _mm_permute_ps(zmm10, 0xc9)
zmm1[0].o = __vsubps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm1[0].o)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm1[0].o)
zmm5[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm5[0].o)
zmm5[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm4[0].o, 3)
zmm5 = _mm256_shuffle_ps(zmm3, zmm5, 0)
zmm6[0].o = __vxorps_xmmdq_xmmdq_xmmdq(zmm6[0].o, zmm6[0].o)
zmm5 = _mm256_shuffle_ps(zmm5, zmm6, 0x42)
zmm6 = _mm256_blend_ps(_mm256_insertf128_ps(zmm0, zmm1[0].o, 1), zmm5, 0xc0)
zmm6[0].o = _mm256_extractf128_ps(zmm6[0].o, 1)
zmm6[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm4[0].o, 2)
zmm6[0].o = __vinsertps_xmmdq_xmmdq_xmmdq_immb(zmm6[0].o, zmm3[0].o, 0x60)
zmm3 = _mm256_blend_ps(
    _mm256_blend_ps(__vunpckhpd_ymmqq_ymmqq_memqq(zmm3, data_143443080), zmm1, 2), zmm4, 4)
zmm4[0].o = _mm256_extractf128_ps(zmm3[0].o, 1)
zmm0[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm0[0].o, zmm4[0].o, 8)
zmm4[0].o = __vsubps_xmmdq_xmmdq_xmmdq(arg3, arg3)
zmm7[0].o = data_142e6d9f0
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm7[0].o)
float temp0_628[0x4] = __vmulps_xmmdq_xmmdq_xmmdq(__vaddps_xmmdq_xmmdq_xmmdq(arg3, arg3), zmm7[0].o)
zmm1[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(zmm5[0].o, zmm1[0].o, 4)
zmm5[0].o = _mm_permute_ps(zmm4[0].o, 0)
zmm5[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm1[0].o)
zmm7[0].o = _mm_permute_ps(zmm4[0].o, 0x55)
zmm7[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm7[0].o, zmm6[0].o)
zmm5[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm7[0].o)
zmm4[0].o = _mm_permute_ps(zmm4[0].o, 0xaa)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm3[0].o)
zmm4[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm5[0].o, zmm4[0].o)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm0[0].o)
zmm4[0].o = _mm_permute_ps(temp0_628, 0)
zmm1[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm1[0].o)
zmm4[0].o = _mm_permute_ps(temp0_628, 0x55)
zmm4[0].o = __vmulps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm6[0].o)
float temp0_644[0x4] = __vmulps_xmmdq_xmmdq_xmmdq(_mm_permute_ps(temp0_628, 0xaa), zmm3[0].o)
zmm1[0].o = __vandps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm12[0].o)
zmm3[0].o = __vandps_xmmdq_xmmdq_xmmdq(zmm4[0].o, zmm12[0].o)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, zmm3[0].o)
arg3 = __vandps_xmmdq_xmmdq_xmmdq(temp0_644, zmm12[0].o)
zmm1[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm1[0].o, arg3)
float temp0_650[0x4] = __vsubps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm0[0].o = __vaddps_xmmdq_xmmdq_xmmdq(zmm0[0].o, zmm1[0].o)
zmm1[0].o = _mm_permute_ps(zmm0[0].o, 0x24)
zmm1[0].o = __vblendps_xmmdq_xmmdq_xmmdq_immb(temp0_650, zmm1[0].o, 8)
*arg1 = zmm1[0].o
arg1[1].d = __vextractps_memd_xmmdq_immb(zmm0[0].o, 1)
*(arg1 + 0x14) = __vextractps_memd_xmmdq_immb(zmm0[0].o, 2)
*(arg1 + 0x18) = 1
arg5[0].o = var_88
_mm256_zeroupper()
return result
